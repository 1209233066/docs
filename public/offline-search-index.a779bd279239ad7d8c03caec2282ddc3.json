[{"body":"常用库 把源码包打包成 whl 文件,减少安装依赖\npip3 install --upgrade setuptools wheel pip3 wheel . --wheel-dir=dist ls dist/ibm_db-3.2.3-cp36-cp36m-linux_x86_64.whl 库名 来源 描述 os 标准库 标准库提供与操作系统进行交互的功能，例如文件和目录操作 sys 标准库 标准库提供访问 Python 解释器的能力，处理命令行参数和环境设置 io 标准库 处理流式I/O操作 subprocess 标准库 用于执行子进程，允许调用外部程序和命令(Shell) shutil 标准库 标准库文件和目录的高级操作，例如复制、移动和删除 time 标准库 提供与时间相关的功能，例如获取当前时间和暂停程序执行 datetime 标准库 提供处理日期和时间的类和方法 random 标准库 生成随机数和随机选择元素的工具 json 标准库 用于处理 JSON 数据，提供解析和生成 JSON 的方法 re 标准库 提供正则表达式支持，用于字符串模式匹配和处理 platform 标准库 获取操作系统和平台信息，例如版本号、架构等 fcntl 标准库 文件控制（Unix系统文件描述符操作） optparse 标准库 命令行选项解析（已弃用，推荐使用argparse） glob optparse标准库 文件名模式匹配工具，用于查找符合特定模式的文件 ast 标准库 抽象语法树操作 socket 标准库 网络套接字通信 math 标准库 数学运算函数 decimal 标准库 十进制高精度计算（标准库中的Decimal类） base64 标准库 Base64编解码 random 标准库 生成伪随机数 traceback 标准库 异常堆栈跟踪处理 urlib 标准库 处理 URL 请求和响应的工具，用于网络操作 smtplib 标准库 与SMTP服务器交互，用于发送邮件 requests 第三方 对http 发起请求 psutil 第三方 获取操作系统级别 cpu / mem /disk 等基础信息 Paramiko 第三方 库基于 SSH 的客户端库，用于远程服务器自动化控制 Ansible 第三方 IT 自动化工具，用于配置管理、应用部署和任务白动化 Crontab 第三方 提供与 Linux cron 服务的交互，方便创建和管理定时任务 Docker 第三方 管理 Docker 容器和镜像的 API 库，支持创建、管理和监控容器 Kubernetes 第三方 与K8s API 进行交互，管理K8s集群和资源 pyg2plot 第三方 数据可视化 redis 第三方 redis数据库驱动 mysql.connector 第三方 MySQL官方数据库连接 pymsql 第三方 纯Python实现的MySQL客户端 pymongo 第三方 MongoDB数据库驱动 bson MongoDB BSON格式处理（Binary JSON） oracledb 第三方 Oracle数据库连接（原cx_Oracle） pyodbc 第三方 ODBC数据库连接 kazoo 第三方 zookeeper kafka-python 第三方 rabbitMQ 第三方 argparse 第三方 typer 第三方 uvicorn 第三方 异步 Web 服务 ","categories":["python"],"description":"库|python\n","excerpt":"库|python\n","ref":"/python/package/package.html","tags":["python"],"title":"包管理"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\nelasticsearch版本： 7.15.0、7.17.27\n【注意事项】| doc\nelasticsearch 依赖jdk 环境，如果无法判断使用哪个版本jdk。可以下载/国内下载镜像带有jdk版本的elasticsearch。\nelasticsearch默认监听9200（提供restfull风格的服务） 和 9300 （集群内部选举和数据同步）\nelasticsearch 属于io密集型应用，尽可能把数据目录单独挂载出来。并使用高性能磁盘。\n部署环境准备 内存映射\n内存映射（Memory Mapping）是一种在应用程序地址空间和物理存储器之间建立映射关系的技术\nmax_map_count 参数是用于限制每个进程能够使用的最大内存映射区域数量。\n如果不修改，启动时提示错误： max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]\nsysctl -w vm.max_map_count=655360 echo 'vm.max_map_count=655360' \u003e\u003e/etc/sysctl.conf 内存锁定\n当开启内存锁定后，以下参数必须设定报错信息： memory locking requested for elasticsearch process but memory is not locked\necho \"DefaultLimitMEMLOCK=infinity\" \u003e\u003e/etc/systemd/system.conf # 该命令会重启systemd 进程 systemctl daemon-reexec 文件描述符设置\ncat \u003e\u003e/etc/security/limits.conf\u003c\u003cEOF * soft nofile 65536 * hard nofile 65536 * soft nproc 32000 * hard nproc 32000 * hard memlock unlimited * soft memlock unlimited EOF cat \u003e\u003e /etc/systemd/system.conf\u003c\u003cEOF DefaultLimitNOFILE=65536 DefaultLimitNPROC=32000 DefaultLimitMEMLOCK=infinity EOF 安装部署 创建服务账号\nelasticsearch 无法以root用户启动\nuseradd elasticsearch 安装elasticsearch\nversion=7.17.29 wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-${version}-linux-x86_64.tar.gz tar -xzf elasticsearch-${version}-linux-x86_64.tar.gz -C /data/elk # $ES_HOME 对应/data/elk/elasticsearch ln -svf /data/elk/{elasticsearch-${version},elasticsearch} 创建日志和数据目录\nmkdir -p /data/elk/instance1/{data,log,config} chown -R elasticsearch:elasticsearch /data/elk/{elasticsearch-${version},elasticsearch} chown -R elasticsearch:elasticsearch /data/elk/instance1 启动elasticsearch\n本示例实现单机多实例的部署，因此需要实现在配置文件、日志文件、数据文件上隔离。Elasticsearch 有三个配置文件：\nelasticsearch.yml用于配置 Elasticsearch jvm.options用于配置 Elasticsearch JVM 设置 log4j2.properties用于配置 Elasticsearch 日志记录 grep -vE \"^#|^$\" /data/elk/elasticsearch/config/jvm.options \u003e/data/elk/instance1/config/jvm.options grep -vE \"^#|^$\" /data/elk/elasticsearch/config/log4j2.properties \u003e/data/elk/instance1/config/log4j2.properties 主配置文件\ntee /data/elk/instance1/config/elasticsearch.yml \u003c\u003c'EOF' path: data: /data/elk/instance1/data # 等同path.data: /data/elk/instance1/data，其他依次类推 logs: /data/elk/instance1/log node: name: node-1 cluster: name: cluster-alerts initial_master_nodes: [\"node-1\"] bootstrap: # 是否在程序启动时立即分配指定大小内存给elasticsearch进程 memory_lock: false network: host: 0.0.0.0 http: port: 9200 ingest: # 关闭geoip 组件下载 geoip: downloader: enabled: false EOF systemd 管理服务\n测试启动命令\n# *命令行传参启动，在参数前添加`-E`来覆盖配置文件对应参数* su - elasticsearch -c \"export ES_PATH_CONF=/data/elk/instance1/config ;\\ export ES_JAVA_HOME=/data/elk/elasticsearch/jdk ;\\ /data/elk/elasticsearch/bin/elasticsearch \\ -Ehttp.port=8200\" tee /etc/systemd/system/elasticsearch.service \u003c\u003c'EOF' [Unit] Description= elasticsearch service Documentation=https://www.elastic.co/docs After=network.target [Service] Environment=\"ES_PATH_CONF=/data/elk/instance1/config\" Environment=\"ES_JAVA_HOME=/data/elk/elasticsearch/jdk\" ExecStart=/data/elk/elasticsearch/bin/elasticsearch User=elasticsearch LimitNOFILE=65535 [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable elasticsearch --now systemctl status elasticsearch 验证集群状态\n通常情况下使用 elasticsearch-head 插件来监听elasticsearch。也可以使用api接口来查询\n通过chrome 商店安装，需要vpn 通过github上提供的docker 镜像启动 #由于前后端服务做了分离，因此需要开启elasticsearch跨域请求 http.cors.enabled: true http.cors.allow-origin: \"*\" docker run --rm -it -p9100:9100 mobz/elasticsearch-head:5-alpine curl http://127.0.0.1:9200/_cat/nodes?v curl -s http://127.0.0.1:9200/_cluster/health?pretty=true |jq \".status\" 安全设置 开启用户认证\n⚠️\nERROR: [1] bootstrap checks failed. You must address the points described in the following [1] lines before starting Elasticsearch. bootstrap check failure [1] of [1]: Transport SSL must be enabled if security is enabled on a [basic] license. Please set [xpack.security.transport.ssl.enabled] to [true] or disable security by setting [xpack.security.enabled] to [false]\nxpack: security: enabled: true transport: # xpack.security.enabled 依赖xpack.security.transport.ssl.enabled 功能开启，实现在9300端口的https,而不是9200 ssl: enabled: true 设置密码\n/data/elk/elasticsearch/bin/elasticsearch-setup-passwords interactive [root@seagullcore01-uat-s2 ~]# /data/elk/elasticsearch/bin/elasticsearch-setup-passwords interactive -u http://100.64.0.5:8200 warning: usage of JAVA_HOME is deprecated, use ES_JAVA_HOME Initiating the setup of passwords for reserved users elastic,apm_system,kibana,kibana_system,logstash_system,beats_system,remote_monitoring_user. You will be prompted to enter passwords as the process progresses. Please confirm that you would like to continue [y/N]y Enter password for [elastic]: Reenter password for [elastic]: Enter password for [apm_system]: Reenter password for [apm_system]: Enter password for [kibana_system]: Reenter password for [kibana_system]: Enter password for [logstash_system]: Reenter password for [logstash_system]: Enter password for [beats_system]: Reenter password for [beats_system]: Enter password for [remote_monitoring_user]: Reenter password for [remote_monitoring_user]: Changed password for user [apm_system] Changed password for user [kibana_system] Changed password for user [kibana] Changed password for user [logstash_system] Changed password for user [beats_system] Changed password for user [remote_monitoring_user] Changed password for user [elastic] 开启https\n# 生成ca su - elasticsearch -c \"/data/elk/elasticsearch/bin/elasticsearch-certutil ca \\ --out /data/elk/instance1/config/certs/ca.crt --pass mypass\" # 生成公钥 su - elasticsearch -c \"/data/elk/elasticsearch/bin/elasticsearch-certutil cert --ca /data/elk/instance1/config/certs/ca.crt \\ --out /data/elk/instance1/config/certs/server.crt --ca-pass mypass\" xpack.security.http.ssl.enabled: true xpack.security.http.ssl.keystore.path: certs/server.crt ","categories":["ELK"],"description":"\n使用elasticsearch\r\n","excerpt":"\n使用elasticsearch\r\n","ref":"/elasticsearch/install.html","tags":["elasticsearch"],"title":"单机版本安装"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\njdk版本：openjdk 11\nzookeeper版本: 3.4.14 支持jdk1.8\nkafka版本：3.3.2\nzookeeper可视化工具: PrettyZoo\n通常多个kafka集群通过chroot的方式使用同一个zookeer集群。在Kafka 0.9.0.0 版本之前，除了 broker 之外，消费者也会使用 Zookeeper 来保存一些信息，比如消费者群组的信息、主题信息、消费分区的偏移量\n安装部署 安装依赖\njdk\nwget https://download.java.net/java/ga/jdk11/openjdk-11_linux-x64_bin.tar.gz tar xf openjdk-11_linux-x64_bin.tar.gz -C /opt ln -svf /opt/{jdk-11,jdk} cat\u003e\u003e/etc/profile\u003c\u003c'EOF' export JAVA_HOME=/opt/jdk export JAVA_JRE=$JAVA_HOME/jre export CLASSPATH=$JAVA_HOME/lib:$JAVA_HOME/jre/lib export PATH=$JAVA_HOME/bin:$JAVA_JRE/bin:$PATH:. EOF source /etc/profile java -version 部署安装\n下载软件\nwget https://archive.apache.org/dist/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz tar xf zookeeper-3.4.14.tar.gz -C /opt ln -sv /opt/{zookeeper-3.4.14,zookeeper} mkdir -p /opt/zookeeper/{data,logs} 修改配置文件\n配置zookeeper: 单节点配置 集群配置 tee /opt/zookeeper/conf/zoo.cfg\u003c\u003c'EOF' # 集群成员间每隔2000毫秒检查一下集群心跳 2s tickTime=2000 # follower 初次连接leader 的tick次数。例如 10 * 2000 =20s ,follower要在20s内连接上leader 否则被视为超时 initLimit=10 # follower 与leader通讯的tick 次数。例如 5*2000=10s,follower要在10s内与leader完成通讯，否则被视为超时 syncLimit=5 # zookeeper 会定期在内存中生成快照，并定义把快照内容写入 dataDir dataDir=/opt/zookeeper/data dataLogDir=/opt/zookeeper/log # 2181 用于接受客户端连接的主要端口 clientPort=2181 EOF tee /opt/zookeeper/conf/zoo.cfg\u003c\u003c'EOF' # 集群成员间每隔2000毫秒检查一下集群心跳 2s tickTime=2000 # follower 初次连接leader 的tick次数。例如 10 * 2000 =20s ,follower要在20s内连接上leader 否则被视为超时 initLimit=10 # follower 与leader通讯的tick 次数。例如 5*2000=10s,follower要在10s内与leader完成通讯，否则被视为超时 syncLimit=5 # zookeeper 会定期在内存中生成快照，并定义把快照内容写入 dataDir dataDir=/opt/zookeeper/data dataLogDir=/opt/zookeeper/logs # 2181 用于接受客户端连接的主要端口 clientPort=2181 # 2888 用于 ZooKeeper 集群内节点从leader同步数据，只有leader才监听该端口 # 3888 用于 ZooKeeper 集群中的 Leader 选举过程中节点之间的通信 # server.X=host:port:port 这里的X（1～255）必须是一个全局唯一的数字，且需要与 myid文件中的数字相对应 server.1=node-1:2888:3888 server.2=node-2:2888:3888 server.3=node-3:2888:3888 EOF # 为每个节点分配id，在dataDir下myid文件中 # node-1 echo \"1\" \u003e/opt/zookeeper/data/myid # node-2 echo \"2\" \u003e/opt/zookeeper/data/myid # node-3 echo \"3\" \u003e/opt/zookeeper/data/myid ​\n启动服务\ntee /usr/lib/systemd/system/zookeeper.service \u003c\u003c'EOF' [Unit] Description=zookeeper service Documentation=https://zookeeper.apache.org/ After=network.target [Service] Type=forking Environment=JAVA_HOME=/opt/jdk ExecStart=/opt/zookeeper/bin/zkServer.sh start ExecStop=/opt/zookeeper/bin/zkServer.sh stop ExecReload=/opt/zookeeper/bin/zkServer.sh restart Restart=on-failure RestartSec=10 PIDFile=/opt/zookeeper/data/zookeeper_server.pid [Install] WantedBy=multi-user.target EOF Info zookeeper 主进程名为 QuorumPeerMain systemctl daemon-reload systemctl enable zookeeper --now 功能验证\n/opt/zookeeper/bin/zkServer.sh status [root@hdss7-12 bin]# /opt/zookeeper/bin/zkCli.sh -server localhost:2181 [zk: localhost:2181(CONNECTED) 2] ls / ","categories":["kafka"],"description":"\n*如何快速启动一个zookeeper服务*\r\n","excerpt":"\n*如何快速启动一个zookeeper服务*\r\n","ref":"/elk/kafka/zookeeper.html","tags":["zookeeper","kafka"],"title":"部署zookeeper"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\nfilebeat版本：7.17.19\nlogstash版本：7.15.0\nelasticsearch版本： 7.15.0\nkibana版本：7.17.19 、8.8.1\n在elasticsearch 8.8.1版本默认开启了用户认证，因此需要为kibana 服务创建用户并授予权限：\n# 创建用户 ./elasticsearch-8.8.1/bin/elasticsearch-users useradd kibanauser # 把 superuser,kibana_system,kibana_user 角色授权给kubanauser ./elasticsearch-8.8.1/bin/elasticsearch-users roles -a superuser,kibana_system,kibana_user kibanauser 安装部署 下载安装包\nversion=7.17.19 #wget https://repo.huaweicloud.com/kibana/${version}/kibana-${version}-linux-x86_64.tar.gz wget https://artifacts.elastic.co/downloads/kibana/kibana-${version}-linux-x86_64.tar.gz tar xf kibana-${version}-linux-x86_64.tar.gz -C /data/elk/ ln -svf /data/elk/{kibana-${version}-linux-x86_64,kibana} 修改配置文件\n开启认证 未开认证 tee /data/elk/kibana/config/kibana.yml \u003c\u003cEOF server.port: 5601 server.host: \"192.168.0.161\" elasticsearch.hosts: [\"https://192.168.0.161:9200\"] elasticsearch.username: \"kibanauser\" elasticsearch.password: \"kibana\" # 取消自签证书的ca 验证 elasticsearch.ssl.verificationMode: \"none\" #i18n.locale: \"en\" i18n.locale: \"zh-CN\" EOF tee /data/elk/kibana/config/kibana.yml \u003c\u003cEOF server.port: 5601 server.host: \"192.168.0.151\" elasticsearch.hosts: [\"http://192.168.0.151:9200\",\"http://192.168.0.152:9200\",\"http://192.168.0.153:9200\"] i18n.locale: \"zh-CN\" EOF chown -R elasticsearch:elasticsearch /data/elk/kibana* 创建systemd启动文件\ntee /etc/systemd/system/kibana.service \u003c\u003cEOF [Unit] Description= kibana serveice After=network.target [Service] WorkingDirectory=/data/elk/kibana ExecStart=/data/elk/kibana/bin/kibana User=elasticsearch LimitNOFILE=65535 [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable kibana --now systemctl status kibana 检查kibana状态\nhttp://127.0.0.1:5601/status ","categories":["elk"],"description":"\n安装使用kibana\r\n","excerpt":"\n安装使用kibana\r\n","ref":"/elk/kibana.html","tags":["kibana"],"title":"kibana"},{"body":"\nTso(timestamp oracle) 用于标识事务的先后顺序\ntidb数据库架构 tidb server tidb server为无状态服务，在生产中可以启动多个实例并通过LB对外提供统一的连接地址。\n主要功能：\n对外提供mysql 5.7协议的连接服务 （protocol layer）\n负责对sql解析、编译、优化并最终生成执行计划 (parese、compile)\nparse 通过此法分析器将sql 拆借成一个一个的字段，接着进行此法分析。将解析后的内容传递给 compile, 编译器执行逻辑优化和物理优化生成执行计划\n执行sql (executor、distsql、kv)、事务（transaction、kv）\n负责sql语句与tikv 数据结构的转化 。\n表数据与 Key-Value 的映射关系:\nTiDB 会为每个表分配一个表 ID，用 TableID 表示。表 ID 是一个整数，在整个集群内唯一。TiDB 会为表中每行数据分配一个行 ID，用 RowID 表示。行 ID 也是一个整数，在表内唯一。对于行 ID，TiDB 做了一个小优化，如果某个表有整数型的主键，TiDB 会使用主键的值当做这一行数据的行 ID。\n最终将每一行转换成以k:v 格式的数据,其中 tablePrefix 和 recordPrefixSep 都是特定的字符串常量，用于在 Key 空间内区分其他数据\nKey: tablePrefix{TableID}_recordPrefixSep{RowID} Value: [col1, col2, col3, col4] 索引数据和 Key-Value 的映射关系:\n与表数据映射方案类似，TiDB 为表中每个索引分配了一个索引 ID，用 IndexID 表示。\n对于主键和唯一索引，需要根据键值快速定位到对应的 RowID，因此，按照如下规则编码成 (Key, Value) 键值对：\nKey: tablePrefix{tableID}_indexPrefixSep{indexID}_indexedColumnsValue Value: RowID 对于不需要满足唯一性约束的普通二级索引，一个键值可能对应多行，需要根据键值范围查询对应的 RowID。因此，按照如下规则编码成 (Key, Value) 键值对：\nKey: tablePrefix{TableID}_indexPrefixSep{IndexID}_indexedColumnsValue_{RowID} Value: null 执行online DDL (schema load、worker、start job)在同一时刻只有一个tidbserver执行该操作\n垃圾回收（对于update 等多次的修改，数据库会保留历史版本，tidbserver 每隔10分钟(gc lifetime)执行一次历史版本的回收） （gc）\n缓存（memBuffer）默认使用tidb-server 所有内存。\nsql结果 线程缓存 元数据，统计信息 # 每个sql可以使用最大内存 tidb_mem_quota_query # 超过最大缓存时的动作，可以是杀掉sql oom-action 热点小表缓存功能（v6.0版本开始支持对热点表缓存）(cache table)\n64MB 以下的表\n查询频繁，修改极少\n缓存租约时间内，阻塞写操作。\n开启热点小表缓存的表，无法执行ddl 修改。\npd 被称为placement driver 是整个集群的大脑内置etcd,有状态建议部署奇数实现高可用。\n存储每个 TiKV 节点实时的数据分布情况和集群的整体拓扑结构等元数据\n分配全局id和事务id\n负责为集群内各组件分配TSO 时间戳。这些时间戳用于为事务和数据分配时间标记。对于事务提供一个开始TSO 和一个结束TSO 。\nTSO 时间戳由两部分组成：\n物理时间戳：自 1970 年 1 月 1 日以来的 UNIX 时间戳，单位为毫秒 逻辑时间戳：递增计数器，用于需要在一毫秒内使用多个时间戳的情况，或某些事件可能触发时钟进程逆转的情况。在这些情况下，物理时间戳会保持不变，而逻辑时间戳保持递增。该机制可以确保 TSO 时间戳的完整性，确保时间戳始终递增而不会倒退。 根据 TiKV 节点上报的数据动态均衡数据分布,例如：\n对tikv执行leader、热点region 均衡 集群拓扑 缩容 故障恢复 Region merge 根据label，支持对region分布的手动调度。\ntikv实例添加zone 、rack、host等标签后，可以通过pd的调度功能实现基于标签不同级别的高可用。（k8s的node节点添加标签后当pod调度时添加亲和反亲和策略）\n提供 TiDB Dashboard 管控界面\nTiKV 作为分布式存储系统,tikv基于key-value方式存储数据。数据按照key的范围保存到region中，region是tikv的最小存储单元（大小在96MB-144MB之间），每个region默认保证三个副本通过raft协议保证数据的强一致性，而数据的落盘则调用 Facebook 开源的rocksDB实现。有状态服务\nRocksDB RocksDB 是 Facebook 开源的单机 KV 存储引擎，内部使用LSM存储引擎\n数据存储流程： 数据存储时先写入wal 预写日志和 memTable中，当memtable 写入达到指定大小后将该memtable 归档并入immutable 的队列中，从新开启一个新的memtable继续接收新的写入。开启一个新线程将队列中的数据持续写入磁盘持久化\n当队列中等待写入磁盘的队列大于5时，rocksdb开启流量控制降低写入速度。\n磁盘中文件按照压缩级别分为6个级别：\nlevel0 是 immutable的数据复刻，level0数据达到一定大小时rocksdb 执行数据合并压缩（compaction）生成level1文件，依次类推。 每一个文件称为SST（stored string table）\n数据读取流程： rocksdb中存在一个blockcache 的查询内存空间用于缓存最近查询，如果缓存中不存在则从memtable中查找，如果memtable找不到，继续去磁盘中通过二分查找法继续查找\n数据删除流程： 无需要查找到原始数据，只需要执行del操作\nRocksDB列族（column family）: 是rocksdb的数据分片技术，可以按照不同的id分配给不同的列族，如果未指定列族则写入default。一个rocksdb 实例可以对应多个列族，每个列族有自己的memtabl 、SST ，共享wal\nRaft RocksDB提供单机存储能力，为保证数据的高可用引入了raft协议。TiKV 利用 Raft 来做数据复制，每个数据变更都会落地为一条 Raft 日志，通过 Raft 的日志复制功能，将数据安全可靠地同步到复制组的每一个节点中。通过实现 Raft，TiKV 变成了一个分布式的 Key-Value 存储。\nLeader选举。\n在raft协议中节点刚创建时所有节点都处于flollower状态，在指定的 election timeout 时间内如果没有找到 leader ,优先达到 election timeout 的节点将发起选举进入candidate 状态并将term+1，在获得多数投票后该成员状态转为 leader 状态。\n集群关系建立后，leader定期向follower 发送心跳消息，在指定的 heartbeat time interval 内未收到 leader 的心跳信息的follwer 将会进入下一个term 并发起选举进去candidate状态，在获得多数投票后该成员状态转为 leader 状态。\n日志复制\nraft 主从节点之间通过日志复制实现数据的一致性，期间经历以下阶段：\npropose leader将写入请求转成raft log 格式 append 保存日志到 leader 本地 replicate 收到集群内大多数节点日志复制成功的确认消息 commited 收到集群内大多数节点日志复制成功并持久化到本地确认消息 apply 成功完成日志的持久化写入 成员变更（如添加副本、删除副本、转移 Leader 等操作）\nMVCC MVCC (Multi-Version Concurrency Control)假设有这样一种场景：某客户端 A 在写一个 Key，另一个客户端 B 同时在对这个 Key 进行读操作。如果没有数据的多版本控制机制，那么这里的读写操作必然互斥。在分布式场景下，这种情况可能会导致性能问题和死锁问题。有了 MVCC，只要客户端 B 执行的读操作的逻辑时间早于客户端 A，那么客户端 B 就可以在客户端 A 写入的同时正确地读原有的值。即使该 Key 被多个写操作修改过多次，客户端 B 也可以按照其逻辑时间读到旧的值。\nTransaction Transaction提供了分布式事务能力\n两阶段提交\n第一阶段：prewrite 设置锁信息，并将锁在rocksdb的lock列族中（锁信息仅在事务的第一行所在的tikv实例的lock列族中添加锁，其他行所在tikv lock列族引用该锁的地址）。需要修改的信息放入到default 列族中（修改信息大于255byte 放入default 列族中，小于255bytes放入到write列族中）\n第二阶段：commit 从pd中获取tso,在write列族中写入提交信息。并释放lock 列族中的锁信息\ncoprocessor coprocessor算子下推(在每个tikv中执行例如 where 、group by 等操作，过滤后再发给tidb server) tikv 数据读取流程\n用户从tidb-server 发起查询请求，此时从pd 分配一个tso给到该线程，并告知该线程用户需要读取的数据在哪个tikv节点\ngraph TB\rsubgraph TiKV node\rA(Transation)\rB(MVCC)\rC(Raft)\rsubgraph rocksdb\rD[(RocksDB raft)]\rE[(RocksDB kv)]\rend\rend tidb HTAP TiDB 是PingCAP 公司开源的分布式数据库，兼容mysql协议支持水平扩缩容能力，同时实现HTAP（OLTP online transactional processing 和 OLAP online analytical processing）。具备智能选择（CBO自动或人工选择）能力\nOLTP 特征：\n支持试试更新的行存 高并发、一致性要求高 每次操作少量数据 OLAP 特征：\n批量更新列存\n并发数低\n每次操作大量数据\ntiflash： 是TiDB HTAP(OLAP OLTP ) 形态的关键组件，它是 TiKV 的列存扩展。和普通 TiKV 节点不一样的是，在 TiFlash 内部作为learner角色加入到tikv 的region 通过raft log 异步复制leader数据,tiflash不参与tikv数据选举、投票，数据是以列式的形式进行存储，主要的功能是为分析型的场景加速。可以理解为tikv是行存储，tiflash 是列存储。\nMPP架构：每个tiflash 作为worker节点首先过滤符合条件的数据，之后按照 等值连接 将具有符合条件的数据执行数据交换到同一台tiflash中执行聚合计算\ntidb_allow_mpp tidb_enforce_mpp 6.0新功能 热点小表缓存（表的特征：1. 表的数据不大(小于64MB) 2. 只读表或者修改不频繁的表 3.表的访问特别频繁）用于将表缓存在tidb server 的cache table 中提升读性能。\n开启热点小表缓存的表，无法做ddl 的修改\n实现原理：通过租约的方式缓存表内容，在租约时间范围内该表只能读，写处于阻塞状态。租约到期后批量将写内容更新的tikv，并更新tidb server的cache table重新计算租约时间。 相关参数 tidb_table_cache_lease=5\n具体实现：\nalter table t1 cache; placement rules in sql 使用sql的方式精细化调度表在tikv的分布、副本数\n创建一个规则\nCREATE PLACEMENT POLICY P1 PRIMARY_REGION = \"Beijing\" --leader 角色的region位置 REGIONS = \"Beijing, Tokyo, Shanghai, London\" --follower角色的region位置 FOLLOWERS = 4; --follower数量 创建表时引用该规则\nCREATE TABLE T5 (id INT) PLACEMENT POLICY=P1; 内存悲观锁, 事务的锁信息放置在leader 所在tikv的缓存中不会将锁信息复制到follwer节点，减少了磁盘io。当leader所在节点的tivk宕机后锁信息会丢失此时事务执行回滚。\n--在线开启内存悲观锁 set config tikv pessimistic-txn.pipelined='true'; set config tikv pessimistic-txn.in-memory='true'; topsql 基于cpu的使用量查找对应的sql。\n例如：单个tikv 实例负载较高，此时可以使用topsql 功能查找到对应sql\ntidb enterprise manager (TiEM) 可视化管理工具\n部署集群 升级集群 参数管理 组件管理 备份恢复与高可用管理 集群监控与告警 集群日志收集 审计与安全 tidb cloud 是一个DBaas 服务\n1. 关于 TiKV 或 TiDB Server，下列说法不正确的是?\nA. 数据被持久化在 TiKV 的 RocksDB 引擎中 B. 对于老版本数据的回收（GC），是由 TiDB Server 在 TiKV 上完成的 C. 两阶段提交的锁信息被持久化到 TiDB Server 中 D. Region 可以在多个 TiKV 节点上进行调度，但是需要 PD 节点发出调度指令 解题：tidb server 不存储数据\n2.关于关系型数据与KV的转化，下列说法不正确的是？\nA. 如果没有定义主键，key中包含RowID，IndexID和TableID，都是int64类型 B. Table ID在整个集群内唯一 C. 如果定义了主键，那么将使用主键作为RowID D. 不需要为每张表指定主键 3.关于关系型数据与 KV 的转化，下列说法不正确的是？\nA. 如果没有定义主键，key 中包含 RowID、IndexID 和 TableID，都是 int64 类型 B. Table ID 在整个集群内唯一 C. 如果定义了主键，那么将使用主键作为 RowID D. 不需要为每张表指定主键 解释： C 如果定义了主键，并主键为整数型的，TiDB 会使用主键的值当做这一行数据的RowID。\n6.0版本region 默认96MB,当达到144MB 是自动分裂\n所有tidb server都可以接受ddl任务，并将ddl任务放在tikv 队列中。同一时刻只有一个tidb server的worker 模块是ower角色，该worker拥有执行ddl 任务的权利\ntidb server 的gc 间隔默认10m一次\n课程中心\nTiDB 整体架构 | TiDB 文档中心\nTiKV 简介 | TiDB 文档中心\nmysql\u003e set global tidb_enable_top_sql=ON; mysql\u003e show variables like '%top%'; +------------------------------------+------------+ | Variable_name | Value | +------------------------------------+------------+ | ft_stopword_file | (built-in) | | innodb_ft_enable_stopword | ON | | innodb_ft_server_stopword_table | | | innodb_ft_user_stopword_table | | | rpl_stop_slave_timeout | 31536000 | | tidb_enable_top_sql | ON | | tidb_top_sql_max_meta_count | 5000 | | tidb_top_sql_max_time_series_count | 100 | +------------------------------------+------------+ 8 rows in set (0.01 sec) sql 中的锁和事务 SQL-92 标准定义了 4 种隔离级别：读未提交 ()、读已提交 ()、可重复读 ()、串行化 ()。 详见下表：READ UNCOMMITTED、READ COMMITTED、REPEATABLE READ、SERIALIZABLE\n隔离级别 脏写 脏读 模糊读取 幻读 READ UNCOMMITTED 不可能 可能 可能 可能 READ COMMITTED 不可能 不可能 可能 可能 REPEATABLE READ 不可能 不可能 不可能 可能 SERIALIZABLE 不可能 不可能 不可能 不可能 TiDB 语法上支持设置 和 两种隔离级别：READ COMMITTED``REPEATABLE READ\n缓存表的功能\nrocksdb 性能与cpu数量的关系\n导入数据后的一致性校验可以借助Coprocessor 由TiKV来完成\ntidb server 各个组件的功能\nGC life time是无法手工调整的\nSQL语句的解析和编译可以与TSO的获取异步执行\nsql 解析和编译流程，物理编译和逻辑编译\ntidb cloud 的功能\n","categories":["tidb"],"description":"\n平凯数据库认证 TiDB 数据库专员（简称 PCTA）是平凯星辰对于数据库从业者安装部署及日常运维分布式关系型数据库能力的认证，要求数据库从业者熟练掌握 TiDB Open Core 架构原理、安装部署、周边工具等基础知识。\r\n","excerpt":"\n平凯数据库认证 TiDB 数据库专员（简称 PCTA）是平凯星辰对于数据库从业者安装部署及日常运维分布式关系型数据库能力的认证，要求数据库从业者熟练掌握 TiDB Open Core 架构原理、安装部署、周边工具等基础知识。\r\n","ref":"/tidb/pcta.html","tags":["pcta"],"title":"pcta"},{"body":"","categories":["serviceless"],"description":"\n微服务治理\r\n","excerpt":"\n微服务治理\r\n","ref":"/kubernetes/serviceless/appolo.html","tags":["appolo"],"title":"appolo"},{"body":"redis（remote dictionary server） 是开源的k-v数据库，使用内存存储，主要定位为数据缓存。除此之外还可以用于 session 共享，排行榜、计数器，消息队列\n安装部署: 单节点 主从 哨兵 集群 redis部署之单节点\n部署安装\nwget http://download.redis.io/releases/redis-5.0.14.tar.gz tar xf redis-5.0.14.tar.gz \u0026\u0026 cd redis-5.0.14 make \u0026\u0026 make PREFIX=/opt/redis install 二进制文件介绍\n命令 释义 redis-server 启动server redis-cli 客户端工具 redis-benchmark 基准测试工具 redis-check-aof aof 检测和修复工具 redis-check-dump rdb 检测和修复工具 redis-sentinel 启动 sentinel 启动服务\n/opt/redis/bin/redis-server redis部署之主从\n主从架构在单节点基础上提供了数据的冗余能力，提升了redis的读取能力\n部署安装\n参照 redis部署之单节点\n启动服务\n【支持三种配置方式：】\n1. 在启动命令中添加 --SLAVEOF 10.4.7.251 6379\n2. 通过redis-cli 登录到redis 执行 slaveof 10.4.7.251 6379\n3. 在redis.conf配置文件中写入 slaveof 10.4.7.251 6379\n/opt/redis/bin/redis-server --bind 10.4.7.251 --port 6379 --daemonize yes --protected-mode no /opt/redis/bin/redis-server --bind 10.4.7.251 --port 6380 --daemonize yes --protected-mode no --SLAVEOF 10.4.7.251 6379 /opt/redis/bin/redis-server --bind 10.4.7.251 --port 6381 --daemonize yes --protected-mode no --SLAVEOF 10.4.7.251 6379 确认主从关系\n[root@master01 ~]# /opt/redis/bin/redis-cli -h 10.4.7.251 -p 6379 info replication # Replication role:master connected_slaves:2 slave0:ip=10.4.7.251,port=6380,state=online,offset=56,lag=0 slave1:ip=10.4.7.251,port=6381,state=online,offset=56,lag=0 ... 测试主从切换\n首先观察从节点复制偏移量\n/opt/redis/bin/redis-cli -h 10.4.7.251 -p 6380 info replication|grep master_repl_offset /opt/redis/bin/redis-cli -h 10.4.7.251 -p 6381 info replication|grep master_repl_offset 选择一个偏移量最大的从节点，执行切换为主节点。当前环境我们把 10.4.7.251:6381切换为新的主节点。其他节点从新建立与该节点的主从关系\n/opt/redis/bin/redis-cli -h 10.4.7.251 -p 6381 slaveof no one /opt/redis/bin/redis-cli -h 10.4.7.251 -p 6381 role /opt/redis/bin/redis-cli -h 10.4.7.251 -p 6379 SLAVEOF 10.4.7.251 6381 /opt/redis/bin/redis-cli -h 10.4.7.251 -p 6380 SLAVEOF 10.4.7.251 6381 复制原理及相关参数\n复制原理\n1. 从库通过slaveof 指令连接到master，并发起sync请求\r2. master确认后，派生子线程bgsave生成rdb快照，并将快照传递给salve\r3. salve接收到rdb快照后，清空本机rdb,加载主库传递过来的rdb 到内存\r4. 以上完成后，master 会将变更数据缓存到本地 buffer中，按照一定规则传递给slave。允许从节点在短时间与主节点断开。\r复制相关参数 解释 repl-backlog-szie 1M 环形复制积压缓冲区。最小值16k repl-diskless-sync yes 开启无盘复制 repl-diskless-sync-delay 5 无盘复制延迟等待时长 redis部署之哨兵\n哨兵集群是对主从的扩展，提供了主节点故障的自动转移并通知应用方能力。Redis从2.8开始正式 提供了Redis Sentinel\n架构：\n+----+ | M1 | | S1 | +----+ | +----+ | +----+ | R2 |----+----| R3 | | S2 | | S3 | +----+ +----+ Configuration: quorum = 2 部署安装\n参照redis部署之单节点\n启动服务\n参照redis部署之主从\n配置sentinel\nMaster-specific configuration parameters are modified using：SENTINEL SET\nGlobal configuration parameters are modified using SENTINEL CONFIG SET.\ntee redis-sentinel.conf \u003c\u003cEOF daemonize yes # mymaster 为该集群起一个别名。一个sentinel 可以同时监控多个集群，因此使用别名区分 # redis master的地址和端口10.4.7.251 6379 # 2 表示有2个sentinel 判断master失败后才进行主从切换 sentinel monitor mymaster 10.4.7.251 6379 2 # 每个Sentinel节点都要通过定期发送ping命令来判断Redis数据节点和其 # 余Sentinel节点是否可达，如果超过了down-after-milliseconds配置的时间且没 # 有有效的回复，则判定节点不可达 30000ms sentinel down-after-milliseconds mymaster 30000 # 选举出新的主节点后，同时向主节点执行psync的从节点个数 sentinel parallel-syncs mymaster 1 # 故障转移超时时间 180000ms（选举新的主节点+从节点完成主从复制） sentinel failover-timeout mymaster 180000 # 设置sentinel 密码 sentinel auth-pass mymaster 123456 #sentinel auth-pass mymaster 12345 #### 一组sentinel 支持多个主从集群 #sentinel monitor resque 192.168.1.3 6380 2 #sentinel down-after-milliseconds resque 10000 #sentinel failover-timeout resque 180000 #sentinel parallel-syncs resque 5 EOF 启动sentinel【默认监听26379】\n【sentinel 的启动方式有2种：】\nredis-sentinel /path/to/sentinel.conf\nredis-server /path/to/sentinel.conf --sentinel\nmkdir /tmp/{26379,26380,26381} /opt/redis/bin/redis-server redis-sentinel.conf --sentinel \\ --bind 10.4.7.251 \\ --port 26379 \\ --dir /tmp/26379 \\ --logfile /tmp/26379.log /opt/redis/bin/redis-server redis-sentinel.conf --sentinel \\ --bind 10.4.7.251 \\ --port 26380 \\ --dir /tmp/26380 \\ --logfile /tmp/26380.log /opt/redis/bin/redis-server redis-sentinel.conf --sentinel \\ --bind 10.4.7.251 \\ --port 26381 \\ --dir /tmp/26381 \\ --logfile /tmp/26381.log /opt/redis-4.2/bin/redis-server ./redis-4.0.2/redis-sentinel17778.conf --port 17778 --sentinel --protected-mode no --logfile /tmp/17778.log /opt/redis-4.2/bin/redis-server ./redis-4.0.2/redis-sentinel17779.conf --port 17779 --sentinel --protected-mode no --logfile /tmp/17779.log /opt/redis-4.2/bin/redis-server ./redis-4.0.2/redis-sentinel17777.conf --port 17777 --sentinel --protected-mode no --logfile /tmp/17777.log 验证状态\n[root@master01 ~]# /opt/redis/bin/redis-cli -p 26379 -h 10.4.7.251 10.4.7.251:26379\u003e info sentinel # Sentinel sentinel_masters:1 sentinel_tilt:0 sentinel_running_scripts:0 sentinel_scripts_queue_length:0 sentinel_simulate_failure_flags:0 master0:name=mymaster,status=ok,address=10.4.7.251:6379,slaves=2,sentinels=2 #ok 状态正常 #sdown 主观下线 #odown 客观下线 #Redis Sentinel 有两种不同的宕机概念，一种称为 主观上为 Down 条件 （SDOWN），并且是 local 添加到给定的 Sentinel 实例。另一个称为客观关闭条件 （ODOWN），当有足够的 Sentinel（至少 number 配置为被监控 master 的参数）具有 SDOWN 条件 验证主宕机\n/opt/redis/bin/redis-cli -p 6379 -h 10.4.7.251 shutdown sentinel 节点30s 后标记节点主观（Subjectively）下线\nsentinel1_1 | 1:X 15 Apr 2024 08:40:26.982 # +sdown master mymaster 10.4.7.251 6379 sentinel2_1 | 1:X 15 Apr 2024 08:40:26.983 # +sdown master mymaster 10.4.7.251 6379 sentinel3_1 | 1:X 15 Apr 2024 08:40:27.032 # +sdown master mymaster 10.4.7.251 6379 等待30s 后选出主节….\n重新启动宕机的节点，会自动加入到该主从中\n/opt/redis/bin/redis-server --bind 10.4.7.251 --port 6379 --daemonize yes 命令 解释 sentinel master mymaster 查询主节点状态 sentinel slaves mymaster 查询从节点状态。或者命令 sentinel replicas mymaster sentinel sentinels mymaster 查询除自己外的sentinel节点信息 sentinel get-master--by-name mymaster 查询主节点的ip:port sentinel ckquorum mymaster 检查哨兵集群是否满足切换需求 sentinel flushconfig 强制将sentinel 配置写入磁盘 sentinel failover mymaster 强制主从切换 sentinel info-cache 返回master-slave的缓存信息 sentinel config get * 获取sentinel 配置信息 \u003e=6.2 sentinel config set \u003ckey\u003e \u003cvalue\u003e 动态设置sentinel 配置 \u003e=6.2 redis部署之集群\nredis cluster 是redis 的分布式解决方案，在3.0后正式推出。其主要解决单节点写能力和存储能力受限的问题。\nRedis集群相对单机在功能上存在一些限制，需要开发人员提前了解， 在使用时做好规避。限制如下：\n1）key批量操作支持有限。如mset、mget，目前只支持具有相同slot值的 key执行批量操作。对于映射为不同slot值的key由于执行mset、mget等操作可能存在于多个节点上因此不被支持。\n2）key事务操作支持有限。同理只支持多key在同一节点上的事务操作，当多个key分布在不同的节点上时无法使用事务功能。\n3）key作为数据分区的最小粒度，因此不能将一个大的键值对象如 hash、list等映射到不同的节点。\n4）不支持多数据库空间。单机下的Redis可以支持16个数据库，集群模式下只能使用一个数据库空间，即db0。\n5）复制结构只支持一层，从节点只能复制主节点，不支持嵌套树状复制结构。\n部署方法一：\n部署安装\n参照 redis部署之单节点\n启动服务\nfor port in {6379,6380,6381,7379,7380,7381};do mkdir -p /opt/redis_${port} /opt/redis/bin/redis-server \\ --bind 0.0.0.0 --port ${port} \\ --daemonize yes \\ --cluster-enabled yes \\ --cluster-config-file /opt/redis_${port}/nodes.conf \\ --logfile /opt/redis_${port}/redis.log \\ --cluster-node-timeout 1500 \\ --requirepass pytc@2024 done 创建集群\necho yes |/opt/redis/bin/redis-cli -a pytc@2024 --cluster create \\ --cluster-replicas 1 \\ 192.168.0.161:6379 192.168.0.161:6380 192.168.0.161:6381 192.168.0.161:7379 192.168.0.161:7380 192.168.0.161:7381 部署方法二：\n部署安装\n参照 redis部署之单节点\n启动服务\n/opt/redis/bin/redis-server --bind 10.4.7.251 --port 6379 --daemonize yes --cluster-enabled yes --cluster-config-file nodes-6379.conf --cluster-node-timeout 1500 /opt/redis/bin/redis-server --bind 10.4.7.251 --port 6380 --daemonize yes --cluster-enabled yes --cluster-config-file nodes-6380.conf --cluster-node-timeout 1500 /opt/redis/bin/redis-server --bind 10.4.7.251 --port 6381 --daemonize yes --cluster-enabled yes --cluster-config-file nodes-6381.conf --cluster-node-timeout 1500 安装ruby环境\nrpm -e ruby --nodeps wget https://github.com/postmodern/ruby-install/archive/refs/tags/v0.8.5.tar.gz tar xf v0.8.5.tar.gz cd ruby-install-0.8.5/ make install ruby-install --system ruby 2.6.10 安装redis-trib 工具\n# gem 是ruby的包管理工具，可以类比python的pip yum install rubygems redis-trib -y gem sources gem sources --remove https://rubygems.org/ gem sources -a https://mirrors.aliyun.com/rubygems/ gem install redis 创建集群\nredis-trib create --replicas 0 \\ 10.4.7.251:6379 10.4.7.251:6380 10.4.7.251:6381 部署方法三：\n部署安装\n参照 redis部署之单节点\n启动服务\n/opt/redis/bin/redis-server --bind 10.4.7.251 --port 6379 --daemonize yes --cluster-enabled yes --cluster-config-file nodes-6379.conf --cluster-node-timeout 1500 /opt/redis/bin/redis-server --bind 10.4.7.251 --port 6380 --daemonize yes --cluster-enabled yes --cluster-config-file nodes-6380.conf --cluster-node-timeout 1500 /opt/redis/bin/redis-server --bind 10.4.7.251 --port 6381 --daemonize yes --cluster-enabled yes --cluster-config-file nodes-6381.conf --cluster-node-timeout 1500 加入集群\n/opt/redis/bin/redis-cli -h 10.4.7.251 cluster meet 10.4.7.251 6380 /opt/redis/bin/redis-cli -h 10.4.7.251 cluster meet 10.4.7.251 6381 分配槽位\n/opt/redis/bin/redis-cli -h 10.4.7.251 cluster addslots {0..5460} /opt/redis/bin/redis-cli -h 10.4.7.251 -p 6380 cluster addslots {5461..10921} /opt/redis/bin/redis-cli -h 10.4.7.251 -p 6381 cluster addslots {10922..16383} 建立主从关系\n新增分片\nredis-cli --cluster add-node 127.0.0.1:7006 127.0.0.1:7000 $ redis-cli -p 7000 cluster nodes 46a768cfeadb9d2aee91ddd882433a1798f53271 127.0.0.1:7006@17006 master - 0 1616754504000 0 connected 1f2bc068c7ccc9e408161bd51b695a9a47b890b2 127.0.0.1:7003@17003 slave a138f48fe038b93ea2e186e7a5962fb1fa6e34fa 0 1616754504551 3 connected 5b4e4be56158cf6103ffa3035024a8d820337973 127.0.0.1:7001@17001 master - 0 1616754505584 2 connected 5461-10922 a138f48fe038b93ea2e186e7a5962fb1fa6e34fa 127.0.0.1:7002@17002 master - 0 1616754505000 3 connected 10923-16383 71e078dab649166dcbbcec51520742bc7a5c1992 127.0.0.1:7005@17005 slave 5b4e4be56158cf6103ffa3035024a8d820337973 0 1616754505584 2 connected f224ecabedf39d1fffb34fb6c1683f8252f3b7dc 127.0.0.1:7000@17000 myself,master - 0 1616754502000 1 connected 0-5460 04d71d5eb200353713da475c5c4f0a4253295aa4 127.0.0.1:7004@17004 slave f224ecabedf39d1fffb34fb6c1683f8252f3b7dc 0 1616754505896 1 connect 添加从节点\n$ redis-cli -p 7000 --cluster add-node 127.0.0.1:7007 127.0.0.1:7000 --cluster-slave --cluster-master-id 46a768cfeadb9d2aee91ddd882433a1798f53271 为新分片分配slot\n$ redis-cli -p 7000 --cluster reshard 127.0.0.1:7000 .... .... .... How many slots do you want to move (from 1 to 16384)? 4096 What is the receiving node ID? 46a768cfeadb9d2aee91ddd882433a1798f53271 Please enter all the source node IDs. Type 'all' to use all the nodes as source nodes for the hash slots. Type 'done' once you entered all the source nodes IDs. Source node #1: all Ready to move 4096 slots. Source nodes: M: f224ecabedf39d1fffb34fb6c1683f8252f3b7dc 127.0.0.1:7000 slots:[0-5460] (5461 slots) master 1 additional replica(s) M: 5b4e4be56158cf6103ffa3035024a8d820337973 127.0.0.1:7001 slots:[5461-10922] (5462 slots) master 1 additional replica(s) M: a138f48fe038b93ea2e186e7a5962fb1fa6e34fa 127.0.0.1:7002 slots:[10923-16383] (5461 slots) master 1 additional replica(s) Destination node: M: 46a768cfeadb9d2aee91ddd882433a1798f53271 127.0.0.1:7006 slots: (0 slots) master 1 additional replica(s) Resharding plan: Moving slot 5461 from 5b4e4be56158cf6103ffa3035024a8d820337973 Moving slot 5462 from 5b4e4be56158cf6103ffa3035024a8d820337973 Do you want to proceed with the proposed reshard plan (yes/no)? Moving slot 5461 from 127.0.0.1:7001 to 127.0.0.1:7006: Moving slot 5462 from 127.0.0.1:7001 to 127.0.0.1:7006: Moving slot 5463 from 127.0.0.1:7001 to 127.0.0.1:7006: ","categories":["redis"],"description":"\nredis部署安装\r\n","excerpt":"\nredis部署安装\r\n","ref":"/redis/install.html","tags":["安装部署"],"title":"安装部署"},{"body":"部署安装 实例名称 mysql-01 示例使用全局变量引用\nexport clsName='mysql-01' 添加用户\nuseradd -s /sbin/nologin -u 3306 -M mysql 安装mysql依赖\n# libaio提供 Linux 原生异步 I/O（AIO）支持，允许程序发起非阻塞的磁盘读写操作，提高高并发场景下的 I/O 性能 yum install -y libaio libaio-devel ncurses ncurses-devel cpanminus mysql软件安装，示例安装在/opt/mysql目录下\n软件安装: 二进制安装 编译安装 软件下载\nwget https://downloads.mysql.com/archives/get/p/23/file/mysql-5.6.40-linux-glibc2.12-x86_64.tar.gz tar xf mysql-5.6.40-linux-glibc2.12-x86_64.tar.gz -C /opt 安装到/opt/mysql下\nln -svf /opt/{mysql-5.6.40-linux-glibc2.12-x86_64,mysql} ln -svf /opt/mysql-5.6.40-linux-glibc2.12-x86_64/bin/mysql /usr/bin/mysql 软件下载\nwget https://downloads.mysql.com/archives/get/p/23/file/mysql-5.6.40.tar.gz tar xf mysql-5.6.40.tar.gz -C /opt cd /opt/mysql-5.6.40 安装编译依赖\nyum install -y cmake libaio-devel gcc-c++ perl-devel cpanminus ncurses-devel cmake . -DCMAKE_INSTALL_PREFIX=/opt/mysql \\ -DMYSQL_DATADIR=/data/instances/${clsName}/data \\ -DMYSQL_UNIX_ADDR=/data/instances/${clsName}/mysql.sock \\ -DDEFAULT_CHARSET=utf8 \\ -DDEFAULT_COLLATION=utf8_general_ci \\ -DWITH_EXTRA_CHARSETs=all \\ -DWITH_INNOBASE_STORAGE_ENGINE=1 \\ -DWITH_FEDERATED_STORAGE_ENGINE=1 \\ -DWITH_BLACKHOLE_STORAGE_ENGINE=1 \\ -DWITH_EXAMPLE_STORAGE_ENGINE=1 \\ -DWITH_ZLIB=bundled \\ -DWITH_SSL=bundled \\ -DENABLED_LOCAL_INFILE=1 \\ -DWITH_EMBEDDED_SERVER=1 \\ -DENABLE_DOWNLOADS=1 \\ -DWITH_DEBUG=0 make -j 4 \u0026\u0026 make install ln -svf /opt/mysql/bin/mysql /usr/bin/mysql 初始化\n# 构建目录结构 mkdir -p /data/instances/${clsName}/{data,binlog,logs,relay_log,conf} chown -R mysql:mysql /data/instances/${clsName} # 初始化数据库 /opt/mysql/scripts/mysql_install_db \\ --basedir=/opt/mysql/ \\ --datadir=/data/instances/${clsName}/data \\ --explicit_defaults_for_timestamp \\ --user=mysql Details [root@seagullcore01-uat-s2 ~]# /opt/mysql/scripts/mysql_install_db \\ \u003e --basedir=/opt/mysql/ \\ \u003e --datadir=/data/instances/${clsName}/data \\ \u003e --explicit_defaults_for_timestamp \\ \u003e --user=mysql Installing MySQL system tables...2025-07-28 11:23:40 0 [Note] Ignoring --secure-file-priv value as server is running with --bootstrap. 2025-07-28 11:23:40 0 [Note] /opt/mysql//bin/mysqld (mysqld 5.6.40) starting as process 29771 ... 2025-07-28 11:23:40 29771 [Note] InnoDB: Using atomics to ref count buffer pool pages 2025-07-28 11:23:40 29771 [Note] InnoDB: The InnoDB memory heap is disabled 2025-07-28 11:23:40 29771 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins 2025-07-28 11:23:40 29771 [Note] InnoDB: Memory barrier is not used 2025-07-28 11:23:40 29771 [Note] InnoDB: Compressed tables use zlib 1.2.3 2025-07-28 11:23:40 29771 [Note] InnoDB: Using Linux native AIO 2025-07-28 11:23:40 29771 [Note] InnoDB: Using CPU crc32 instructions 2025-07-28 11:23:40 29771 [Note] InnoDB: Initializing buffer pool, size = 128.0M 2025-07-28 11:23:40 29771 [Note] InnoDB: Completed initialization of buffer pool 2025-07-28 11:23:40 29771 [Note] InnoDB: The first specified data file ./ibdata1 did not exist: a new database to be created! 2025-07-28 11:23:40 29771 [Note] InnoDB: Setting file ./ibdata1 size to 12 MB 2025-07-28 11:23:40 29771 [Note] InnoDB: Database physically writes the file full: wait... 2025-07-28 11:23:40 29771 [Note] InnoDB: Setting log file ./ib_logfile101 size to 48 MB 2025-07-28 11:23:40 29771 [Note] InnoDB: Setting log file ./ib_logfile1 size to 48 MB 2025-07-28 11:23:40 29771 [Note] InnoDB: Renaming log file ./ib_logfile101 to ./ib_logfile0 2025-07-28 11:23:40 29771 [Warning] InnoDB: New log files created, LSN=45781 2025-07-28 11:23:40 29771 [Note] InnoDB: Doublewrite buffer not found: creating new 2025-07-28 11:23:40 29771 [Note] InnoDB: Doublewrite buffer created 2025-07-28 11:23:40 29771 [Note] InnoDB: 128 rollback segment(s) are active. 2025-07-28 11:23:40 29771 [Warning] InnoDB: Creating foreign key constraint system tables. 2025-07-28 11:23:40 29771 [Note] InnoDB: Foreign key constraint system tables created 2025-07-28 11:23:40 29771 [Note] InnoDB: Creating tablespace and datafile system tables. 2025-07-28 11:23:40 29771 [Note] InnoDB: Tablespace and datafile system tables created. 2025-07-28 11:23:40 29771 [Note] InnoDB: Waiting for purge to start 2025-07-28 11:23:40 29771 [Note] InnoDB: 5.6.40 started; log sequence number 0 2025-07-28 11:23:41 29771 [Note] Binlog end 2025-07-28 11:23:41 29771 [Note] InnoDB: FTS optimize thread exiting. 2025-07-28 11:23:41 29771 [Note] InnoDB: Starting shutdown... 2025-07-28 11:23:42 29771 [Note] InnoDB: Shutdown completed; log sequence number 1625977 OK Filling help tables...2025-07-28 11:23:42 0 [Note] Ignoring --secure-file-priv value as server is running with --bootstrap. 2025-07-28 11:23:42 0 [Note] /opt/mysql//bin/mysqld (mysqld 5.6.40) starting as process 29794 ... 2025-07-28 11:23:42 29794 [Note] InnoDB: Using atomics to ref count buffer pool pages 2025-07-28 11:23:42 29794 [Note] InnoDB: The InnoDB memory heap is disabled 2025-07-28 11:23:42 29794 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins 2025-07-28 11:23:42 29794 [Note] InnoDB: Memory barrier is not used 2025-07-28 11:23:42 29794 [Note] InnoDB: Compressed tables use zlib 1.2.3 2025-07-28 11:23:42 29794 [Note] InnoDB: Using Linux native AIO 2025-07-28 11:23:42 29794 [Note] InnoDB: Using CPU crc32 instructions 2025-07-28 11:23:42 29794 [Note] InnoDB: Initializing buffer pool, size = 128.0M 2025-07-28 11:23:42 29794 [Note] InnoDB: Completed initialization of buffer pool 2025-07-28 11:23:42 29794 [Note] InnoDB: Highest supported file format is Barracuda. 2025-07-28 11:23:42 29794 [Note] InnoDB: 128 rollback segment(s) are active. 2025-07-28 11:23:42 29794 [Note] InnoDB: Waiting for purge to start 2025-07-28 11:23:43 29794 [Note] InnoDB: 5.6.40 started; log sequence number 1625977 2025-07-28 11:23:43 29794 [Note] Binlog end 2025-07-28 11:23:43 29794 [Note] InnoDB: FTS optimize thread exiting. 2025-07-28 11:23:43 29794 [Note] InnoDB: Starting shutdown... 2025-07-28 11:23:44 29794 [Note] InnoDB: Shutdown completed; log sequence number 1625987 OK To start mysqld at boot time you have to copy support-files/mysql.server to the right place for your system PLEASE REMEMBER TO SET A PASSWORD FOR THE MySQL root USER ! To do so, start the server, then issue the following commands: /opt/mysql//bin/mysqladmin -u root password 'new-password' /opt/mysql//bin/mysqladmin -u root -h seagullcore01-uat-s2 password 'new-password' Alternatively you can run: /opt/mysql//bin/mysql_secure_installation which will also give you the option of removing the test databases and anonymous user created by default. This is strongly recommended for production servers. See the manual for more instructions. You can start the MySQL daemon with: cd . ; /opt/mysql//bin/mysqld_safe \u0026 You can test the MySQL daemon with mysql-test-run.pl cd mysql-test ; perl mysql-test-run.pl Please report any problems at http://bugs.mysql.com/ The latest information about MySQL is available on the web at http://www.mysql.com Support MySQL by buying support/licenses at http://shop.mysql.com New default config file was created as /opt/mysql//my.cnf and will be used by default by the server when you start it. You may edit this file to change server settings WARNING: Default config file /etc/my.cnf exists on the system This file will be read by default by the MySQL server If you do not want to use this, either remove it, or use the --defaults-file argument to mysqld_safe when starting the server 启动服务\ncat \u003e/data/instances/${clsName}/conf/my.cnf\u003c\u003cEOF [mysqld] performance_schema=ON server_id=1921680152 port=3306 character-set-server=utf8mb4 basedir=/opt/mysql datadir=/data/instances/${clsName}/data/ pid-file=/data/instances/${clsName}/mysql.pid socket=/data/instances/${clsName}/mysql.sock log_error=/data/instances/${clsName}/logs/mysql-error.log slow_query_log_file=/data/instances/${clsName}/logs/mysql_slow_query.log slow_query_log=on long_query_time=1 binlog_format=row log-bin=/data/instances/${clsName}/binlog/log_bin log-bin-index=/data/instances/${clsName}/binlog/binlog.index #gtid-mode=on #enforce-gtid-consistency=on relay_log=/data/instances/${clsName}/relay_log relay_log_index=/data/instances/${clsName}/relaylog/relay-bin.index relay_log_recovery=on default_authentication_plugin=mysql_native_password master_info_repository=table relay_log_info_repository=table EOF tee /usr/lib/systemd/system/mysql.service \u003c\u003c'EOF' [Unit] Description=mysql service https://dev.mysql.com/doc/refman/5.6/en After=network.target [Service] ExecStart=/opt/mysql/bin/mysqld_safe \\ --defaults-file=/data/instances/mysql-01/conf/my.cnf \\ --pid-file=/data/instances/mysql-01/mysql.pid ExecReload=/bin/kill -HUP $MAINPID User=mysql # 设置最大文件描述符 LimitNOFILE=1024 # 设置CPU使用率限制为50% CPUQuota=50% # 设置内存限制为1G MemoryLimit=1G [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable mysql --now systemctl status mysql 创建用户\n--- 修改root密码 ALTER USER 'root'@'localhost' identified by 'li\u003c)\u003c#jyi9S)2024'; --- 超管用户 create user 'root'@'%' identified by 'li\u003c)\u003c#jyi9S)2024'; grant all privileges on *.* to 'root'@'%' WITH GRANT OPTION ; --- 管理用户 create user 'rdsAdmin'@'%' identified by 'li\u003c)\u003c#jyi9S)2024'; grant all privileges on *.* to 'rdsAdmin'@'%' WITH GRANT OPTION ; --- 复制用户 create user 'rdsRpl'@'%' identified by 'li\u003c)\u003c#jyi9S)2024'; GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'rdsRpl'@'%'; flush privileges; ","categories":["mysql"],"description":"mysql 5.6版本二进制安装和编译安装\n","excerpt":"mysql 5.6版本二进制安装和编译安装\n","ref":"/mysql/install5.6.html","tags":["安装mysql"],"title":"install mysql5.6"},{"body":" 内容概要 什么是rabbitMQ 安装单机版本rabbitMQ python 编写demo RabbitMQ 是一个开源的消息中间件（Message Broker），实现了 AMQP（Advanced Message Queuing Protocol） 协议，同时也支持 STOMP、MQTT 等协议。它用于在分布式系统中异步传递消息，解决系统间解耦、削峰、异步处理等问题。\nRabbitMQ 的核心概念\n概念 说明 类比 Producer 生产者发送消息到 RabbitMQ 的应用程序。 寄快递的人 Consumer 消费者从队列中接收并处理消息的程序。 收快递的人 Queue 存储消息的缓冲区，消息在这里等待被消费。 快递柜 Exchange 接收生产者消息，根据规则路由到一个或多个队列。 快递分拣中心 Routing Key 生产者发送消息时指定的“地址”，用于匹配绑定的规则。 快递上的地址 Binding 定义 Exchange 和 Queue 之间的关系，包含路由规则。 快递分拣规则 Channel 轻量级连接，复用 TCP 连接，减少资源消耗。 快递员的送货路线 Virtual Host 逻辑隔离的命名空间，类似数据库的 schema。 不同快递公司的仓库 常见 Exchange 类型\n类型 描述 场景 Direct 精确匹配 routing key。 日志级别分发（error、info、warn）。 Fanout 广播消息到所有绑定队列，忽略 routing key。 群发通知。 Topic 模糊匹配 routing key（支持通配符 * 和 #）。 多维度日志分类。 Headers 基于消息头（headers）匹配，少用。 复杂规则匹配。 单机部署 软件 版本 端口 erlang 26.2 rabbitMQ 3.12.10、4.1.3 提供给客户端连接 5672 浏览器管理15672 集群内部通讯 25672 第一步：安装erlang依赖\nerlang 环境可以将其理解为运行java程序时的jre 环境\nyum install make gcc gcc-c++ perl ncurses-devel git clone https://github.com/erlang/otp.git \u0026\u0026 cd otp/ git checkout maint-26 git reset --hard OTP-26.2 ./configure --prefix /opt/erlang make -j 8 make install # 26.2 是erlang软件的版本，这里看到的14.2是erlang内部运行时系统的版本 [root@seagullcore01-uat-s2 rabbitmq_server-3.12.10]# /opt/erlang/bin/erl -version Erlang (SMP,ASYNC_THREADS) (BEAM) emulator version 14.2 export PATH=/opt/erlang/bin:$PATH 第二步：安装rabbitMQ\nwget https://mirrors.huaweicloud.com/rabbitmq-server/v3.12.10/rabbitmq-server-generic-unix-3.12.10.tar.xz tar xf rabbitmq-server-generic-unix-3.12.10.tar.xz -C /opt ln -svf /opt/rabbitmq_server-3.12.10 /opt/rabbitmq 启动\n管理插件 3.12.10版本没有自带管理页面，因此需要提前启用该plugins\n/opt/rabbitmq/sbin/rabbitmq-plugins enable rabbitmq_management # 启动rabbitMQ # 日志文件：/opt/rabbitmq/var/log/rabbitmq/rabbit@seagullcore01-uat-s2.log # 数据文件：/opt/rabbitmq/var/lib/rabbitmq/mnesia/rabbit@seagullcore01-uat-s2 /opt/rabbitmq/sbin/rabbitmq-server -detached # 查看rabbtiMQ状态 /opt/rabbitmq/sbin/rabbitmqctl status # 停止rabbitMQ # /opt/rabbitmq/sbin/rabbitmqctl shutdown 创建管理员用户admin\n# /opt/rabbitmq//sbin/rabbitmqctl delete_user admin /opt/rabbitmq//sbin/rabbitmqctl add_user admin /opt/rabbitmq//sbin/rabbitmqctl change_password admin admin /opt/rabbitmq//sbin/rabbitmqctl set_user_tags admin administrator # 授予 / vhost的所有权限，给admin 用户 /opt/rabbitmq/sbin/rabbitmqctl set_permissions -p / admin \".*\" \".*\" \".*\" /opt/rabbitmq/sbin/rabbitmqctl set_permissions -p test admin \".*\" \".*\" \".*\" ]# /opt/rabbitmq/sbin/rabbitmqctl list_users Listing users ... user tags admin [administrator] guest [administrator] ]# /opt/rabbitmq/sbin/rabbitmqctl list_permissions -p / Listing permissions for vhost \"/\" ... user configure write read guest .* .* .* admin .* .* .* 配置rabbitmq 通过上面启动示例，rabbitmq 启动时并没有指定配置文件。如果我们希望做一些定制化的参数修改可以在/opt/rabbitmq/etc/rabbitmq/ 目录中创建 rabbitmq.config配置文件\n查看运行时配置: /opt/rabbitmq/sbin/rabbitmq-diagnostics environment\n修改端口和内存限制\ncat \u003e/opt/rabbitmq/etc/rabbitmq/rabbitmq.conf \u003c\u003c'EOF' # 网络配置 listeners.tcp.default = 0.0.0.0:5672 # 资源限制 ## 允许使用总内存的40% #vm_memory_high_watermark.relative = 0.4 ## 允许使用1G内存 vm_memory_high_watermark.absolute = 1073741824 EOF 修改数据文件和日志文件路径\nmkdir -p /data/rabbitmq/{mnesia,logs} #chown -R rabbitmq:rabbitmq /data/rabbitmq cat \u003e/opt/rabbitmq/etc/rabbitmq/rabbitmq-env.conf\u003c\u003c'EOF' # 数据目录（Mnesia 数据库、消息存储等） RABBITMQ_MNESIA_BASE=/data/rabbitmq/mnesia # 日志目录 RABBITMQ_LOG_BASE=/data/rabbitmq/logs EOF 声明交换机 exchange.declare 声明队列 queue.declare 队列绑定到交换机 queue.bind ","categories":["rabbitmq"],"description":"","excerpt":" 内容概要 什么是rabbitMQ 安装单机版本rabbitMQ python 编写demo RabbitMQ 是一个开源的消息中间件（Message Broker），实现了 AMQP（Advanced Message Queuing Protocol） 协议，同时也支持 STOMP、MQTT 等协议。它用于在分布式系统中异步传递消息，解决系统间解耦、削峰、异步处理等问题。\nRabbitMQ 的核 …","ref":"/rabbitmq/install.html","tags":["安装rabbitmq"],"title":"快速开始"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\nfilebeat版本：7.17.29\nelasticsearch版本： 7.15.0\n文档\n下载\ndocker镜像\r下载安装包\nversion=7.17.29 wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-${version}-linux-x86_64.tar.gz #wget https://repo.huaweicloud.com/filebeat/${version}/filebeat-${version}-linux-x86_64.tar.gz tar xf filebeat-${version}-linux-x86_64.tar.gz -C /data/elk ln -svf /data/elk/filebeat-${version}-linux-x86_64 /data/elk/filebeat 修改配置文件 秘钥的保存和引用 参考文档\n通过 filebeat keystore xxx 创建和管理秘钥\ncat \u003e/data/elk/filebeat/filebeat.yml\u003c\u003c'EOF' filebeat.inputs: - type: log id: log paths: - /var/log/dmesg - /var/log/messages - /var/log/secure - /var/log/cron - /var/log/audit/audit.log - /var/log/yum.log output.elasticsearch: hosts: [\"192.168.0.114:9200\"] indices: - index: \"warning-%{[agent.version]}-%{+yyyy.MM.dd}\" when: contains: message: \"WARN\" - index: \"error-%{[agent.version]}-%{+yyyy.MM.dd}\" when: contains: message: \"ERR\" - index: \"info-%{[agent.version]}-%{+yyyy.MM.dd}\" when: contains: message: \"INFO\" # 默认索引 - index: \"default-%{[agent.version]}-%{+yyyy.MM.dd}\" EOF 创建systemd启动文件\n类型 描述 默认值 配置参数 home Home of the Filebeat installation. path.home bin The location for the binary files. {path.home}/bin config The location for configuration files. {path.home} path.config data The location for persistent data files. {path.home}/data path.data logs The location for the logs created by Filebeat. {path.home}/logs path.logs tee /etc/systemd/system/filebeat.service \u003c\u003c'EOF' [Unit] Description=Filebeat sends log files to Logstash or directly to Elasticsearch. Documentation=https://www.elastic.co/products/beats/filebeat Wants=network-online.target After=network-online.target [Service] Environment=\"BEAT_LOG_OPTS=\" Environment=\"BEAT_CONFIG_OPTS=-c /data/elk/filebeat/filebeat.yml\" Environment=\"BEAT_PATH_OPTS=--path.home /data/elk/filebeat --path.config /data/elk/filebeat --path.data /data/elk/filebeat/data --path.logs /data/elk/filebeat/logs\" ExecStart=/data/elk/filebeat/filebeat --environment systemd $BEAT_LOG_OPTS $BEAT_CONFIG_OPTS $BEAT_PATH_OPTS Restart=always [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable filebeat --now systemctl status filebeat ","categories":["ELK"],"description":"\n*filebeat安装部署，配置详情请参阅其他文档*\r\n","excerpt":"\n*filebeat安装部署，配置详情请参阅其他文档*\r\n","ref":"/elk/filebeat/filebeat.html","tags":["filebeat"],"title":"filebeat_install"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\nfilebeat版本：7.17.29\nelasticsearch版本： 7.15.0\n顶级配置段中支持的常用全部配置字段\ngraph LR\rsubgraph \"【主配置文件】\"\rA\u003efilebeat.yml]\rend\rA --\u003e name\rA --\u003e max_procs\rA --\u003e tags\rA --\u003e A1[\"fields|fields_under_root\"]\rA --\u003e processors name: \"my-shipper\" # 静态定义agent.name, 默认值为操作系统的hostname max_procs: 1 # 可用最大cpu，默认所有cpu tags: - \"dev\" - \"os\" fields: center: BeiJing fields_under_root: true processors: {} filebeat.inputs: - type: udp max_message_size: 10MiB host: \"0.0.0.0:9000\" rfc5424_parser.enabled: true fields: log_topic: os output.kafka: hosts: [\"192.168.0.151:9092\",\"192.168.0.152:9092\",\"192.168.0.153:9092\"] topic: '%{[fields.log_topic]}' required_acks: 1 compression: gzip max_message_bytes: 1000000 ","categories":["ELK"],"description":"\nfilebeat配置文件之通用配置\r\n","excerpt":"\nfilebeat配置文件之通用配置\r\n","ref":"/elk/filebeat/config/general.html","tags":["通用配置","filebeat"],"title":"通用配置"},{"body":"\nCoreDNS是由Go语言编写的开源DNS服务，通过大量插件实现了复杂的功能。因此在实现上不同于BIND，Knot，PowerDNS，Unbound 等dns软件\n基本概念 A/AAAA 记录\nSRV 记录\nPTR 记录 是dns的反向解析，用于将ip地址解析为域名。例如存在一条A记录： 172.168.0.1 kubernetes.default.svc.cluster.local 使用PTR可以通过 172.168.0.1 找到对应的域名为kubernetes.default.svc.in-addr.arpa 域名。 PTR 记录存储在 DNS 的 .arpa 顶级域中，.arpa 是一个用于管理网络基础设施的域，是为互联网定义的第一个顶级域名。（“arpa”这个名字可以追溯到互联网的早期：它的名字来源于高级研究计划署 (ARPA)，它创建了互联网的重要前身 ARPANET。） 二级域名 .in-addr 代表了ipv4，因此表示为 .in-addr.arpa 。对于ipv6 对应 .ip6.arpa\n安装部署 容器 二进制 docker run --name coredns -p53:53 coredns/coredns:latest 下载\nwget https://github.com/coredns/coredns/releases/download/v1.10.0/coredns_1.10.0_linux_amd64.tgz tar xf coredns_1.10.0_linux_amd64.tgz -C /usr/bin/ 启动\n[root@lavm-ioreaqndwv ~]# coredns -dns.port1053 .:1053 CoreDNS-1.10.0 linux/amd64, go1.19.1, 596a9f9 配置文件 启动时默认查找当前目录下的Corefile配置文件， 可通过 coredns -conf /etc/Corefile 指定自定义配置文件\n#开头视为注释\n{$ENV_VAR} 引用操作系统变量\n支持片段定义和引用\n# 定义一个名为plg的片段 (plg) { errors forward . 8.8.8.8 223.5.5.5 reload log } .:53 { # 引用片段 import plg } server 块的定义 定义一组dns服务\n\u003czone\u003e[:\u003cport\u003e] { \u003cplugin1\u003e [参数...] \u003cplugin2\u003e [参数...] ... } . { \u003cplugin1\u003e [参数...] } .:53 { \u003cplugin1\u003e [参数...] } coredns.io:53 { \u003cplugin1\u003e [参数...] } 配置示例： 配置了两个server,每个server有自己的插件链\n# server 定义语法 [zone] [port...] {...} # 对所有域名执行解析 (plg) { errors forward . 8.8.8.8 223.5.5.5 reload log } .:53 { prometheus :9153 hosts { 1.2.3.4 zero-dew.cn fallthrough } import plg } # 仅解析 .com com.:54 { prometheus :9154 hosts { 1.2.3.4 zero-dew.cn 1.2.3.5 zero-dew.com fallthrough } import plg } ","categories":["coredns"],"description":"\ncoredns|kubernetes\r\n","excerpt":"\ncoredns|kubernetes\r\n","ref":"/kubernetes/coredns.html","tags":["coredns"],"title":"快速开始"},{"body":"任务：\n通过cgroupfs 限制cpu/memory/diskio 通过systemd 限制cpu/memory/diskio 通过cgroupfs 限制kubernetes 容器的cpu/memory/diskio cgroups 由 Google 工程师于 2006 年提出，并在 2007 年被合并到 Linux 2.6.24 内核。虽然目前有两个版本的 cgroups，但大多数发行版和机制都使用v1版本。\n在linux中一切皆文件的宗旨下，cgroup通过文件结构来实现对进程的控制和限制，目录结构：\n目录结构: Tab 1 Tab 2 /sys/fs/cgroup/ ├── blkio\t# 限制进程对硬盘的读写速率 ├── cpu -\u003e cpu,cpuacct ├── cpuacct -\u003e cpu,cpuacct ├── cpu,cpuacct\t# 限制和监控进程的CPU消耗 ├── cpuset\t# 绑定进程到特定CPU核心或内存节点（NUMA 架构） ├── devices\t# 控制进程对设备文件（如 /dev/sda）的访问权限（读/写/创建设备） ├── freezer\t# 暂停/恢复 进程组中的所有进程 ├── hugetlb\t# 限制大页内存的使用量。 ├── memory\t# 控制内存用量 和 Swap 交换空间 ├── net_cls -\u003e net_cls,net_prio ├── net_cls,net_prio ├── net_prio -\u003e net_cls,net_prio ├── perf_event\t# 允许性能监控工具（如 perf）追踪 CGroup 内进程的性能事件 ├── pids\t# 限制 CGroup 内允许的 最大进程数量 └── systemd 管理cgroup cgroup作为linux内核的一部分，在用户层面centos提供了三种管理cgroup的工具，分别为：libcgroup、 cgroupfs 、systemd libcgroup（已经弃用） 是一个用户空间的 cgroup 管理库和工具集，提供了命令行工具（如 cgcreate、cgexec、cgclassify 等）和 C 语言 API。 cgroupfs 不是一个单独的工具，而是指 Linux 内核通过挂载 cgroup 文件系统，暴露出来的接口。通过操作 /sys/fs/cgroup 目录，用户和程序通过直接操作 cgroup（如创建目录、写入参数文件）来管理资源。 systemd 是现代 Linux 的初始化系统和服务管理器，它内置了对 cgroup 的原生支持 任务一：通过cgroupfs 限制cpu/memory/diskio\ncgroupfs管理cgroup: 管理cpu 管理内存 管理磁盘io 创建新的cpu 子系统\nmkdir /sys/fs/cgroup/cpu,cpuacct/m 找到所有进程及子进程\n[root@seagullcore01-uat-s2 ~]# ps -ef|grep /usr/bin/m root 65787 61380 99 16:46 pts/1 00:02:07 /usr/bin/m root 65850 63340 0 16:47 pts/2 00:00:00 grep --color=auto /usr/bin/m [root@seagullcore01-uat-s2 ~]# pstree -p 65787 m(65787)─┬─{m}(65788) ├─{m}(65789) ├─{m}(65790) └─{m}(65791) 配额cpu限额\nfor pid in 65787 65788 65789 65790 65791; do echo $pid \u003e/sys/fs/cgroup/cpu,cpuacct/m/tasks;done echo 200000 \u003e/sys/fs/cgroup/cpu,cpuacct/m/cpu.cfs_quota_us # 在一个周期内允许使用的 CPU 时间（μs）,默认-1不限制 echo 100000 \u003e/sys/fs/cgroup/cpu,cpuacct/m/cpu.cfs_period_us # 调度周期，单位为微秒（μs），通常设为 100000（即 100ms） 验证是否生效，使用 top 或查看 cpu.stat [root@seagullcore01-uat-s2 ~]# cat /sys/fs/cgroup/cpu,cpuacct/m/cpu.stat nr_periods 3083 # 自创建以来经历了多少个调度周期 nr_throttled 2716 # 因为达到cgroup限制而中断的次数（中断后等待下一次调度） throttled_time 521614106629 # 因达到cgroup限制中断停止的时长（纳秒） 清理子cgroup\n[root@seagullcore01-uat-s2 ~]# rmdir /sys/fs/cgroup/cpu,cpuacct/m rmdir: failed to remove ‘/sys/fs/cgroup/cpu,cpuacct/m’: Device or resource busy [root@seagullcore01-uat-s2 ~]# cat /sys/fs/cgroup/cpu,cpuacct/m/cgroup.procs 65787 [root@seagullcore01-uat-s2 ~]# echo 65787 \u003e/sys/fs/cgroup/cpu,cpuacct/cgroup.procs [root@seagullcore01-uat-s2 ~]# cat /sys/fs/cgroup/cpu,cpuacct/m/cgroup.procs [root@seagullcore01-uat-s2 ~]# rmdir /sys/fs/cgroup/cpu,cpuacct/m 创建新的内存子系统\nmkdir /sys/fs/cgroup/memory/m 找到所有进程及子进程\n[root@seagullcore01-uat-s2 ~]# ps -ef|grep /usr/bin/m root 65787 61380 99 16:46 pts/1 00:02:07 /usr/bin/m root 65850 63340 0 16:47 pts/2 00:00:00 grep --color=auto /usr/bin/m [root@seagullcore01-uat-s2 ~]# pstree -p 65787 m(65787)─┬─{m}(65788) ├─{m}(65789) ├─{m}(65790) └─{m}(65791) 配额内存限额\nfor pid in 65787 65788 65789 65790 65791; do echo $pid \u003e/sys/fs/cgroup/memory/m/cgroup.procs;done echo 1g \u003e/sys/fs/cgroup/memory/m/memory.limit_in_bytes systemd管理cgroup: 方式一 方式二 # 动态设置进程不超过2个cpu ~]# systemctl set-property m.service CPUQuota=200% # 使用命令行设置进程不超过2G ~]# systemctl set-property m.service MemoryLimit=2G tee \u003e/usr/lib/systemd/system/m.service \u003c\u003c'EOF' [Unit] Description=A demo for load cpu [Service] ExecStart=/usr/bin/m # 限制2核心cpu CPUQuota=200% EOF tee \u003e/usr/lib/systemd/system/m.service \u003c\u003c'EOF' [Unit] Description=A demo for load memory [Service] ExecStart=/usr/bin/m # 限制2G 内存 MemoryLimit=2G EOF 任务二：通过cgroupfs 调整kubernetes容器的限制\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: cgrouptest name: cgrouptest spec: replicas: 1 selector: matchLabels: app: cgrouptest strategy: {} template: metadata: creationTimestamp: null labels: app: cgrouptest spec: containers: - image: harbor.pytc.com/library/m:latest name: m resources: limits: cpu: 1000m memory: 128Mi - image: harbor.pytc.com/library/m:latest name: m2 resources: limits: cpu: 2000m memory: 128Mi EOF 获取pod uid\n[root@master-01 ~]# kubectl get pod -l app=cgrouptest -ojsonpath='{.items[0].metadata.uid}{\"\\n\"}'|tr \"-\" \"_\" 5dba196d_897b_4ebb_865e_0e43b612575d 获取pod中容器的containerID\n[root@master-01 ~]# kubectl get pod -l app=cgrouptest -ojsonpath='{range .items[0].status.containerStatuses[*] } {.name}{\"\\t\"}{.containerID}{\"\\n\"}{end}' m containerd://05d994653b88f6fe97254c206efba6dc34f6e03a0ede108be081d71a0e1c6857 m2 containerd://3f06542e87e0fb80b38c86181b8fdd48580c258969770c56466cd68498deb2a5 查看容器的限制\n[root@master-01 kubepods-pod5dba196d_897b_4ebb_865e_0e43b612575d.slice]# cat /sys/fs/cgroup/cpu/kubepods.slice/kubepods-pod5dba196d_897b_4ebb_865e_0e43b612575d.slice/cri-containerd-05d994653b88f6fe97254c206efba6dc34f6e03a0ede108be081d71a0e1c6857.scope/cpu.cfs_quota_us 100000 [root@master-01 kubepods-pod5dba196d_897b_4ebb_865e_0e43b612575d.slice]# cat /sys/fs/cgroup/cpu/kubepods.slice/kubepods-pod5dba196d_897b_4ebb_865e_0e43b612575d.slice/cri-containerd-3f06542e87e0fb80b38c86181b8fdd48580c258969770c56466cd68498deb2a5.scope/cpu.cfs_quota_us 200000 修改容器的限制，并观察变化\n报错：\n[root@master-01 kubepods-pod5dba196d_897b_4ebb_865e_0e43b612575d.slice]# echo 400000 \u003e/sys/fs/cgroup/cpu/kubepods.slice/kubepods-pod5dba196d_897b_4ebb_865e_0e43b612575d.slice/cri-containerd-05d994653b88f6fe97254c206efba6dc34f6e03a0ede108be081d71a0e1c6857.scope/cpu.cfs_quota_us -bash: echo: write error: Invalid argument 排查和解决:\n设置问题：由于容器上一层pod 中 cpu.cfs_quota_us 的值为300000（即两个容器的资源限制总和不会超过3个核心），在cgroup中子系统受父级限制。因此最大设置不能超过 300000。\n资源限制问题： 同时对于该示例中一个pod包含两个容器，即使两个容器cpu.cfs_quota_us 的值都设置为300000。操作系统也不会分配6个cpu。原因是cgroup中子系统受父级限制。因此两个容器合计最大可以使用3个核心。\n突破限制： 首先修改父级cpu.cfs_quota_us 限制，其次修改容器限制\n[root@master-01 kubepods-pod5dba196d_897b_4ebb_865e_0e43b612575d.slice]# cat /sys/fs/cgroup/cpu/kubepods.slice/kubepods-pod5dba196d_897b_4ebb_865e_0e43b612575d.slice/cpu.cfs_quota_us 300000 [root@master-01 kubepods-pod5dba196d_897b_4ebb_865e_0e43b612575d.slice]# echo 300000 \u003e/sys/fs/cgroup/cpu/kubepods.slice/kubepods-pod5dba196d_897b_4ebb_865e_0e43b612575d.slice/cri-containerd-05d994653b88f6fe97254c206efba6dc34f6e03a0ede108be081d71a0e1c6857.scope/cpu.cfs_quota_us [root@master-01 kubepods-pod5dba196d_897b_4ebb_865e_0e43b612575d.slice]# echo 500000 \u003e/sys/fs/cgroup/cpu/kubepods.slice/kubepods-pod5dba196d_897b_4ebb_865e_0e43b612575d.slice/cpu.cfs_quota_us 故障处理 使用 umount -a 后 cgroup文件系统被卸载，以下为重新挂载 cgroup 文件系统的步骤。 重建基础结构\nmount -t tmpfs tmpfs /sys/fs/cgroup 挂载 cgroup v2 挂载 cgroup v1 mkdir /sys/fs/cgroup/unified mount -t cgroup2 none /sys/fs/cgroup/unified controllers=(blkio cpu,cpuacct cpuset devices freezer hugetlb memory net_cls,net_prio perf_event pids systemd ) for ctrl in \"${controllers[@]}\"; do mkdir -p /sys/fs/cgroup/$ctrl mount -t cgroup -o $ctrl cgroup /sys/fs/cgroup/$ctrl done # 特别处理 systemd 控制器 umount /sys/fs/cgroup/systemd mount -t cgroup -o none,name=systemd systemd /sys/fs/cgroup/systemd # 创建符号链接 cd /sys/fs/cgroup/ ln -sv cpu,cpuacct cpuacct ln -sv cpu,cpuacct cpu ln -sv net_cls,net_prio net_cls ln -sv net_cls,net_prio net_prio 示例代码 cpu 压测代码\nDetails package main import ( \"runtime\" \"sync/atomic\" ) // 定义全局变量阻止编译器优化 var counter uint64 func main() { // 获取逻辑 CPU 核心数 (如 4 核 8 线程则返回 8) // numCPU := runtime.NumCPU() numCPU := 4 // 设置 Go 运行时使用的最大 CPU 核数 runtime.GOMAXPROCS(numCPU) // 为每个逻辑 CPU 核心启动一个满载 goroutine for i := 0; i \u003c numCPU; i++ { go worker() } // 阻塞主线程防止退出 select {} } // CPU 密集型任务函数 func worker() { // 原子操作循环 (避免循环被 Go 编译器优化) for { atomic.AddUint64(\u0026counter, 1) } } 内存压测代码\nDetails package main import ( \"fmt\" \"runtime\" \"time\" ) const targetMem = 2 * 1024 * 1024 * 1024 // 2GB func main() { // 创建内存池避免被GC回收 var memoryHolder [][]byte // 分块分配更贴近真实场景 blockSize := 100 * 1024 * 1024 // 每次分配100MB for allocated := 0; allocated \u003c targetMem; allocated += blockSize { block := make([]byte, blockSize) memoryHolder = append(memoryHolder, block) // 读取内存数据防止优化 for i := 0; i \u003c len(block); i += 4096 { block[i] = byte(i % 256) } // 打印当前分配状态 printMemUsage() } fmt.Println(\"\\n📊 内存分配完成，持续占用中...\") fmt.Println(\"✅ 可使用以下命令监控：\") fmt.Println(\" top -p $(pgrep your_program_name)\") fmt.Println(\" watch -n 1 'ps -eo pid,rss,comm | grep your_program_name'\") // 保持程序运行直到kill ticker := time.NewTicker(30 * time.Second) defer ticker.Stop() for range ticker.C { printMemUsage() } } func printMemUsage() { var m runtime.MemStats runtime.ReadMemStats(\u0026m) fmt.Printf(\"➤ 系统视角内存: %.2fGB | Alloc=%.2fMB | Sys=%.2fMB\\n\", float64(m.HeapSys)/1024/1024/1024, float64(m.HeapAlloc)/1024/1024, float64(m.Sys)/1024/1024) } https://docs.redhat.com/zh-cn/documentation/red_hat_enterprise_linux/7/html/resource_management_guide/chap-introduction_to_control_groups https://segmentfault.com/a/1190000009732550\nhttps://www.redhat.com/sysadmin/cgroups-part-one\nhttps://www.redhat.com/sysadmin/cgroups-part-two\nhttps://www.redhat.com/sysadmin/cgroups-part-three\nhttps://www.redhat.com/sysadmin/cgroups-part-four\nhttps://www.redhat.com/en/services/training/do080-deploying-containerized-applications-technical-overview?intcmp=701f20000012ngPAAQ\nhttps://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/resource_management_guide/index\nhttps://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/resource_management_guide/chap-introduction_to_control_groups\nhttps://isovalent.com/blog/post/demystifying-cni/\nhttps://zhuanlan.zhihu.com/p/346050404\nhttps://systemd-by-example.com/\nhttps://www.ibm.com/support/pages/node/6393890?mhsrc=ibmsearch_a\u0026mhq=cgroup\nhttps://www.redhat.com/en/services/training/do080-deploying-containerized-applications-technical-overview?intcmp=701f20000012ngPAAQ§ion=overview\nhttps://www.redhat.com/sysadmin/cgroups-part-three\nhttps://zhuanlan.zhihu.com/p/346050404\nhttps://blog.csdn.net/qq_37041791/article/details/126031351\n","categories":["kubernetes"],"description":"k8s cgroup\n","excerpt":"k8s cgroup\n","ref":"/2025-05-25/cgroup.html","tags":["kubernetes"],"title":"Cgroup"},{"body":"\n\u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003cmeta charset=\"utf-8\" /\u003e \u003ctitle\u003eECharts\u003c/title\u003e \u003c!-- 引入刚刚下载的 ECharts 文件 --\u003e \u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/echarts/5.4.2/echarts.min.js\"\u003e\u003c/script\u003e \u003c/head\u003e \u003cbody\u003e \u003c!-- 为 ECharts 准备一个定义了宽高的 DOM --\u003e \u003cdiv id=\"main\" style=\"width: 600px;height:400px;\"\u003e\u003c/div\u003e \u003cscript type=\"text/javascript\"\u003e // 基于准备好的dom，初始化echarts实例 var myChart = echarts.init(document.getElementById('main')); // 指定图表的配置项和数据 var option = { title: { text: 'ECharts 入门示例' }, tooltip: {}, legend: { data: ['销量'] }, xAxis: { data: ['衬衫', '羊毛衫', '雪纺衫', '裤子', '高跟鞋', '袜子'] }, yAxis: {}, series: [ { name: '销量', type: 'bar', data: [5, 20, 36, 10, 10, 20] } ] }; // 使用刚指定的配置项和数据显示图表。 myChart.setOption(option); \u003c/script\u003e \u003c/body\u003e \u003c/html\u003e ","categories":["echart"],"description":"echart 是一个基于 JavaScript 的开源可视化库，用于创建各种类型的图表，如折线图、柱状图、饼图等。它提供了丰富的配置选项和交互功能，使得开发者可以轻松地将数据以直观的方式呈现给用户。\n","excerpt":"echart 是一个基于 JavaScript 的开源可视化库，用于创建各种类型的图表，如折线图、柱状图、饼图等。它提供了丰富的配置选项和交互功能，使得开发者可以轻松地将数据以直观的方式呈现给用户。\n","ref":"/2025-05-25/echart.html","tags":["echart"],"title":"Echart"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/database/mysql/","tags":"","title":""},{"body":"\n安装部署 第一步：安装依赖 centos ubuntu #GCC (gun compiler collection)c语言编译器 #gcc-c++ c++语言编译器 yum install gcc gcc-c++ -y # rewrite模块需要 pcre (perl compatible regular expression per 兼容正则表达式) yum install pcre pcre-devel -y # zlib 配置中gizp on 使用 yum install zlib zlib-devel -y # openssl 提供https 和md5 sha1等 yum install openssl openssl-devel -y # 更新软件包列表 sudo apt-get update # 安装GCC和G++编译器 sudo apt-get install gcc g++ -y # 安装PCRE库 sudo apt-get install libpcre3 libpcre3-dev -y # 安装zlib库 sudo apt-get install zlib1g zlib1g-dev -y # 安装OpenSSL库 sudo apt-get install openssl libssl-dev -y 第二步下载源码并修改源代码（非必须）\nwget http://nginx.org/download/nginx-1.22.1.tar.gz tar xf nginx-1.22.1.tar.gz \u0026\u0026 cd nginx-1.22.1 隐藏nginx标识和版本信息:\n修改源码位置：src/http/ngx_http_header_filter_module.c\n源码 修改后 static u_char ngx_http_server_string[] = \"Server: nginx\" CRLF; static u_char ngx_http_server_full_string[] = \"Server: \" NGINX_VER CRLF; static u_char ngx_http_server_build_string[] = \"Server: \" NGINX_VER_BUILD CRLF; static u_char ngx_http_server_string[] = \"Server: IIS\" CRLF; static u_char ngx_http_server_full_string[] = \"Server: IIS\" CRLF; static u_char ngx_http_server_build_string[] = \"Server: IIS\" CRLF; 修改源码位置：src/http/ngx_http_special_response.c\n源码 修改后 static u_char ngx_http_error_full_tail[] = \"\u003chr\u003e\u003ccenter\u003e\" NGINX_VER \"\u003c/center\u003e\" CRLF \"\u003c/body\u003e\" CRLF \"\u003c/html\u003e\" CRLF ; static u_char ngx_http_error_build_tail[] = \"\u003chr\u003e\u003ccenter\u003e\" NGINX_VER_BUILD \"\u003c/center\u003e\" CRLF \"\u003c/body\u003e\" CRLF \"\u003c/html\u003e\" CRLF ; static u_char ngx_http_error_tail[] = \"\u003chr\u003e\u003ccenter\u003enginx\u003c/center\u003e\" CRLF \"\u003c/body\u003e\" CRLF \"\u003c/html\u003e\" CRLF ; static u_char ngx_http_error_full_tail[] = \"\u003chr\u003e\u003ccenter\u003e IIS \u003c/center\u003e\" CRLF \"\u003c/body\u003e\" CRLF \"\u003c/html\u003e\" CRLF ; static u_char ngx_http_error_build_tail[] = \"\u003chr\u003e\u003ccenter\u003e IIS \u003c/center\u003e\" CRLF \"\u003c/body\u003e\" CRLF \"\u003c/html\u003e\" CRLF ; static u_char ngx_http_error_tail[] = \"\u003chr\u003e\u003ccenter\u003eIIS\u003c/center\u003e\" CRLF \"\u003c/body\u003e\" CRLF \"\u003c/html\u003e\" CRLF ; 第三步执行编译\nuseradd nginx -M -s /bin/nologin ./configure \\ --prefix=/opt/nginx \\ --user=nginx \\ --group=nginx \\ --with-http_ssl_module \\ --with-http_flv_module \\ --with-http_mp4_module \\ --with-http_stub_status_module \\ --with-http_gzip_static_module \\ --with-stream_ssl_module \\ --with-stream \\ --with-http_realip_module \\ --with-pcre \\ --with-debug make \u0026\u0026 make install #查看编译信息 /opt/nginx/sbin/nginx -V # 启动服务 /opt/nginx/sbin/nginx nginx-1.22.1]# curl -I 127.0.0.1 HTTP/1.1 200 OK Server: IIS Date: Thu, 22 May 2025 07:39:59 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Thu, 22 May 2025 07:39:16 GMT Connection: keep-alive ETag: \"682ed4a4-267\" Accept-Ranges: bytes ","categories":["nginx"],"description":"install|nginx\n","excerpt":"install|nginx\n","ref":"/nginx/install.html","tags":["nginx"],"title":"安装nginx"},{"body":"\n查看服务器基本信息 分类 功能 命令 硬件 通过DMI获取系统硬件信息 dmidecode 或 lshw 显示PCI/USB接口信息 lspci/lsusb 显示CPU信息 lscpu 或 cat /proc/cpuinfo 检查硬件虚拟化的支持 egrep --color \"vmx|svm\" /proc/cpuinfo 显示物理内存大小 free -m 或 `cat /proc/meminfo 系统 查看系统发行版本 cat /etc/system-release 查看系统内核版本 uname -r 显示机器的体系结构 arch 或 uname -m 显示系统加载的内核模块 lsmod 查看磁盘的inode size 和 block size dumpe2fs /dev/mapper/seagullcore-data 日志 查看系统启动、硬件 dmesg 或 /var/log/dmesg 系统日志，记录内核、服务、应用程序的非关键信息 /var/log/messages 用户登录日志 /var/log/secure cron 定时任务日志 /var/log/cron 审计日志 /var/log/audit/audit.log yum日志 /var/log/yum.log postfix 或 sendmail 等邮件服务的日志 /var/log/maillog journalctl 命令 centos7 中journalctl 命令用于显示系统日志。 默认情况下，journalctl 仅显示当前会话的日志，并滚动显示最新的日志条目。 若要显示完整的日志信息，应使用 journalctl 命令配合一些选项来获取更多的日志细节。以下是一些常用的选项：\n-u \u003cunit\u003e：指定要查看的服务名称，例如 -u nginx。 -f：以滚动模式显示日志，实时显示新日志条目。 -r：以逆序（即从新到旧的顺序）显示日志条目。 -o \u003cformat\u003e：指定输出格式，可以是 short、verbose、json 等。例如，使用 journalctl -o json 将输出日志以 JSON 格式显示。 --since \u003ctime\u003e：指定要显示的日志开始时间，该参数可以是时间戳，或者使用类似 yesterday，1 hour ago 的相对时间。例如，使用 journalctl --since \"2023-06-24 10:00:00\" 将显示从 2023-06-24 10:00:00 开始的所有日志。 --until \u003ctime\u003e：指定要显示的日志截止时间。例如: journalctl --until \"1 hour ago\" 将显示截止到 1 小时前的所有日志。 -n \u003cnumber\u003e：指定要显示的日志条目数量。例如，使用 journalctl -n 50 将仅显示最新的 50 条日志。 以下是一个示例，可以使用该示例来查看系统的所有服务的日志，显示所有启动时间为 2022 年 1 月 1 日之后的日志，以详细模式显示这些日志，并按照事件发生的反向顺序排列输出：\njournalctl --since \"2022-01-01\" -o verbose -u all -r journald 日志系统 centos7 中journalctl是systemd日志系统的管理工具，默认临时存储在/run/log/journal/目录中，重启后丢失。\n开启持久化:\n/etc/systemd/journald.conf 配置文件中关键参数：\n指标名称 可选值 注释 Storage volatile 仅保存在内存中。 例如 Storage=volatile persistent 优先保存在磁盘。例如：Storage=persistent auto 若存在/var/log/journal则持久化，否在仅保存内存中。例如：Storage=auto SystemMaxUse 限制最大磁盘使用量。例如 SystemMaxUse=10G RuntimeMaxUse 限制最大内存使用量。例如 RuntimeMaxUse=1G 日志清理 # 清理早于指定时间的日志 journalctl --vacuum-time=7d # 限制日志体积（如保留500MB） journalctl --vacuum-size=500M mkdir -p /var/log/journal chown root:systemd-journal /var/log/journal chmod 2755 /var/log/journal sed -i 's/.*Storage.*/Storage=persistent/' /etc/systemd/journald.conf systemctl restart systemd-journald 系统性能监视工具 yum install htop iftop iptraf nethogs iperf3 strace perf systemtap-sdt-devel.x86_64 -y\n工具 说明 htop 动态显示系统进程任务 top 动态显示系统进程任务 iotop 显示进程的磁盘 I/O 信息（iostat 和 top 的结合体） slabtop iftop vmstat 显示进程队列、内存、交换空间、块 I/O、CPU 活动信息 sysstat 输出 CPU 的各种统计信息。可以用来分析程序运行时在内核态和用户态的时间 iostat 显示当前的网络流量 pidstat fstat 输出 CPU、IO 系统和磁盘分区的统计信息。可以用来分析磁盘 I/O、带宽等 uptime 显示系统的运行时间和平均负载 procps 显示系统内存的使用 sar 定时搜集系统的各种状态信息，然后对系统各个时间点的状态进行监控 pmap iptraf nethogs iperf3 strace dtrace ionice nice renice top命令 VIRT：virtual memory usage 虚拟内存\n1、进程“需要的”虚拟内存大小，包括进程使用的库、代码、数据等 2、假如进程申请100m的内存，但实际只使用了10m，那么它会增长100m，而不是实际的使用量\nRES：resident memory usage 常驻内存 1、进程当前使用的内存大小，但不包括swap out 2、包含其他进程的共享 3、如果申请100m的内存，实际使用10m，它只增长10m，与VIRT相反 4、关于库占用内存的情况，它只统计加载的库文件所占内存大小 SHR：shared memory 共享内存 1、除了自身进程的共享内存，也包括其他进程的共享内存 2、虽然进程只使用了几个共享库的函数，但它包含了整个共享库的大小 3、计算某个进程所占的物理内存大小公式：RES – SHR 4、swap out后，它将会降下来 DATA 1、数据占用的内存。如果top没有显示，按f键可以显示出来。 2、真正的该程序要求的数据空间，是真正在运行中要使用的。\n快捷键 注释 1 展开逻辑cpu详情 u 显示指定用户的进程 k 杀死指定进程 c 显示命令的完整信息 f 选择要输出显示的列 或 立即刷新显示 l – 关闭或开启第一部分第一行 top 信息的表示 t – 关闭或开启第一部分第二行 Tasks 和第三行 Cpus 信息的表示 m – 关闭或开启第一部分第四行 Mem 和 第五行 Swap 信息的表示 N – 以 PID 的大小的顺序排列表示进程列表 P – 以 CPU 占用率大小的顺序排列进程列表 M – 以内存占用率大小的顺序排列进程列表 iostat命令 选项 说明 -c 仅显示 CPU 统计信息。与 -d 选项互斥 -d 仅显示磁盘统计信息。与 -c 选项互斥 -k 以 KB 为单位显示每秒的磁盘请求数。默认单位为块 -m 以 MB 为单位显示每秒的磁盘请求数。默认单位为块 -p {device|ALL} 用于显示块设备及系统分区的统计信息。与 -x 选项互斥 -x 输出扩展信息 [root@iostat ~]# iostat -x Linux 3.10.0-957.el7.x86_64 (redis-uat07-s1) 05/27/2025 _x86_64_ (2 CPU) avg-cpu: %user %nice %system %iowait %steal %idle 2.56 0.00 3.90 0.00 0.00 93.54 Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util sda 0.00 0.01 0.00 0.29 0.04 8.19 56.11 0.00 0.43 17.40 0.37 0.11 0.00 dm-0 0.00 0.00 0.00 0.30 0.03 8.19 54.11 0.00 0.44 18.19 0.38 0.11 0.00 dm-1 0.00 0.00 0.00 0.00 0.00 0.00 54.67 0.00 0.33 0.33 0.00 0.30 0.00 avg-cpu 部分输出项说明\n输出项 含义 %user 在用户级别运行所使用的 CPU 的百分比 %nice 高优先级进程（nice=0）使用的 CPU 的百分比 %system 在核心级别（kernel）运行所使用 CPU 的百分比 %iowait CPU 等待硬件 IO 所占用 CPU 的百分比 %steal 当管理程序（hypervisor）为另一个虚拟进程提供服务而等待虚拟 CPU 的百分比 %idle CPU 空闲时间的百分比 Device 部分基本输出项说明\n输出项 含义 Blk_read 读入的数据总量，单位为块 Blk_wrtn 写入的数据总量，单位为块 kb_read 读入的数据总量，单位为 KB kb_wrtn 写入的数据总量，单位为 KB MB_read 读入的数据总量，单位为 MB MB_wrtn 写入的数据总量，单位为 MB Blk_read/s 每秒从驱动器读入的数据量，单位为块/s Blk_wrtn/s 每秒向驱动器写入的数据量，单位为块/s KB_read/s 每秒从驱动器读入的数据量，单位为 KB/s KB_wrtn/s 每秒向驱动器写入的数据量，单位为 KB/s MB_read/s 每秒从驱动器读入的数据量，单位为 MB/s MB_wrtn/s 每秒向驱动器写入的数据量，单位为 MB/s tps 每秒向物理设备写入的 IO 传输总量 Device 部分扩展输出项说明\n输出项 含义 rrqm/s 将读入请求合并后，每秒发送到设备的读入请求数 wrqm/s 将写入请求合并后，每秒发送到设备的写入请求数 r/s 每秒发送到设备的读入请求数 w/s 每秒发送到设备的写入请求数 rkB/s 每秒从设备读入的数据量，单位为 KB/s wkB/s 每秒向设备写入的数据量，单位为 KB/s avgrq-sz 发送到设备的请求的平均大小，单位为扇区 avgqu-sz 发送到设备的请求的平均队列长度 await IO 请求平均执行时间，包括发送请求和执行的时间，单位为毫秒 r_await 读入请求平均执行时间，单位为毫秒 w_await 写入请求平均执行时间，单位为毫秒 svctm IO 请求在设备上的平均执行时间，单位为毫秒 %util 设备利用率，表示设备的忙碌程度，当值接近 100% 时，表示设备带宽已占满 usec/s 每秒向设备写入的扇区数 rMB/s 每秒从设备读入的数据量，单位为 MB/s wMB/s 每秒向设备写入的数据量，单位为 MB/s vmstat命令 选项 说明 -n 只在开始时显示一次各字段名称 -a 显示活跃和非活跃内存 -m 显示 procs/abinfo -S 使用指定单位显示，K(1000)、K(1024)、M(1000000)、M(1048576) 字节，默认单位为 K [root@vmstat ~]# vmstat 5 procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 159304 64 1821284 0 0 0 4 0 0 3 4 94 0 0 输出结果字段说明\n分类 输出项 说明 procs r 在运行队列中等待运行的进程数 b 等待 IO 的进程数 memory (默认单位为 KB) swpd 当前使用的交换空间 free 当前空闲的物理内存 buff 用作缓冲的内存大小 cache 用作缓存的内存大小 inactive 非活跃的内存大小（当使用 -a 选项时显示） active 活跃的内存大小（当使用 -a 选项时显示） swap si 每秒写入交换区的内存大小 so 每秒写入交换区的内存大小 io bi 每秒读取块设备的块数 bo 每秒写入块设备的块数 system in 每秒的中断数，包括时钟中断 cs 每秒的环境（上下文）切换次数 cpu us 用户进程执行时间的百分比 sy 系统进程执行时间的百分比 id 空闲时间（包括等待 IO 时间）的百分比 wa 等待 IO 时间的百分比 st 管理程序（hypervisor）为另一个虚拟进程提供服务而等待虚拟 CPU 的百分比 mpstat命令 mpstat 通过分析 /proc/stat 文件报告cpu相关信息\n输出结果字段说明\n输出项 说明 %user 在多 CPU 系统里，每个 CPU 有一个 ID 号。第一个 CPU 为 0，all 表示统计信息为所有 CPU 的平均值 %nice 显示在用户级别运行所占用 CPU 总时间的百分比 %system 显示在核心级别（kernel）运行所占用 CPU 总时间的百分比。这个值并不包括服务中断和 softirq %iowait 显示用于等待 IO 操作中，占用 CPU 总时间的百分比 %steal 显示用于虚拟机管理程序（hypervisor）为另一个虚拟进程提供服务而等待虚拟 CPU 的百分比 %idle 显示 CPU 在空闲状态，占用 CPU 总时间的百分比 intr/s 显示 CPU 每秒接收到的中断总数 sar命令 用于收集、报告、保存系统活跃信息\n在sar的基础上增加 sar [ interval [ count ] ]选项。可以指定统计间隔和统计次数：\n[root@master01 ~]# sar 1 2 Linux 3.10.0-1160.92.1.el7.x86_64 (master01) 10/03/2023 _x86_64_ (4 CPU) 11:20:15 AM CPU %user %nice %system %iowait %steal %idle 11:20:16 AM all 4.11 0.00 4.38 0.00 0.00 91.51 11:20:17 AM all 2.79 0.00 5.29 0.00 0.00 91.92 Average: all 3.45 0.00 4.83 0.00 0.00 91.71 将 sar [ interval [ count ] ] -o datafile 命令结果输出到文件中。\n举例：\nsar -o datafile 1 2 \u003e/dev/null 2\u003e\u00261 \u0026 查看datafile 中的内容 :\nsar -f ./datafile 查看cpu状态\nsar -u 1 2 查看cpu的统计信息，此时和 sar 1 2 结果一样\n[root@master01 ~]# sar -u 1 2 Linux 3.10.0-1160.92.1.el7.x86_64 (master01) 10/03/2023 _x86_64_ (4 CPU) 11:22:44 AM CPU %user %nice %system %iowait %steal %idle 11:22:45 AM all 2.41 0.00 4.83 0.00 0.00 92.76 11:22:46 AM all 2.66 0.00 3.99 0.00 0.00 93.35 Average: all 2.54 0.00 4.41 0.00 0.00 93.06 sar -P 0 1 2 查看cpu0的统计信息\n[root@master01 ~]# sar -P 0 1 2 Linux 3.10.0-1160.92.1.el7.x86_64 (master01) 10/03/2023 _x86_64_ (4 CPU) 11:19:19 AM CPU %user %nice %system %iowait %steal %idle 11:19:20 AM 0 5.26 0.00 2.11 0.00 0.00 92.63 11:19:21 AM 0 2.08 0.00 5.21 0.00 0.00 92.71 Average: 0 3.66 0.00 3.66 0.00 0.00 92.67 sar -P ALL 1 2 展开显示所有核心的统计信息\n[root@master01 ~]# sar -P ALL 1 2 Linux 3.10.0-1160.92.1.el7.x86_64 (master01) 10/03/2023 _x86_64_ (4 CPU) 11:26:58 AM CPU %user %nice %system %iowait %steal %idle 11:26:59 AM all 2.70 0.00 4.59 0.00 0.00 92.70 11:26:59 AM 0 3.37 0.00 3.37 0.00 0.00 93.26 11:26:59 AM 1 3.30 0.00 4.40 0.00 0.00 92.31 11:26:59 AM 2 3.09 0.00 5.15 0.00 0.00 91.75 11:26:59 AM 3 3.23 0.00 4.30 0.00 0.00 92.47 11:26:59 AM CPU %user %nice %system %iowait %steal %idle 11:27:00 AM all 3.98 0.00 2.39 0.00 0.00 93.63 11:27:00 AM 0 2.13 0.00 3.19 0.00 0.00 94.68 11:27:00 AM 1 6.32 0.00 4.21 0.00 0.00 89.47 11:27:00 AM 2 3.16 0.00 2.11 0.00 0.00 94.74 11:27:00 AM 3 3.12 0.00 3.12 0.00 0.00 93.75 Average: CPU %user %nice %system %iowait %steal %idle Average: all 3.35 0.00 3.48 0.00 0.00 93.17 Average: 0 2.73 0.00 3.28 0.00 0.00 93.99 Average: 1 4.84 0.00 4.30 0.00 0.00 90.86 Average: 2 3.12 0.00 3.65 0.00 0.00 93.23 Average: 3 3.17 0.00 3.70 0.00 0.00 93.12 lsof命令 Lsof是遵从Unix哲学的典范，它只完成一个功能，并且做的相当完美——它可以列出某个进程打开的所有文件信息。打开的文件可能是普通的文件、目录、NFS文件、块文件、字符文件、共享库、常规管道、命名管道、符号链接、Socket流、网络Socket、UNIX域Socket，以及其它更多类型。因为“一切皆文件”乃为Unix系统的重要哲学思想之一，因此可以想象lsof命令的重要地位。\nlsof ［options］ filename lsof /path/to/somefile：显示打开指定文件的所有进程之列表 lsof -c string：显示其COMMAND列中包含指定字符(string)的进程所有打开的文件；此选项可以重复使用，以指定多个模式； lsof -p PID：查看该进程打开了哪些文件；进程号前可以使用脱字符“^”取反； lsof -u USERNAME：显示指定用户的进程打开的文件；用户名前可以使用脱字符“^”取反，如“lsof -u ^root”则用于显示非root用户打开的所有文件； lsof -g GID：显示归属gid的进程情况 lsof +d /DIR/：显示指定目录下被进程打开的文件 lsof +D /DIR/：基本功能同上，但lsof会对指定目录进行递归查找，注意这个参数要比grep版本慢： lsof -a：按“与”组合多个条件，如lsof -a -c apache -u apache lsof -N：列出所有NFS（网络文件系统）文件 lsof -d FD：显示指定文件描述符的相关进程；也可以为描述符指定一个范围，如0-2表示0,1,2三个文件描述符；另外，-d还支持其它很多特殊值，如： mem: 列出所有内存映射文件； mmap：显示所有内存映射设备； txt：列出所有加载在内存中并正在执行的进程，包含code和data； cwd：正在访问当前目录的进程列表； lsof -n：不反解IP至HOSTNAME lsof -i：用以显示符合条件的进程情况 lsof -i[46] [protocol][@hostname|hostaddr][:service|port] 46：IPv4或IPv6 protocol：TCP or UDP hostname：Internet host name hostaddr：IPv4地址 service：/etc/service中的服务名称(可以不只一个) port：端口号 (可以不只一个) 例如： 查看22端口现在运行的情况 [root@www ~]# lsof -i :22 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME sshd 1390 root 3u IPv4 13050 0t0 TCP *:ssh (LISTEN) sshd 1390 root 4u IPv6 13056 0t0 TCP *:ssh (LISTEN) sshd 36454 root 3r IPv4 94352 0t0 TCP www.magedu.com:ssh-\u003e172.16.0.1:50018 (ESTABLISHED) 上述命令中，每行显示一个打开的文件，若不指定条件默认将显示所有进程打开的所有文件。lsof输出各列信息的意义如下： COMMAND：进程的名称 PID：进程标识符 USER：进程所有者 FD：文件描述符，应用程序通过文件描述符识别该文件。如cwd、txt等 TYPE：文件类型，如DIR、REG等 DEVICE：指定磁盘的名称 SIZE：文件的大小 NODE：索引节点（文件在磁盘上的标识） NAME：打开文件的确切名称 ","categories":["linux"],"description":"问题排查|linux\n","excerpt":"问题排查|linux\n","ref":"/linux/system.html","tags":["问题排查"],"title":"问题排查"},{"body":"\nAnsible 是一个 IT 自动化工具,每年大约发布三到四次新的 Ansible 主要版本。\n文档使用版本2.9\n概念 控制节点： 安装了ansible 的主机，window主机不能作为控制节点 被管理节点：受控制节点管理的主机 inventory : 被管理节点的主机清单 模块：ansible 调用的python包 tasks: 要ansible执行的任务 playbook: 通过编排后的task集合，可以完成复杂任务 工作原理 选择被管理节点 通过ssh连接到节点 将一个或多个模块复制到被管理节点执行 快速开始 yum install ansible-2.9.27-1.el7.noarch # 2.9 开始支持补全功能 yum install epel-release yum install python-argcomplete ansible 127.0.0.1 -m ping 主配置文件: /etc/ansible/ansible.cfg\n定义资产配置清单: /etc/ansible/hosts\n资产配置清单: ini格式 yaml格式 # /etc/ansible/hosts [master] 10.4.7.9 10.4.7.1[0:1] [node] node[1:15] [all:vars] ansible_ssh_user: root # /etc/ansible/hosts all: children: master: hosts: \"10.4.7.9,10.4.7.1[0:1]\" # 展开为 10.4.7.9, 10.4.7.10, 10.4.7.11 node: hosts: \"node[1:15]\" # 展开为 node1, node2, ..., node15 vars: ansible_ssh_user: root # 全局 SSH 用户设置为 root 查询配置清单中的主机\nansible all --list-hosts 模块 ping 模块 ansible all -m ping debug ansible 127.0.0.1 -m debug -a \"msg=hello world\" hostname # 修改主机名 ansible 127.0.0.1 -m hostname -a \"name=ansible-master\" fetch # ansible-doc -s fetch 从远端拉取文件 ansible 127.0.0.1 -m fetch -a \"src=/etc/hosts dest=/tmp\" file # ansible-doc -s file 创建/删除 文件/目录/软连接 ansible 127.0.0.1 -m file -a \"path=/tmp/a.log state=touch\" ansible 127.0.0.1 -m file -a \"path=/tmp/a.log state=absent\" ansible 127.0.0.1 -m file -a \"path=/tmp/a/b state=directory recurse=yes\" ansible 127.0.0.1 -m file -a \"path=/tmp/a state=absent\" ansible 127.0.0.1 -m file -a \"path=/tmp/a state=link src=/etc/hosts\" blockinfile # ansible-doc -s blockinfile 按照标记添加/删除文件内容 # 插入内容 ansible 127.0.0.1 -m blockinfile -a \"path=/tmp/a marker=#{mark}20220318 block='101.43.43.9 test.com'\" # 删除标记位置内容 ansible 127.0.0.1 -m blockinfile -a \"path=/tmp/a marker=#{mark}20220318 state=absent\" # 在开头插入 BOF（begin of file） ansible 127.0.0.1 -m blockinfile -a \"path=/tmp/a marker=#{mark}20220318 block='10.43.43.9 test.com' insertbefore=BOF\" # 在结尾插入 EOF (end of file) ansible 127.0.0.1 -m blockinfile -a \"path=/tmp/a marker=#{mark}20220318 block='10.43.43.9 test.com' insertbefore=EOF\" # 在insertafter='^127.0.0.1' 之后插入 ansible 127.0.0.1 -m blockinfile -a \"path=/tmp/a marker=#{mark}20220318 block='10.43.43.9 test.com' insertafter='^127.0.0.1'\" lineinfile # sed -i 's/^10/#############/g' /tmp/a # backrefs=yes 支持正则分组引用 ，当正则不匹配时不执行动作 ansible 127.0.0.1 -m lineinfile -a \"path=/tmp/a regexp='^10' line='#############' backrefs=yes\" # sed -i 's/^#(.*)/\\1g' /tmp/a ansible 127.0.0.1 -m lineinfile -a \"path=/tmp/a regexp='^#(.*)' line='\\1' backrefs=yes # sed - '/#############/d' /tmp/a ansible 127.0.0.1 -m lineinfile -a \"path=/tmp/a line='#############' state=absent\" # sed - '/^.*20220318$/d' /tmp/a ansibel 127.0.0.1 -m lineinfile -a \"path=/tmp/a regexp='^.*20220318$' state=absent\" find # ansible-doc -s find ansible 127.0.0.1 -m find -a \"paths='/root' patterns='zookeeper.out' recurse=yes\" replace # sed -i 's/^[a-Z]/##/g' /tmp/a ansible 127.0.0.1 -m replace -a \"path=/tmp/a regexp='^[a-Z]' replace='##'\" command ansible 127.0.0.1 -m command -a \"chdir='/' ls -a\" shell # 支持管道，command 不支持 ansible 127.0.0.1 -m shell -a \"chdir='/' ls -a|wc\" script\n# /tmp/1.sh 在运维主机 ansible 127.0.0.1 -m script -a \"chdir='/' /tmp/1.sh\" cron # 添加定时任务 # */5 * * * * usr/sbin/ntpdate ntpdate ntp.aliyun.com \u003e/dev/null 2\u003e\u00261 ansible 127.0.0.1 -m cron -a \"minute=*/5 hour=* name='ntp date' job='/usr/sbin/ntpdate ntpdate ntp.aliyun.com \u003e/dev/null 2\u003e\u00261'\" # 启用注释的定时任务 ansible 127.0.0.1 -m cron -a \"disabled=false name='ntp date' job='/usr/sbin/ntpdate ntpdate ntp.aliyun.com \u003e/dev/null 2\u003e\u00261'\" # 删除定时任务 ansible 127.0.0.1 -m cron -a \"state=absent name='ntp date' backup=yes\" yum # 安装 ansible 127.0.0.1 -m yum -a \"name=rsync,vim,lrzsz state=present\" # 卸载 ansible 127.0.0.1 -m yum -a \"name=rsync,vim,lrzsz state=absent\" systemd ansible all -m systemd -a \"name=network state=restarted enabled=yes\" group # groupadd mysql ansible all -m group -a \"name=mysql state=present\" # groupadd mysql ansible all -m group -a \"name=db state=present\" user # useradd -g mysql -G root,db -M -s /bin/nologin mysql ansible all -m user -a \"name=mysql group=mysql groups=db,root create_home=no shell=/bin/nologin state=present\" template - hosts: - all tasks: - name: template template:\t# 与copy类似，支持变量解析 src: /tmp/nginx.conf.j2 dest: /tmp/nginx.conf 剧本playbook # 检查语法 ansible-playbook --syntax-check /etc/ansible/play.yml # dry-run ansible-playbook --check /etc/ansible/play.yml # 执行剧本 ansible-playbook /etc/ansible/play.yml ansible-playbook /etc/ansible/play.yml --list-hosts ansible-playbook /etc/ansible/play.yml --list-tasks ansible-playbook /etc/ansible/play.yml --limit 10.4.7.21 基本语法tasks # /etc/ansible/play.yml - hosts: all tasks: - name: ping host ping: - hosts: - 10.4.7.11 - 10.4.7.12 tasks: - name: fetch file fetch: src: /etc/services dest: /tmp - name: copy file copy: src: /etc/services dest: /tmp 基本语法handlers # 通过notify 调用handlers - hosts: - all tasks: - name: touch file file: path: /tmp/file.prom state: touch notify: wirte file handlers: - name: wirte file blockinfile: path: /tmp/file.prom block: 'this is first line' - name: remove file file: path: /tmp/file.prom state: absent 基本语法meta - hosts: - all tasks: - name: touch file file: path: /tmp/file.prom state: touch notify: wirte file # python 按顺序执行，如果添加了meta: flush_handlers 对应的task完成后会直接触发handlers - meta: flush_handlers - name: ping ping: handlers: - name: wirte file blockinfile: path: /tmp/file.prom block: 'this is first line' - name: remove file file: path: /tmp/file.prom state: absent 基本语法listen - hosts: - all tasks: - name: touch file file: path: /tmp/file.prom state: touch notify: group1 handlers: - name: wirte file listen: group1 blockinfile: path: /tmp/file.prom block: 'this is first line' - name: modify file listen: group1 lineinfile: path: /tmp/file.prom regexp: '(.*)first(.*)' line: \\1\\2 backrefs: true 基本语法tags - hosts: - all tasks: - name: touch file file: path: /tmp/file.prom state: touch notify: group1 - name: ping ping: tags: tag_ping handlers: - name: wirte file listen: group1 blockinfile: path: /tmp/file.prom block: 'this is first line' - name: modify file listen: group1 lineinfile: path: /tmp/file.prom regexp: '(.*)first(.*)' line: \\1\\2 backrefs: true # 查看有哪些tag ansible-playbook --list-tags /etc/ansible/play.yml ansible-playbook --tags=tag_ping /etc/ansible/play.yml ansible-playbook --skip-tags=tag_ping /etc/ansible/play.yml # 所有任务都不会被执行 ansible-playbook --skip-tags all /etc/ansible/play.yml # ansible-playbook --tags tagged /etc/ansible/play.yml # ansible-playbook --tags untagged /etc/ansible/play.yml # always 默认执行，除非明确指定--skip-tags ansible-playbook --skip-tags always /etc/ansible/play.yml # never 在不明确指定的情况下默认不执行 ansible-playbook --tags never /etc/ansible/play.yml 循环 - hosts: - all tasks: - name: debug debug: msg: \"{{ item }}\" with_items: - first line - second line # 带索引的列表 - hosts: - all tasks: - name: debug debug: msg: \"{{item.1}}\" with_indexed_items: - first line - second line # range(1,10,3) - hosts: - all tasks: - name: debug debug: msg: \"{{ item }}\" with_sequence: start=1 end=10 stride=3 - hosts: - all tasks: - name: debug debug: msg: \"{{ item.name }}{{ item.age }}\" with_items: - {name: wangendao,age: 31} - {name: zhangsan,age: 18} 判断 when - hosts: - all tasks: - debug: msg: \"{{ansible_hostname}}\" when: # 可以使系统变量或自定义变量 ansible_hostname == \"hdss7-200\" 变量 一、系统变量：\n# cpu 核心数 ansible_processor_vcpus # 主机名 ansible_hostname # ip ansible_host # 内存 ansible_memtotal_mb # ansible的版本信息 ansible_version # ansible-play 1.yml --list-host play_hosts # groups # 系统发行版 ansible_distribution # 系统版本号 ansible_distribution_major_version 二、用户自定义变量\n在资产清单中定义\n# /etc/ansible/hosts all: hosts: 10.4.7.11: name: wangendao 10.4.7.12: 10.4.7.200: vars: ansible_ssh_user: root ansible_ssh_pass: 123 children: k8s_node: hosts: 10.4.7.21: 10.4.7.22: vars: ansible_ssh_user: root ansible_ssh_pass: 123 主机变量\n[atlanta] host1 http_port=80 maxRequestsPerChild=808 host2 http_port=303 maxRequestsPerChild=909 all: children: atlanta: host1: http_port: 80 maxRequestsPerChild: 808 host2: http_port: 303 maxRequestsPerChild: 909 [targets] # 指定连接方式 localhost ansible_connection=local other1.example.com ansible_connection=ssh ansible_user=myuser other2.example.com ansible_connection=ssh ansible_user=myotheruser 此外还可以在/etc/ansible/host_vars/主机名 举例：\ntee /etc/ansible/host_vars/localhost \u003c\u003cEOF username=admin passwd=123 组变量\n[atlanta] host1 host2 [atlanta:vars] ntp_server=ntp.atlanta.example.com proxy=proxy.atlanta.example.com all: children: atlanta: hosts: host1: host2: vars: ntp_server: ntp.atlanta.example.com proxy: proxy.atlanta.example.com [atlanta] host1 host2 [raleigh] host2 host3 [southeast:children] atlanta raleigh [southeast:vars] some_server=foo.southeast.example.com halon_system_timeout=30 self_destruct_countdown=60 escape_pods=2 [usa:children] southeast northeast southwest northwest all: children: usa: children: southeast: children: atlanta: hosts: host1: host2: raleigh: hosts: host2: host3: vars: some_server: foo.southeast.example.com halon_system_timeout: 30 self_destruct_countdown: 60 escape_pods: 2 northeast: northwest: southwest: 此外还可以在/etc/ansible/group_vars/分组名称/... 举例：\nini格式\ntee /etc/ansible/group_vars/db \u003c\u003cEOF username=root passwd=123 EOF yaml格式\ntee /etc/ansible/group_vars/db \u003c\u003cEOF username: root passwd: 123 EOF 指定别名\njumper ansible_port=5555 ansible_host=192.0.2.50 all: hosts: jumper: ansible_port: 5555 ansible_host: 192.0.2.50 playbook中定义\n书写方式一：\n- hosts: - 127.0.0.1 vars: - info: {'name': wangendao,'age':31} - filename: f1 tasks: - name: touch file file: path: /tmp/{{filename}} state: touch tags: touch file - name: debug debug: msg: \"name: {{ info.name }} age: {{ info.age }}\" 书写方式二：\n# vars.yml name: wangendao age: 31 - hosts: - all vars_files: - vars.yml tasks: - name: debug debug: msg: \"{{ name }}{{age}}\" 书写方式三：\n# 接收键盘输入 - hosts: - 127.0.0.1 vars_prompt: - name: \"name\" prompt: \"please input your name: \" # 显示输入的信息，默认隐藏 private: no - name: \"age\" prompt: \"please input your age: \" # 显示输入的信息，默认隐藏 private: no tasks: - name: debug debug: msg: \"{{ name }} {{ age }}\" 书写方式四：\n- hosts: - 127.0.0.1 tasks: - name: debug debug: msg: \"{{ name }}\" -e ,--extra-vars ansible-playbook /etc/ansible/play_vars.yml --extra-vars \"name=wangendao\" ansible-playbook /etc/ansible/play_vars.yml -e \"name=wangendao\" 变量注册\n- hosts: - 127.0.0.1 vars: - info: {'name': wangendao,'age':31} - filename: f1 tasks: - name: touch file file: path: /tmp/{{filename}} state: touch # 把 touch file 的执行结果保存到变量AAA 中 register: AAAA tags: touch file - name: debug debug: msg: \"name: {{ info.name }} age: {{ info.age }} {{ AAAA }}\" roles 下载roles的方法\nansible-galaxy install\nroles roles |- nginx |- tasks |- 定义任务列表 |- handler |- main.yml\t定义handlers |- vars |- main.yml\t定义变量 |- templates |- files |- meta |- main.yml 作者、版本信息 |- defaults |- main.yml 定义变量的初始值 # 在/tmp 下创建一个标准的roles目录结构 ansible-galaxy init /tmp/nginx 从互联网 现在roles\n# 查找仓库中的role ansible-galaxy search \"k8s\" # 查看仓库中role信息 ansible-galaxy info 24_komal.ec2_k8s_master # 下载到/tmp/roles下 ansible-galaxy install 24_komal.ec2_k8s_master -p /tmp/roles 通过requirements.yml 文件下载\nrequiremets.yml的格式如下:\n# 从galaxy 官网下载http://galaxy.ansible.com/ - src: # 从git下载 - src: http://github.com/xxx/xxx.git scm: git version: 56200a54 name: nginx-acme # 把roles 打包成tar.gz 上传到自检的http服务器后，要想从这个http服务器下载 - src: http://127.0.0.1/my.tar.gz name: my 二进制命令 二进制命令\n名称 功能 举例 ansible 用于临时执行命令 ansible all -m ping ansible-console 交互式命令 ansible-vault 由于文件加密和解密 ansible-galaxy 用于和roles仓库交互 ansible-galaxy init ansible-playbook 用于运行playbook ansible-doc 查询帮助 ansible-doc -lansible-doc -s ping # 加密 yml bash-4.4# ansible-vault encrypt /etc/ansible/play1.yml # 查看加密的 yml bash-4.4# ansible-vault view /etc/ansible/play1.yml # 编辑加密的 yml bash-4.4# ansible-vault edit /etc/ansible/play1.yml # 修改加密的密码 bash-4.4# ansible-vault rekey /etc/ansible/play1.yml # 解密 yml bash-4.4# ansible-vault decrypt /etc/ansible/play1.yml 参考 官方文档\nAnsible中文权威指南- 国内最专业的Ansible中文官方学习手册\nAnsible Galaxy\nhttps://www.cnblogs.com/michael-xiang/p/10462749.html)\n欢迎来到 Jinja2 — Jinja2 2.7 documentation (jinkan.org)\n[yaml官网](\n","categories":["ansible"],"description":"quitstart|ansible\n","excerpt":"quitstart|ansible\n","ref":"/ansible/quitstart.html","tags":["ansible"],"title":"Quitstart"},{"body":" 阅读该文档你将了解到：\n从0到1搭建kubernetes环境 基于kubernetes的可观测平台构建 CI/CD 环境与k8s的集成 本示例完全实现下图架构,并试图对 cni/可观测性/cicd 能力进行扩展。力图构建一套生产环境可用的模型。\n​\n架构 服务器列表 通过自建dns 或 /etc/hosts 文件实现对所有主机间的主机名解析\n独立挂载/var/lib/docker 、/var/lib/kubelet 、etcd数据目录\nhostname os ipaddress roles master01 centos7.9 192.168.0.108/24 master、node、keepalived、nginx、nfs master02 centos7.9 192.168.0.140/24 master、node、keepalived、nginx master03 centos7.9 192.168.0.162/24 master、node、keepalived、nginx node01 centos7.9 node node02 centos7.9 node virtualhost 192.168.0.144/24 vip 系统优化: 系统初始化 LB keepalived #!/bin/bash # author: 1209233066@qq.com # description: #\tinit linux env for kuernetes v24- # function: # 1. sethostname # 2. turn off swap # 3. turn off selinux # 4. turn off firewalld # 5. turn on kernel for netfilter/ip_nonlocal_bind # 6. turn on ipvs modle # 7. modify cgroup driver for docker # 8. allow bind nonlocal ip # LC_ALL=C SetEnv(){ # 设置主机名和时区 hostnamectl set-hostname \"$1\" timedatectl set-timezone Asia/Shanghai # 关闭swap分区 swapoff -a sed -i '/swap/s|^|#|g' /etc/fstab # 关闭selinux setenforce 0 sed -i 's|SELINUX=enforcing|SELINUX=disabled|g' /etc/selinux/config # 关闭firewalld systemctl disable --now firewalld # 加载 br_netfilter 模块 echo br_netfilter \u003e/etc/modules-load.d/br_netfilter.conf # 加载ipvs 模块 ls /usr/lib/modules/$(uname -r)/kernel/net/netfilter/ipvs |awk -F \".\" '{print $1}' \u003e/etc/modules-load.d/ipvs.conf # 加载/etc/modules-load.d 下配置文件 systemctl restart systemd-modules-load # 开启 bridge-nf-call-iptables内核功能 # 开启 bridge-nf-call-ip6tables 内核功能 # 开启数据包内核转发功能 sysctl -w net.bridge.bridge-nf-call-iptables=1 sysctl -w net.bridge.bridge-nf-call-ip6tables=1 sysctl -w net.ipv4.ip_forward=1 sysctl -w net.ipv4.ip_nonlocal_bind=1 echo -e \"net.bridge.bridge-nf-call-iptables = 1\\nnet.bridge.bridge-nf-call-ip6tables = 1\\nnet.ipv4.ip_forward = 1\\nnet.ipv4.ip_nonlocal_bind = 1\" \u003e/etc/sysctl.d/kubernetes.conf # yum install ipvsadm -y # curl https://get.docker.com|bash -s -- --mirror Aliyun --version 20.10.24 \u0026\u0026\\ mkdir /etc/docker \u0026\u0026\\ echo -e '{\\n\\t\"exec-opts\": [\"native.cgroupdriver=systemd\"]\\n}' \u003e/etc/docker/daemon.json systemctl daemon-reload systemctl enable docker --now systemctl status docker } SetEnv L4负载均衡，在所有master节点执行 sysctl -w net.ipv4.ip_nonlocal_bind=1 user nginx; worker_processes 1; events { worker_connections 1024; } stream { log_format main '$remote_addr [$time_local] ' '$protocol $status $bytes_sent $bytes_received ' '$session_time \"$upstream_addr\" ' '\"$upstream_bytes_sent\" \"$upstream_bytes_received\" \"$upstream_connect_time\"'; upstream kube_apiserver { server 192.168.0.108:6443 weight=1 max_fails=3 fail_timeout=30s; server 192.168.0.140:6443 weight=1 max_fails=3 fail_timeout=30s; server 192.168.0.162:6443 weight=1 max_fails=3 fail_timeout=30s; } server { listen 192.168.0.144:16443; proxy_connect_timeout 2s; proxy_timeout 900s; proxy_pass kube_apiserver; access_log logs/kubeapiserver.log main; } } 三个节点使用相同的 BACKUP 和 priority 100 初始化时拥有对等角色,vrrp初次选举时如果priority 相同则会选择ip较大的节点拥有vip，因此不用担心相同priority 脑裂问题。 而state BACKUP 是为了让非抢占参数nopreempt 能够生效\n参数 赋值 作用说明 router_id 不同 标识本主机，便于日志区分 virtual_router_id 相同 VRRP组标识，主备必须一致 state BACKUP 相同 所有节点设置为BACKUP，目的是为了让非抢占参数nopreempt 能够生效 cat \u003e/etc/keepalived/keepalived.conf\u003c\u003c'EOF' ! Configuration File for keepalived ####################### main config ######################## global_defs { notification_email { 810654947@qq.com 1209233066@qq.com } notification_email_from 810654947@qq.com smtp_server smtp.qq.com 587 smtp_connect_timeout 30 # 标识本主机，便于日志区分。每个主机不同 router_id 1921680108 vrrp_garp_master_delay 2 script_user root } vrrp_script check_port { script \"/etc/keepalived/check_port.sh 16443\" interval 2 weight -30 } ################### Vrrp instance config ################### vrrp_instance VI_1 { # MASTER|BACKUP state BACKUP priority 100 interface ens192 virtual_router_id 51 advert_int 1 # 当初始化节点是MASTER时不生效 nopreempt authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.0.144/24 } track_script { check_port } } EOF 健康检查脚本\ncat \u003e/etc/keepalived/check_port.sh\u003c\u003c'EOF' #!/bin/bash count=$(ss -lntp|grep -c \"$1\") [ $count -ge 1 ] \u0026\u0026 exit 0 || pkill keepalived; exit 1 EOF chmod +x /etc/keepalived/check_port.sh keepalived 启动脚本\ntee /usr/lib/systemd/system/keepalived.service \u003c\u003cEOF [Unit] Description = keepalived daemon After = network.target [Service] Type=forking ExecStart=/usr/local/sbin/keepalived --log-console -f /etc/keepalived/keepalived.conf Restart=always RestartSec=5 User=root [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable --now keepalived kubeadm 安装 cat \u003c\u003cEOF \u003e /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF yum install -y kubelet-1.23.17-0.x86_64 kubeadm-1.23.17-0.x86_64 kubectl-1.23.17-0.x86_64 初始化集群 kubeadm init --control-plane-endpoint \"192.168.0.144:16443\" \\ --image-repository k8s-gcr.m.daocloud.io \\ --kubernetes-version 1.23.17 \\ --upload-certs \\ --pod-network-cidr \"10.244.0.0/16\" \\ --service-cidr \"172.168.0.0/16\" \\ -v5 systemctl enable kubelet --now mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 命令自动补全\nyum install bash-completion -y echo 'source \u003c(kubectl completion bash)' \u003e\u003e~/.bashrc source ~/.bashrc 安装基础cni插件flannel kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml 安装ingress controller kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.0.4/deploy/static/provider/cloud/deploy.yaml k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1 k8s.gcr.io/ingress-nginx/controller:v1.1.2 externalIPs: - 192.168.0.236 安装dashboard kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.6.2/high-availability-1.21+.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes/kubernetes/refs/tags/v1.23.2/cluster/addons/dashboard/dashboard.yaml 报错 x509: cannot validate certificate for 192.168.0.xxx because it doesn't contain any IP SANs \"Failed probe\" probe=\"metric-storage-ready\" err=\"no metrics to serve\" 解决办法\n--kubelet-insecure-tls openssl genrsa -out dashboard.key 2048 openssl req -new -key dashboard.key -out ca.csr -subj \"/C=CN/ST=Gd/L=SZ/O=zero-dew.com/CN=dashboard.zero-dew.com\" openssl x509 -req -in ca.csr -out dashboard.crt -signkey dashboard.key -days 3650 kubectl -n kubernetes-dashboard create secret tls dashboard-tls --cert=dashboard.crt --key=dashboard.key cat \u003c\u003c 'EOF' |kubectl apply -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: dashboard namespace: kubernetes-dashboard annotations: nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" spec: ingressClassName: nginx tls: - hosts: - dashboard.zero-dew.com secretName: dashboard-tls rules: - host: dashboard.zero-dew.com http: paths: - path: / pathType: Prefix backend: service: name: kubernetes-dashboard port: number: 443 EOF 192.168.0.236 A dashboard.zero-dew.com https://dashboard.zero-dew.com kubectl create sa dashboard-admin -n kubernetes-dashboard kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin kubectl describe secret dashboard-admin-token-vm6lm -n kubernetes-dashboard 安装nfs storage class # https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner # https://www.cnblogs.com/punchlinux/p/16552183.html --- apiVersion: v1 kind: Namespace metadata: name: nfs-storage --- apiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner namespace: nfs-storage --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: nfs-client-provisioner rules: - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: nfs-client-provisioner roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: nfs-storage --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: leader-locking-nfs-client-provisioner namespace: nfs-storage rules: - apiGroups: - \"\" resources: - endpoints verbs: - get - list - watch - create - update - patch --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: leader-locking-nfs-client-provisioner namespace: nfs-storage roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: leader-locking-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: nfs-storage --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: managed-nfs-storage annotations: storageclass.kubernetes.io/is-default-class: \"true\" # 设置为默认sc provisioner: k8s-sigs.io/nfs-subdir-external-provisioner allowVolumeExpansion: true #允许动态扩容，比如kubectl edit pvc reclaimPolicy: Retain\t#PV的删除策略默认为delete,删除后pv立即删除NFS server的数据 mountOptions: #- vers=4.1 #NFS版本，containerd有部分参数异常 #- noresvport #告知NFS客户端在重新建立网络连接时，使用新的传输控制协议端口 - noatime #访问文件时不更新文件inode中的时间戳，高并发环境可提高性能 parameters: #mountOptions: \"vers=4.1,noresvport,noatime\" archiveOnDelete: \"true\" #删除pod时保留pod数据，默认为false时不保留数据 --- apiVersion: apps/v1 kind: Deployment metadata: name: nfs-client-provisioner labels: app: nfs-client-provisioner namespace: nfs-storage spec: replicas: 1 strategy: #部署策略 type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner #image: k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2 image: registry.cn-hangzhou.aliyuncs.com/liangxiaohui/nfs-subdir-external-provisioner:v4.0.2 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: k8s-sigs.io/nfs-subdir-external-provisioner - name: NFS_SERVER value: 10.4.7.250 - name: NFS_PATH value: /data/volumes volumes: - name: nfs-client-root nfs: server: 192.168.0.108 path: /data/volumes --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-test namespace: nfs-storage spec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi storageClassName: managed-nfs-storage --- apiVersion: v1 kind: Pod metadata: labels: run: test name: test namespace: nfs-storage spec: containers: - args: - sleep - \"1000\" image: busybox name: test resources: {} dnsPolicy: ClusterFirst restartPolicy: Always volumes: - name: dyncmic-pvc persistentVolumeClaim: claimName: pvc-test # yum install rpcbind nfs-utils -y # systemctl enable rpcbind nfs --now # systemctl status rpcbind nfs # install -d /data/volumes # echo \"/data/volumes *(rw,sync,no_root_squash)\" \u003e/etc/exports # root@harbor:~# exportfs -r # showmount -e 192.168.0.108 # mount -t nfs 192.168.0.108:/data/volumes /mnt 安装multus cni插件 + SpiderPool 安装多网卡cni multus\nkubectl apply -f https://raw.githubusercontent.com/k8snetworkplumbingwg/multus-cni/master/deployments/multus-daemonset.yml 安装ipam SpiderPool\nhelm repo add spiderpool https://spidernet-io.github.io/spiderpool helm fetch spiderpool/spiderpool helm install spiderpool spiderpool/spiderpool \\ --set multus.enableMultusConfig=false \\ --set global.imageRegistryOverride=ghcr.m.daocloud.io \\ --wait --namespace kube-system tee \u003c\u003cEOF|kubectl apply -f - apiVersion: spiderpool.spidernet.io/v2beta1 kind: SpiderIPPool metadata: name: macvlan-ipv4 spec: default: true subnet: 192.168.0.0/24 ips: - \"192.168.0.213-192.168.0.216\" - \"192.168.0.174-192.168.0.209\" EOF cat \u003c\u003cEOF | kubectl apply -f - apiVersion: \"k8s.cni.cncf.io/v1\" kind: NetworkAttachmentDefinition metadata: name: macvlan1 namespace: kube-system spec: config: '{ \"cniVersion\": \"0.3.0\", \"type\": \"macvlan\", \"master\": \"ens192\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"spiderpool\" }, \"promiscuous\": true }' EOF 测试mcvlan\ncat \u003c\u003cEOF|kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test spec: selector: matchLabels: app: test template: metadata: labels: app: test annotations: k8s.v1.cni.cncf.io/networks: macvlan spec: tolerations: - effect: \"\" key: \"node-role.kubernetes.io/master\" containers: - name: test securityContext: privileged: true image: quay.io/libpod/alpine:3.10.2 command: [\"sleep\",\"1000\"] EOF prometheus监控告警 目标 对基础组件的指标采集。kube-apiserver、kube-scheduler、kube-controllermanager、kubelet、kubeproxy、etcd、coredns、cni、csi、cri、node 通过label 或annotation 通过kubernetes_sd_config 的对业务pod动态发现 完善一个kubernetes基础平台的dashbaord 完善一个kubernetes基础平台的alert 模版和告警规则 apiVersion: apps/v1 kind: DaemonSet metadata: name: node-exporter namespace: kube-system spec: selector: matchLabels: app: node-exporter template: metadata: labels: app: node-exporter spec: tolerations: - key: \"\" operator: \"Exists\" volumes: - name: root hostPath: path: / hostNetwork: true hostPID: true containers: - name: node-exporter image: quay.io/prometheus/node-exporter:v1.6.1 args: - --path.rootfs=/host volumeMounts: - mountPath: /host name: root scrape_configs: - job_name: node-exporter kubernetes_sd_configs: - api_server: https://192.168.0.144:16443 tls_config: ca_file: /etc/kubernetes/pki/ca.crt cert_file: /etc/kubernetes/pki/apiserver-kubelet-client.crt key_file: /etc/kubernetes/pki/apiserver-kubelet-client.key # 测试改成false也没问题 insecure_skip_verify: true role: node relabel_configs: - source_labels: [__address__] regex: (.*):(.*) replacement: \"$1:9100\" target_label: __address__ - job_name: 'etcd' tls_config: ca_file: /etc/kubernetes/pki/etcd/ca.crt cert_file: /etc/kubernetes/pki/etcd/peer.crt key_file: /etc/kubernetes/pki/etcd/peer.key scheme: https kubernetes_sd_configs: - api_server: https://192.168.0.144:16443 tls_config: ca_file: /etc/kubernetes/pki/ca.crt cert_file: /etc/kubernetes/pki/apiserver-kubelet-client.crt key_file: /etc/kubernetes/pki/apiserver-kubelet-client.key # 测试改成false也没问题 insecure_skip_verify: true role: node relabel_configs: - source_labels: [__meta_kubernetes_node_label_kubernetes_io_role] regex: \"master\" action: keep - source_labels: [__address__] regex: (.*):(.*) replacement: \"$1:2379\" target_label: __address__ - job_name: kube-apiserver honor_timestamps: true scrape_interval: 1m scrape_timeout: 1m metrics_path: /metrics scheme: https tls_config: ca_file: /etc/kubernetes/pki/ca.crt cert_file: /etc/kubernetes/pki/apiserver-kubelet-client.crt key_file: /etc/kubernetes/pki/apiserver-kubelet-client.key insecure_skip_verify: false follow_redirects: true enable_http2: true kubernetes_sd_configs: - api_server: https://192.168.0.144:16443 tls_config: ca_file: /etc/kubernetes/pki/ca.crt cert_file: /etc/kubernetes/pki/apiserver-kubelet-client.crt key_file: /etc/kubernetes/pki/apiserver-kubelet-client.key # 测试改成false也没问题 insecure_skip_verify: true role: node relabel_configs: - source_labels: [__meta_kubernetes_node_label_kubernetes_io_role] regex: \"master\" action: keep - source_labels: [__address__] regex: (.*):(.*) replacement: \"$1:6443\" target_label: __address__ - job_name: kubelet honor_timestamps: true scrape_interval: 1m scrape_timeout: 1m metrics_path: /metrics scheme: https tls_config: ca_file: /etc/kubernetes/pki/ca.crt cert_file: /etc/kubernetes/pki/apiserver-kubelet-client.crt key_file: /etc/kubernetes/pki/apiserver-kubelet-client.key insecure_skip_verify: true follow_redirects: true enable_http2: true kubernetes_sd_configs: - api_server: https://192.168.0.144:16443 tls_config: ca_file: /etc/kubernetes/pki/ca.crt cert_file: /etc/kubernetes/pki/apiserver-kubelet-client.crt key_file: /etc/kubernetes/pki/apiserver-kubelet-client.key # 测试改成false也没问题 insecure_skip_verify: true role: node relabel_configs: - source_labels: [__address__] regex: (.*):(.*) replacement: \"$1:10250\" target_label: __address__ - job_name: kube-state-metrics kubernetes_sd_configs: - api_server: https://192.168.0.144:16443 tls_config: ca_file: /etc/kubernetes/pki/ca.crt cert_file: /etc/kubernetes/pki/apiserver-kubelet-client.crt key_file: /etc/kubernetes/pki/apiserver-kubelet-client.key # 测试改成false也没问题 insecure_skip_verify: true role: service relabel_configs: - source_labels: [\"__meta_kubernetes_service_name\"] action: keep regex: \"kube-state-metrics\" - source_labels: [\"__meta_kubernetes_service_label_prometheus_io_external\",\"__meta_kubernetes_service_label_prometheus_io_ports\"] regex: ([0-9\\.]+);(\\d+) replacement: $1:$2 action: replace target_label: __address__ filebeat日志采集 目标 采集基础组件：kube-apiserver、kube-scheduler、kube-controllermanager、kubelet、kubeproxy、etcd、coredns、cni、csi、cri、node 通过label 或annotation 对业务pod动态发现 Details kubectl annotation ns kube-system filebeat=true filebeat.autodiscover: providers: - type: kubernetes node: ${NODE_NAME} templates: - condition: or: - equals: kubernetes.labels.filebeat: \"true\" - equals: kubernetes.namespace_labels.filebeat: \"true\" config: - type: filestream id: container-${data.kubernetes.container.id} prospector.scanner.symlinks: true paths: - /var/log/containers/*-${data.kubernetes.container.id}.log parsers: - container: ~ - multiline: type: pattern pattern: '^\\[' negate: true match: after skywalking [root@master01 ~]# kubeadm token create 43bjhh.j2l5jqrixcd022zc [root@master01 ~]# kubeadm token list TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS 43bjhh.j2l5jqrixcd022zc 23h 2025-11-28T12:14:03Z authentication,signing \u003cnone\u003e system:bootstrappers:kubeadm:default-node-token 继续添加master节点 kubeadm init phase upload-certs --upload-certs [root@master01 ~]# kubeadm token create --print-join-command kubeadm join 192.168.0.144:16443 --token mhnut3.z7xz7l9xzoj8vorg \\ --discovery-token-ca-cert-hash sha256:3e77b765acb011d12cfb2649040ba6b51dd07ac6ee1746ce540740fc9d31bea3 \\ --control-plane --certificate-key 33bdaf597978b2355117cb742562dbaf4b91a20abb8e3a9f145439122b4eae48 \\ -v5 systemctl enable kubelet 添加node节点 [root@master01 ~]# kubeadm token create --print-join-command kubeadm join 192.168.0.144:16443 --token hmop0f.c82n8s9ns25thhqh --discovery-token-ca-cert-hash sha256:3e77b765acb011d12cfb2649040ba6b51dd07ac6ee1746ce540740fc9d31bea3 -v5 systemctl enable kubelet ","categories":["kubernetes"],"description":"构建一套高可用、可扩展的kubernetes生态环境\n","excerpt":"构建一套高可用、可扩展的kubernetes生态环境\n","ref":"/kubernetes/kubernetes_setup/quitstart.html","tags":["kubeadm","kubeaz"],"title":"kubeadm环境搭建"},{"body":"\n简介 etcd 是CoreOS团队2013年6月发起的分布式k-v 数据库。采用raft协议作为一致性算法。\n由于 etcd 将数据写入磁盘，因此其性能很大程度上取决于磁盘性能。强烈建议使用ssd硬盘并做好性能测试fio。，etcd 默认将可配置的存储大小配额设置为 2GB，因此需留足RAM。对于生产环境建议最大不超过8GB，如果配置的值超过该值，etcd 在启动时发出警告。\n硬件最小建议：\n8GB 内存 100GB 磁盘 4核 CPU 第一节集群安装 主机名 主机ip etcd节点名称 版本 etcd-1.k8s.com 10.4.7.200/24 etcd-1 v.13.5.0 etcd-2.k8s.com 10.4.7.201/24 etcd-2 v.13.5.0 etcd-3.k8s.com 10.4.7.202/24 etcd-3 v.13.5.0 第一步生成证书\n请求文件\ncat etcd.cnf [req] req_extensions = v3_req distinguished_name = req_distinguished_name [ req_distinguished_name ] [ v3_req ] basicConstraints = CA:FALSE keyUsage = nonRepudiation,digitalSignature,keyEncipherment subjectAltName = @alt_names [alt_names] IP.1 = 10.4.7.200 IP.2 = 10.4.7.201 IP.3 = 10.4.7.202 ca证书\nopenssl genrsa -out ca.key 2048 openssl req -new -key ca.key -out ca.csr -subj \"/CN=etcd\" openssl x509 -req -in ca.csr -out ca.crt -signkey ca.key -days 365 server证书\nopenssl genrsa -out server.key 2048 openssl req -new -key server.key -out server.csr -subj \"/CN=etcd-server\" -config etcd.cnf openssl x509 -req -in server.csr -out server.crt -signkey server.key -CA ca.crt -CAkey ca.key -CAcreateserial -days 365 -extensions v3_req -extfile etcd.cnf peer证书\nopenssl genrsa -out peer.key 2048 openssl req -new -key peer.key -out peer.csr -subj \"/CN=etcd-peer\" -config etcd.cnf openssl x509 -req -in peer.csr -out peer.crt -signkey peer.key -CA ca.crt -CAkey ca.key -CAcreateserial -days 365 -extensions v3_req -extfile etcd.cnf 第二步配置启动文件\n注意\n每个节点需要按需修改\n--name --listen-peer-urls --initial-advertise-peer-urls --listen-client-urls --advertise-client-urls\n#!/bin/bash # v3.4+ 可以不指定ETCDCTL_API export ETCDCTL_API=3 /opt/etcd/etcd \\ --name etcd-3 \\ --listen-peer-urls 'https://10.4.7.202:2380' \\ --initial-advertise-peer-urls 'https://10.4.7.202:2380' \\ # etcd 服务监听的地址，用于客户端连接 --listen-client-urls 'https://10.4.7.202:2379,http://127.0.0.1:2379' \\ # etcd 对外广播的地址，用户客户端和其他etcd成员连接 --advertise-client-urls 'https://10.4.7.202:2379' \\ --initial-cluster-state 'new' \\ --initial-cluster-token 'etcd-cluster' \\ --initial-cluster 'etcd-1=https://10.4.7.200:2380,etcd-2=https://10.4.7.201:2380,etcd-3=https://10.4.7.202:2380' \\ --client-cert-auth --trusted-ca-file=./ca.crt \\ --cert-file=server.crt --key-file=server.key \\ --peer-client-cert-auth --peer-trusted-ca-file=./ca.crt \\ --peer-cert-file=./peer.crt --peer-key-file=./peer.key 第三步启动etcd并检查\n/opt/etcd/etcdctl \\ --endpoints=https://10.4.7.200:2379,https://10.4.7.201:2379,https://10.4.7.202:2379 \\ --cacert=./ca.crt \\ --cert=peer.crt \\ --key=peer.key \\ member list -w table 第二节集群备份和恢复 COMMANDS: restore Restores an etcd member snapshot to an etcd directory save Stores an etcd node backend snapshot to a given file status [deprecated] Gets backend snapshot status of a given file 第一步生成测试内容\n/opt/etcd/etcdctl --endpoints=https://10.4.7.200:2379 --cacert=./ca.crt --cert=peer.crt --key=peer.key put name 张三 第二步备份数据\n连接到集群任意节点\n/opt/etcd/etcdctl \\ --endpoints=https://10.4.7.200:2379 \\ --cacert=./ca.crt \\ --cert=peer.crt \\ --key=peer.key \\ snapshot save /data/backup/2022-11-13.db 查看备份文件状态\n/opt/etcd/etcdctl snapshot status /data/backup/2022-11-13.db -w table 第三步模拟破坏数据\n/opt/etcd/etcdctl --endpoints=https://10.4.7.200:2379 --cacert=./ca.crt --cert=peer.crt --key=peer.key del name 第四步恢复数据\n注意\n停止集群所有节点etcd进程，备份原数据文件 停止所有kube-apiserver 集群所有节点执行恢复动作，注意修改 --name --initial-advertise-peer-urls --data-dir 执行启动命令 ETCDCTL_API=3 /opt/etcd/etcdctl \\ snapshot restore /data/backup/2022-11-13.db \\ --name etcd-1 \\ --initial-cluster \"etcd-1=https://10.4.7.200:2380,etcd-2=https://10.4.7.201:2380,etcd-3=https://10.4.7.202:2380\" \\ --initial-cluster-token etcd-cluster \\ --initial-advertise-peer-urls https://10.4.7.200:2380 \\ --data-dir=./etcd-1.etcd 第五步启动集群\n参考集群安装中启动脚本启动集群\n验证恢复\n[root@radius-db03 ~]# /opt/etcd/etcdctl --endpoints=https://10.4.7.200:2379 --cacert=./ca.crt --cert=peer.crt --key=peer.key get name name 张三 备份脚本\nDetails #！/bin/bash # describe: this scribe to backup etcd # create by 1209233066@qq.com # usage: # MAILTO=\"\" # 59 23 * * * /bin/bash /scribe/etcd_backup.sh \u003e/dev/null 2\u003e\u00261 # 主机名 hostName=$(hostname) # IP hostIp=$(hostname -i|awk '{print $NF}') # 备份路径 backupDir=/data/backup/etcd # 证书文件 caCert=/etc/kubernetes/pki/etcd/ca.crt cert=/etc/kubernetes/pki/etcd/server.crt key=/etc/kubernetes/pki/etcd/server.key # 是否为leader ETCDCTL_API=3 isLeader=$(/usr/bin/etcdctl --endpoints=${hostIp}:2379 endpoint status|awk -F \",\" '{print $5}') # 备份 if ${isLeader};then echo \"The ${hostName} instance is the etcd leader,does not require backup\" else /usr/bin/etcdctl --cacert=${caCert} --cert=${cert} --key=${key} snapshot save ${backupDir}/etcd-${hostName}-`date+%Y%m%d`.db fi # 推送s3 # 本地保留 find ${backupDir} -name *.db -mtime +30 |xargs rm -f 第三节下线节点 第一步获取节点id\n/opt/etcd/etcdctl \\ --endpoints=https://10.4.7.200:2379,https://10.4.7.201:2379,https://10.4.7.202:2379 \\ --cacert=./ca.crt \\ --cert=peer.crt \\ --key=peer.key \\ member list 第二步下线节点\n/opt/etcd/etcdctl \\ --endpoints=https://10.4.7.200:2379,https://10.4.7.201:2379,https://10.4.7.202:2379 \\ --cacert=./ca.crt \\ --cert=peer.crt \\ --key=peer.key \\ member remove 19a4c376178d5b49 Member 19a4c376178d5b49 removed from cluster d5f1887e154dc473 第四节集群扩容 第一步集群中执行添加\n注意\n示例中10.4.7.202 需要在签证书时已经存在在etcd.cnf 中\n/opt/etcd/etcdctl \\ --endpoints=https://10.4.7.200:2379,https://10.4.7.201:2379 \\ --cacert=./ca.crt \\ --cert=peer.crt \\ --key=peer.key \\ member add etcd-2 \\ --peer-urls=https://10.4.7.202:2380 执行命令后返回值\nMember 8c993ee76df9da92 added to cluster d5f1887e154dc473 ETCD_NAME=\"etcd-2\" ETCD_INITIAL_CLUSTER=\"etcd-1=https://10.4.7.200:2380,etcd-2=https://10.4.7.201:2380,etcd-2=https://10.4.7.202:2380\" ETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://10.4.7.202:2380\" ETCD_INITIAL_CLUSTER_STATE=\"existing\" 第二步启动新加入节点\n参考集群安装中启动脚本启动集群，需要把--initial-cluster-state 'new' 修改为 --initial-cluster-state 'existing' 第五节其他指令 节点的健康状态 endpoint health\netcdctl --endpoints=10.4.7.250:2379 \\ --cacert=\"/etc/kubernetes/ssl/ca.pem\" \\ --cert=\"/etc/kubernetes/ssl/etcd.pem\" \\ --key=\"/etc/kubernetes/ssl/etcd-key.pem\" \\ endpoint health --write-out=table +-----------------+--------+-------------+-------+ | ENDPOINT | HEALTH | TOOK | ERROR | +-----------------+--------+-------------+-------+ | 10.4.7.250:2379 | true | 12.901855ms | | +-----------------+--------+-------------+-------+ 集群成员 member list\netcdctl member list --write-out=table +------------------+---------+-----------------+-------------------------+-------------------------+------------+ | ID | STATUS | NAME | PEER ADDRS | CLIENT ADDRS | IS LEARNER | +------------------+---------+-----------------+-------------------------+-------------------------+------------+ | 5c3282345e325481 | started | etcd-10.4.7.250 | https://10.4.7.250:2380 | https://10.4.7.250:2379 | false | +------------------+---------+-----------------+-------------------------+-------------------------+------------+ 节点状态endpoint status\netcdctl --endpoints=10.4.7.250:2379 \\ --cacert=\"/etc/kubernetes/ssl/ca.pem\" \\ --cert=\"/etc/kubernetes/ssl/etcd.pem\" \\ --key=\"/etc/kubernetes/ssl/etcd-key.pem\" \\ endpoint status --write-out=table +-----------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | +-----------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | 10.4.7.250:2379 | 5c3282345e325481 | 3.5.0 | 2.9 MB | true | false | 2 | 216020 | 216020 | | +-----------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ 性能测试check perf\n[root@etcd-1 ~]# etcdctl check perf 60 / 60 Booooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo! 100.00% 1m0s PASS: Throughput is 150 writes/s PASS: Slowest request took 0.017941s PASS: Stddev is 0.000807s PASS 查询\n查询key 1. 拿到所有key etcdctl get / --prefix --keys-only 2. 查询指定key.值是被序列化过的，因此可能会乱码 etcdctl get /registry/storageclasses/csi-rbd-sc 增加\netcdctl put /name wang OK 查询\netcdctl get /name /name wang # watch 一直监听 /name 这个key的变动 etcdctl watch /name 删除\n[root@ceph ~]# etcdctl del /name 1 参考 raft|raft\nhttps证书最佳实战目录 - _毛台 - 博客园 (cnblogs.com)\nETCD数据的备份与恢复 - taotaozh - 博客园 (cnblogs.com)\n","categories":["etcd"],"description":"\netcd|kubernetes\r\n","excerpt":"\netcd|kubernetes\r\n","ref":"/kubernetes/etcd.html","tags":["etcd"],"title":"etcd"},{"body":"基于Ruby 的 rails框架开发，主要组件：nginx postgreSQL redis sidekiq\n版本： gitlab-ee（企业版） gitlab-ce（社区版） 极狐（面向中国的版本）\n8c 16G centons7\nhttps://docs.gitlab.com/\nhttps://www.kancloud.cn/apachecn/gitlab-doc-zh/1948588\n安装部署 使用docker安装\nexport GITLAB_HOME=/home/gitlab sudo docker run --detach \\ --hostname gitlab.example.com \\ --publish 443:443 --publish 80:80 --publish 122:22 \\ --name gitlab \\ --restart always \\ --volume $GITLAB_HOME/config:/etc/gitlab \\ --volume $GITLAB_HOME/logs:/var/log/gitlab \\ --volume $GITLAB_HOME/data:/var/opt/gitlab \\ --shm-size 256m \\ -m 4g\\ gitlab-jh.tencentcloudcr.com/omnibus/gitlab-jh:latest 创建数据目录\npvcreate /dev/sdb vgextend centos /dev/sdb lvcreate -L +199G -n gitlab centos mkfs.ext4 /dev/mapper/centos-gitlab # 默认数据放在/opt/gitlab,如何修改这个路径 mkdir /opt/gitlab echo /dev/mapper/centos-gitlab /opt/gitlab ext4 defaults 0 0 \u003e\u003e/etc/fstab mount -a df -hT /gitlab 离线rpm包安装\nwget https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/gitlab-ce-15.0.1-ce.0.el7.x86_64.rpm yum localinstall gitlab-ce-15.0.1-ce.0.el7.x86_64.rpm -y 修改配置文件\nvi /etc/gitlab/gitlab.rb 修改下面这一行 external_url 'http://192.168.0.247' 启动服务\ngitlab-ctl reconfigure 日后重启服务\ngitlab-ctl start gitaly gitlab-ctl start gitlab-kas gitlab-ctl start gitlab-workhorse gitlab-ctl start logrotate gitlab-ctl start nginx gitlab-ctl start postgresql gitlab-ctl start puma gitlab-ctl start redis gitlab-ctl start sidekiq 卸载\nsudo gitlab-ctl stop \u0026\u0026 sudo gitlab-ctl remove-accounts sudo systemctl stop gitlab-runsvdir sudo systemctl disable gitlab-runsvdir sudo rm /usr/lib/systemd/system/gitlab-runsvdir.service sudo systemctl daemon-reload sudo systemctl reset-failed sudo gitlab-ctl uninstall sudo gitlab-ctl cleanse \u0026\u0026 sudo rm -r /opt/gitlab yum remove gitlab-ce 使用 默认用户root,密码 cat /etc/gitlab/initial_root_password 登录后首页 基础设置 修改root密码 基本页面，amdin是后台管理页面 关闭注册\n中文显示\n邮箱配置\nDocs: https://docs.gitlab.com/omnibus/settings/smtp.html ERROR irb(main):001:0\u003e Notify.test_email('1209233066@qq.com','test','gitlab test').deliver_now Delivered mail 6683e6c0128bc_1e1d45d83bd@gitlab.mail (35089.4ms) Traceback (most recent call last): 1: from (irb):1 EOFError (end of file reached) vi /etc/gitlab/gitlab.rb\n### GitLab email server settings ###! Docs: https://docs.gitlab.com/omnibus/settings/smtp.html ###! **Use smtp instead of sendmail/postfix.** gitlab_rails['smtp_enable'] = true gitlab_rails['smtp_address'] = \"smtp.qq.com\" gitlab_rails['smtp_port'] = 465 gitlab_rails['smtp_user_name'] = \"810654947@qq.com\" gitlab_rails['smtp_password'] = \"kqaexaxpbrbdbajd\" gitlab_rails['smtp_domain'] = \"smtp.qq.com\" gitlab_rails['smtp_authentication'] = \"login\" gitlab_rails['smtp_enable_starttls_auto'] = false gitlab_rails['smtp_tls'] = true # gitlab_rails['smtp_pool'] = false ### Email Settings gitlab_rails['gitlab_email_enabled'] = true ##! If your SMTP server does not like the default 'From: gitlab@gitlab.example.com' ##! can change the 'From' with this setting. gitlab_rails['gitlab_email_from'] = '810654947@qq.com' gitlab_rails['gitlab_email_display_name'] = 'gitlabAdmin' # gitlab_rails['gitlab_email_reply_to'] = 'noreply@example.com' # gitlab_rails['gitlab_email_subject_suffix'] = '' # gitlab_rails['gitlab_email_smime_enabled'] = false # gitlab_rails['gitlab_email_smime_key_file'] = '/etc/gitlab/ssl/gitlab_smime.key' # gitlab_rails['gitlab_email_smime_cert_file'] = '/etc/gitlab/ssl/gitlab_smime.crt' # gitlab_rails['gitlab_email_smime_ca_certs_file'] = '/etc/gitlab/ssl/gitlab_smime_cas.crt' gitlab-ctl stop gitlab-ctl reconfigure gitlab-ctl start gitlab-rails console Notify.test_email('1209233066@qq.com', 'Message Subject', 'Message Body').deliver_now 组织配置 namespace 用于逻辑隔离，可以分为用户名称空间，组名称空间，子组名称空间。\nuser namespace\ngroup namespace subgroup namespace\n创建子组 用户管理\n添加成员 加入组 分配角色 groups 用户管理一个或多个项目\n项目管理 创建项目、导入项目 版本升级 不要跨越大版本，直接下载rpm包更新\n创建一个pytc的组 创建一个项目seagull，隶属于pytc 的组\n添加dev01/dev02 属于该组织\n触发jienkins构建 路径： 找到对应项目 \u003e setting \u003e Webhooks 选择对应触发事件进行测试\n忘记root密码 https://blog.csdn.net/qq_21017997/article/details/131420935\n","categories":["devops"],"description":"\ngitlab||devops\r\n","excerpt":"\ngitlab||devops\r\n","ref":"/devops/gitlab.html","tags":["gitlab"],"title":"gitlab"},{"body":"\n服务部署 端口 8080 5000\n数据目录\nmkdir /jenkins pvcreate /dev/sdb vgextend centos /dev/sdb lvcreate --name jenkins --size 99G centos mkfs.ext4 /dev/mapper/centos-jenkins echo '/dev/mapper/centos-jenkins /jenkins ext4 defaults,noatime 0 0' \u003e\u003e/etc/fstab mount -a 安装jenkins 安装方式: yum 方式启动 Docker 方式启动 tomcat 方式启动 wget --no-check-certificate -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key # /usr/share/java/jenkins.war 升级时替换该文件 yum install -y java-11-openjdk.x86_64 jenkins echo 'JENKINS_HOME=/jenkins' \u003e/etc/jenkins.env [Service] EnvironmentFile=/etc/jenkins.env USER=root systemctl daemon-reload systemctl restart jenkins systemctl enable jenkins --now docker run --name jenkins \\ --restart=always \\ --ip=192.168.31.241 \\ --network macvlan31 \\ -v /jenkins:/var/jenkins_home \\ -v /usr/local/jdk:/usr/local/jdk \\ -v /usr/local/maven:/usr/local/maven \\ -e JENKINS_Uc=https://mirrors.cloud.tencent.com/ienkins/ \\ -e JENKINS_UC DOWNLOAD=https://mirrors.cloud.tencent.com/jenkins \\ -d jenkins/jenkins:lts JENKINS_HOME=/jenkins prerun() { yum install fontconfig freetype -y } download(){ wget https://download.java.net/java/ga/jdk11/openjdk-11_linux-x64_bin.tar.gz wget https://mirrors.tuna.tsinghua.edu.cn/jenkins/war-stable/2.452.1/jenkins.war wget https://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-8/v8.5.100/bin/apache-tomcat-8.5.100.tar.gz } jdk(){ tar xf openjdk-11_linux-x64_bin.tar.gz -C /opt ln -svf /opt/{jdk-11,jdk} cat\u003e\u003e/etc/profile\u003c\u003cEOF export JAVA_HOME=/opt/jdk export JAVA_JRE=\\$JAVA_HOME/jre export CLASSPATH=\\$JAVA_HOME/lib:\\$JAVA_HOME/jre/lib export PATH=\\$JAVA_HOME/bin:\\$JAVA_JRE/bin:$PATH:. EOF source /etc/profile java -version } start(){ echo \"export JENKINS_HOME=${JENKINS_HOME}\" /etc/profile source /etc/profile tar xf apache-tomcat-8.5.100.tar.gz -C /opt ln -svf /opt/apache-tomcat-8.5.100/ /opt/tomcat cp jenkins.war /opt/tomcat/webapps/ # 前台启动tomcat /opt/tomcat/bin/catalina.sh run /opt/tomcat/bin/catalina.sh start } main(){ prerun download jdk start } main 安装插件 Info 修改jenkins插件仓库地址\nui操作路径： Dashboard\u003eManager Jenkins\u003ePlugins\u003eAdvanced setting\nhttps://updates.jenkins.io/update-center.json https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json 插件安装命令： jenkins-plugin-cli --plugins uno-choice:2.8.3\n跳过证书认证【 skip-certificate-check 】 中文插件 chinese 【 Localization: Chinese (Simplified) 】 基于RBAC的权限管理 【 Role-based Authorization Strategy 】 参数化构建支持复选框【 uno-choice 】 pipeline流水线插件【 Pipeline 】 pipline可视化插件 【 Pipeline: Stage View】 Blue Ocean插件 【 Blue Ocean 】 聚合git命令【 Git 】 支持参数化构建和jsonpath解析的webhook触发器 【 Generic Webhook Trigger 】 邮件扩展插件 【 Email Extension 】 集成sonarqube扫描插件【 SonarQube Scanner 】 ------------------------- 在consol的输出中添加时间戳 【 Build Timestamp 】 清理流水线中产生的历史文件 【 Workspace Cleanup 】 备份全局配置和job配置【 ThinBackup 】 支持从git中选择分支、版本、tag 的功能【 git parameter 】 Blue Ocean插件 【 BlueOcean Aggregator】 流水线Pipeline插件 【 workflow-aggregator 】 ssh连接到部署主机插件 【 Publish Over SSH 】 把docker作为agent使用【 Docker Slaves 】 构建maven项目【 Maven Integration 】 在gitlab 使用触发器构建时可以匿名触发构建【 Build Authorization Token Root 】 集成ldap认证登录 【 ldap 】 通用环境 git ssh-key认证 docker ansible kubectl 项目编译环境 构建环境: maven 编译环境 Gradle 编译环境 golang 编译环境 nodejs编译环境npm nodejs编译环境yarn php编译环境 python编译环境 jdk 环境\nwget https://download.java.net/java/ga/jdk11/openjdk-11_linux-x64_bin.tar.gz tar xf openjdk-11_linux-x64_bin.tar.gz -C /opt ln -svf /opt/{jdk-11,jdk} cat\u003e\u003e/etc/profile\u003c\u003cEOF export JAVA_HOME=/opt/jdk export JAVA_JRE=\\$JAVA_HOME/jre export CLASSPATH=\\$JAVA_HOME/lib:\\$JAVA_HOME/jre/lib export PATH=\\$JAVA_HOME/bin:\\$JAVA_JRE/bin:$PATH:. EOF source /etc/profile java -version maven 环境\nwget https://mirrors.aliyun.com/apache/maven/maven-3/3.8.8/binaries/apache-maven-3.8.8-bin.tar.gz tar xf apache-maven-3.8.8-bin.tar.gz -C /opt ln -svf /opt/apache-maven-* /opt/maven export PATH=${PATH}:/opt/maven/bin # 设置国内镜像地址 sed -i /\\\u003cmirrors\\\u003e/\\a\"\u003cmirror\u003e\\n \u003cid\u003ealiyunmaven\u003c/id\u003e\\n \u003cmirrorOf\u003e*\u003c/mirrorOf\u003e\\n \u003cname\u003e阿里云公共仓库\u003c/name\u003e\\n \u003curl\u003ehttps://maven.aliyun.com/repository/public\u003c/url\u003e\\n\u003c/mirror\u003e\" /opt/maven/conf/settings.xml mvn clean package -DskipTests=true mvn clean package mvn test jdk 环境\nwget https://download.java.net/java/ga/jdk11/openjdk-11_linux-x64_bin.tar.gz #https://download.java.net/openjdk/jdk17.0.0.1/ri/openjdk-17.0.0.1+2_linux-x64_bin.tar.gz tar xf openjdk-11_linux-x64_bin.tar.gz -C /opt ln -svf /opt/{jdk-11,jdk} cat\u003e\u003e/etc/profile\u003c\u003cEOF export JAVA_HOME=/opt/jdk export JAVA_JRE=\\$JAVA_HOME/jre export CLASSPATH=\\$JAVA_HOME/lib:\\$JAVA_HOME/jre/lib export PATH=\\$JAVA_HOME/bin:\\$JAVA_JRE/bin:$PATH:. EOF source /etc/profile java -version gradle 环境\n# 打包并跳过测试 gradle build -x test golang 1.20.6\n# wget https://golang.google.cn/dl/go1.23.1.linux-amd64.tar.gz wget https://golang.google.cn/dl/go1.20.6.linux-amd64.tar.gz tar xf go1.20.6.linux-amd64.tar.gz -C /opt/ cat \u003e\u003e/etc/profile \u003c\u003cEOF export GOROOT=/opt/go export GOPATH=/opt/gopath export PATH=\\$GOPATH/bin:\\$GOROOT/bin:$PATH export GO111MODULE=\"on\" export GOPROXY=\"https://goproxy.cn,direct\" EOF source /etc/profile go version go env v16.x\nwget https://nodejs.org/dist/v14.21.3/node-v14.21.3-linux-x64.tar.gz tar xf node-v14.21.3-linux-x64.tar.gz -C /opt ln -svf /opt/node-v14.21.3-linux-x64/ /opt/node echo \"export PATH=/opt/node/bin:$PATH\" \u003e\u003e/etc/profile source /etc/profile npm config set registry https://registry.npmmirror.com # https://nodejs.org/zh-cn/download/prebuilt-binaries wget https://nodejs.org/dist/v16.20.2/node-v16.20.2-linux-x64.tar.xz tar xf node-v16.20.2-linux-x64.tar.xz -C /opt ln -svf /opt/node-v16.20.2-linux-x64/ /opt/node echo \"export PATH=/opt/node/bin:$PATH\" \u003e\u003e/etc/profile source /etc/profile npm config set registry https://registry.npmmirror.com 验证\nnode -v # v16.20.2 npm -v # 8.19.4 构建命令\nnpm install --force \u0026\u0026 npm run build 创建一个vue项目\n# 安装vue-cli 工具 npm install vue-cli -g ls /opt/node/bin/ # 初始化项目 vue-init webpack demo cd demo/ # 运行项目 npm run dev v16.x\n# https://nodejs.org/zh-cn/download/prebuilt-binaries wget https://nodejs.org/dist/v16.20.2/node-v16.20.2-linux-x64.tar.xz tar xf node-v16.20.2-linux-x64.tar.xz -C /opt ln -svf /opt/node-v16.20.2-linux-x64/ /opt/node echo \"export PATH=/opt/node/bin:$PATH\" \u003e\u003e/etc/profile source /etc/profile npm config set registry https://registry.npmmirror.com node -v # v16.20.2 npm -v # 8.19.4 npm install yarn -g [root@lavm-ioreaqndwv demo]# ls /opt/node/bin/ corepack node npm npx vue vue-init vue-list yarn yarnpkg [root@lavm-ioreaqndwv demo]# yarn config set registry https://registry.npmmirror.com yarn config v1.22.22 success Set \"registry\" to \"https://registry.npmmirror.com\". Done in 0.05s. # 编译命令 yarn install yarn build 待补充\n待补充\n安装node节点 jenkins 支持分布式构建，按照角色可以分为master 和 node 节点。master 负责调度和控制，node节点负责执行调度来的流水线任务。按照一下步骤配置node节点：\nmaster 开启端口监听\n配置方式： Dashboard \u003e Manage Jenkins \u003e Security\n在jenkins master 端开启用于agent连接的端口,默认是5000 新增一个node 节点\n配置方式： Dashboard \u003e Manage Jenkins \u003e Nodes 新增node节点配置 node节点与master时钟同步\nnode 节点安装java环境\n启动服务\ncurl -sO http://175.178.65.213:8080/jnlpJars/agent.jar java -jar agent.jar -url http://175.178.65.213:8080/ -secret 689e3241630eb34c444894f78caed7eec4c6d83f2b555bd917e8be369f70a59e -name \"seagull-build01\" -workDir \"/jenkins\" tee /usr/lib/systemd/system/jenkins-agent.service \u003c\u003cEOF [Unit] Description=jenkins-agent service https://jenkins.io/ After=network.target [Service] ExecStart=/opt/jdk/bin/java -jar /opt/agent.jar -url http://175.178.65.213:8080/ -secret 689e3241630eb34c444894f78caed7eec4c6d83f2b555bd917e8be369f70a59e -name \"seagull-build01\" -workDir \"/jenkins\" User=root [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable jenkins --now systemctl status jenkins 调度任务测试\npipeline { agent { label 'seagull' } stages { stage('test'){ steps { echo 'test' } } } } 流水线 自由风格流水线 选择freestyle 模式构建，下面是一个简单ci实例：\n# 构建命令 sh -c \"git clone https://gitee.com/mingtian66/magic-api.git \u0026\u0026 cd magic-api \u0026\u0026 git checkout master \u0026\u0026 cd magic-api \u0026\u0026 JAVA_HOME=/opt/jdk-17.0.11;PATH=$JAVA_HOME/bin:$PATH; mvn clean package \" 构建后清理workspace,依赖插件【Workspace Cleanup】\n触发构建 Trigger builds remotely (e.g., from scripts) 【webhook构建】\n配置后打开浏览器在地址栏输入一下内容并回车，会发现magic_api项目被触发了一次构建。\nhttp://10.4.7.250:8080/jenkins/job/magic_api/build?token=54fb6627dbaa37721048e4549db3224d 携带用户名和密码\ncurl -uadmin:admin http://10.4.7.250:8080/jenkins/job/magic_api/build?token=54fb6627dbaa37721048e4549db3224d 参数化构建\ncurl -uadmin:admin \"http://10.4.7.250:8080/jenkins/job/magic_api/buildWithParameters?token=54fb6627dbaa37721048e4549db3224d\u0026VERSION=4.4.4\" 上面这种方式未登录用户无法触发，如果要绕过这个问题需要安装插件【Build Authorization Token Root】实现匿名构建 调用方式\nhttp://10.4.7.250:8080/jenkins/buildByToken/build?job=magic_api\u0026token=54fb6627dbaa37721048e4549db3224d Build after other projects are built 【其他job触发构建】\nBuild periodically 【周期构建】\nH 表示hash 取随机数字，防止多个job同时构建\n* * * * * 每分钟构建一次 H * * * * 使用 H 代替随机数作为起始时间，每分钟构建一次。这个随机数是有job名称hash后得到的，用于避免多个job同时构建 H/15 * * * * 使用 H 代替随机数作为起始时间，每间隔15分钟构建一次 H(0-10) * * * * 在0-10分钟内取一个随机值作为起始时间，每间隔15分钟构建一次 Poll SCM 【jenkins主动周期性发起检查代码变动，则构建】\npipline 流水线 Info 需要安装 【 Pipeline 】 插件\n优势：灵活性高，代码即服务\n语法：1. 声明式语法 2.脚本式语法 。两者可以结合使用,通过在steps{ } 中嵌入 scripts{} 书写groovy脚本。\npipline 声明式语法 Jenkins Pipeline 的定义被写入一个文本文件（称为Jenkinsfile）\ngraph LR\rA[[pipeline]]\rB[agent]\rC[environment]\rD[options]\rH[triggers]\rE[parameters]\rF[stages]\rG[post]\rF1[stage]\rF12[input]\rF13[when]\rF14[parallel]\rF11[steps]\rF111[script]\rA -.-\u003e|定义流水线任务运行的node| B\rA -.-\u003e C\rA -.-\u003e D\rA -.-\u003e E\rA -.-\u003e F\rA -.-\u003e G\rA -.-\u003e H\rF -.-\u003e agent\rF -.-\u003e environment\rF -.-\u003e F1 -.-\u003e F11\rF1 -.-\u003e F12\rF1 -.-\u003e F13\rF11 -.-\u003e F111\rF1 -.-\u003e F14\rsubgraph 全局配置 B\rC\rD\rH\rE\rF\rG\rend subgraph stages\ragent\rF1\renvironment\rend\rsubgraph stage\rF11\rF12\rF13\rF14\rend subgraph steps\rF111\rend 在jenkins中提供了声明式语法生成器，以下是打开这个生成器的方法：\nagent 指定了流水线在那个节点运行\n位置：全局、stages\n// 在任意节点运行 agent any // 在存在标签为 'nodejs'的节点运行 agent { label 'nodejs' } // agent { label 'my-label1 \u0026\u0026 my-label2' } // agent { label 'my-label1 || my-label2' } // 没有节点运行 agent none 动态agent\n待补充 environment 指定变量\n位置：全局、stages\nenvironment用于定义环境变量，在jenkins中已经包含了一些内置变量，例如：${JOB_NAME}、${BUILD_NUMBER} pipeline { agent any stages { stage(\"test\") { steps { script { println \"构建ID在 jenkins1.597+ 版本后与BUILD_NUMBER 功能一样 ： ${BUILD_ID}\" println \"构建NUMBER ： ${BUILD_NUMBER}\" // 拼接了jenkins-${JOB_NAME}-${BUILD_NUMBER} println \"jenkins-JOB_NAME-BUILD_NUMBER: ${BUILD_TAG}\" println BUILD_URL // println JAVA_HOME println JENKINS_URL println JOB_NAME println NODE_NAME } } } } } 自定义环境变量\nenvironment { JAVA_HOME = \"/opt/jdk\" JAVA_JRE = \"$JAVA_HOME/jre\" CLASSPATH = \"$JAVA_HOME/lib:$JAVA_HOME/jre/lib\" PATH = \"$JAVA_HOME/bin:$JAVA_JRE/bin:$PATH:.\" } 动态接收环境变量\nenvironment { // Using returnStdout CC = \"\"\"${sh( returnStdout: true, script: 'hostname' )}\"\"\" // Using returnStatus EXIT_STATUS = \"\"\"${sh( returnStatus: true, script: 'python3' )}\"\"\" } options 指定流水线选项\n位置：全局\n# 指定历史构建 options { buildDiscarder logRotator(artifactDaysToKeepStr: '1', artifactNumToKeepStr: '5', daysToKeepStr: '1', numToKeepStr: '5') } parameters 指定参数\n位置：全局\nparameters { activeChoice choiceType: 'PT_SINGLE_SELECT', description: '选择发布到哪个环境', filterLength: 1, filterable: false, name: 'envirment', randomName: 'choice-parameter-26043236877113', script: groovyScript(fallbackScript: [classpath: [], oldScript: '', sandbox: false, script: ''], script: [classpath: [], oldScript: '', sandbox: true, script: '''return [ \\'dev\\', \\'uat\\' ]''']) reactiveChoice choiceType: 'PT_CHECKBOX', description: '选择发布到哪台机器', filterLength: 1, filterable: false, name: 'hosts', randomName: 'choice-parameter-26043242534733', referencedParameters: 'envirment', script: groovyScript(fallbackScript: [classpath: [], oldScript: '', sandbox: false, script: ''], script: [classpath: [], oldScript: '', sandbox: true, script: ''' if (envirment== \"dev\") { return [\"db2-dev02-s1\", \"db2-dev03-s1\", \"mongodb-dev01-s1\", \"mongodb-dev02-s1\", \"mongodb-dev03-s1\", \"mongodb-dev04-s1\", \"mongodb-dev05-s1\", \"mongodb-dev06-s1\"] } else if (envirment== \"uat\") { return [\"db2-uat01-s1\", \"db2-uat02-s1\", \"db2-uat03-s1\", \"db2-uat04-s1\", \"seagullvictora01-uat-s2\"] } else { return [\"请选择环境\"] }''']) } triggers\n位置：全局\ntriggers { pollSCM ignorePostCommitHooks: true, scmpoll_spec: 'H 9-18/2 * * 1-5' } input\n位置：stages\ninput { message '确认要部署么' ok '执行' parameters { choice choices: ['deploy', 'rollback'], name: 'action' } } when\n位置：stages\ninput { message '确认要部署么' ok '执行' parameters { choice choices: ['deploy', 'rollback'], name: 'action' } } when { environment name: 'action', value: 'deploy' } parallel 并行构建多阶段\n位置：stages\nstage('parallel') { // 当第一个stage 失败时让整个并行阶段快速失败 failFast true parallel { stage('parallel01') { steps { echo \"123\" } } stage('parallel02') { steps { echo \"456\" } } } } post\npost { always { // One or more steps need to be included within each condition's block. } success { // One or more steps need to be included within each condition's block. } failure { // One or more steps need to be included within each condition's block. } } ","categories":["devops"],"description":"\njenkins|devops\r\n","excerpt":"\njenkins|devops\r\n","ref":"/devops/jenkins.html","tags":["jenkins"],"title":"jenkins"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes/kubernetes_setup/","tags":"","title":"环境搭建"},{"body":"版本选择 企业版和社区版的区别\n安装前环境检查 操作系统 支持部署在x86_64、arm_64、容器环境的 Linux 操作系统。\nWarning TiDB 在 v8.4.0 DMR 和 v8.5.0 版本中移除了对 glibc 2.17 的适配，以及对 CentOS Linux 7 的兼容性测试和支持，建议使用 Rocky Linux 9.1 及以上的版本。\n如果在使用 CentOS Linux 7 的情况下将 TiDB 升级到 v8.4.0 DMR 或 v8.5.0 版本，将存在导致集群不可用的风险。\n为了更好地服务仍在使用 CentOS Linux 7 的用户，TiDB 从 v8.5.1 版本起重新适配 glibc 2.17，恢复了对 CentOS Linux 7 的兼容性支持和测试。\n软件要求: 中控机 目标主机 软件 版本 sshpass 1.06 及以上 TiUP 1.5.0 及以上 软件 版本 sshpass 1.06 及以上 numa 2.0.12 及以上 tar 任意 Info 生产环境中的 TiDB 和 PD 可以部署和运行在同一台服务器上，如对性能和可靠性有更高的要求，应尽可能分开部署。 强烈建议分别为生产环境中的 TiDB、TiKV 和 TiFlash 配置至少 8 核的 CPU。强烈推荐使用更高的配置，以获得更好的性能。 TiKV 硬盘大小配置建议 PCIe SSD 不超过 4 TB，普通 SSD 不超过 1.5 TB。 硬件要求: 开发及测试环境 生产环境 组件 CPU 内存 本地存储 网络 实例数量(最低要求) TiDB 8 核+ 16 GB+ 磁盘空间要求 千兆网卡 1（可与 PD 同机器） PD 4 核+ 8 GB+ SAS, 200 GB+ 千兆网卡 1（可与 TiDB 同机器） TiKV 8 核+ 32 GB+ SSD, 200 GB+ 千兆网卡 3 TiFlash 32 核+ 64 GB+ SSD, 200 GB+ 千兆网卡 1 TiCDC 8 核+ 16 GB+ SAS, 200 GB+ 千兆网卡 1 组件 CPU 内存 硬盘类型 网络 实例数量(最低要求) TiDB 16 核+ 48 GB+ SSD 万兆网卡（2 块最佳） 2 PD 8 核+ 16 GB+ SSD 万兆网卡（2 块最佳） 3 TiKV 16 核+ 64 GB+ SSD 万兆网卡（2 块最佳） 3 TiFlash 48 核+ 128 GB+ 1 or more SSDs 万兆网卡（2 块最佳） 2 TiCDC 16 核+ 64 GB+ SSD 万兆网卡（2 块最佳） 2 监控 8 核+ 16 GB+ SAS 千兆网卡 1 网络要求\n组件 默认端口 说明 TiDB 4000 应用及 DBA 工具访问通信端口 TiDB 10080 TiDB 状态信息上报通信端口 TiKV 20160 TiKV 通信端口 TiKV 20180 TiKV 状态信息上报通信端口 PD 2379 提供 TiDB 和 PD 通信端口 PD 2380 PD 集群节点间通信端口 TiFlash 9000 TiFlash TCP 服务端口 TiFlash 3930 TiFlash RAFT 服务和 Coprocessor 服务端口 TiFlash 20170 TiFlash Proxy 服务端口 TiFlash 20292 Prometheus 拉取 TiFlash Proxy metrics 端口 TiFlash 8234 Prometheus 拉取 TiFlash metrics 端口 CDC 8300 CDC 通信接口 Monitoring 9090 Prometheus 服务通信端口 Monitoring 12020 NgMonitoring 服务通信端口 Node_exporter 9100 TiDB 集群每个节点的系统信息上报通信端口 Blackbox_exporter 9115 Blackbox_exporter 通信端口，用于 TiDB 集群端口监控 Grafana 3000 Web 监控服务对外服务和客户端(浏览器)访问端口 Alertmanager 9093 告警 web 服务端口 Alertmanager 9094 告警通信端口 ","categories":["tidb"],"description":"prerun| tidb\n","excerpt":"prerun| tidb\n","ref":"/tidb/prerun.html","tags":["tidb","prerun"],"title":"环境检查"},{"body":"argo CD v2.13.3 是一个遵循gitops 理念的持续交付工具,支持对多k8s集群执行部署\nkubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml [root@master01 ~]# kubectl get pod -n argocd NAME READY STATUS RESTARTS AGE argocd-application-controller-0 1/1 Running 0 5m14s argocd-applicationset-controller-7dd8f694d4-zmfbt 1/1 Running 0 5m14s argocd-dex-server-b5885bb5d-cbw4p 1/1 Running 0 5m14s argocd-notifications-controller-564cb78f6f-8qrj9 1/1 Running 0 5m14s argocd-redis-7857fdd468-b2g8m 1/1 Running 0 5m14s argocd-repo-server-5566c77dd9-k55l9 1/1 Running 0 5m14s argocd-server-6c44fb8d8-zvpnn 1/1 Running 0 5m14s 查看初始化密码\nkubectl -n argocd get secret argocd-initial-admin-secret -ojsonpath='{.data.password}'|base64 -d 支持通过ui 界面和argocd 命令操作\n通过argocd 命令操作\nwget https://github.com/argoproj/argo-cd/releases/download/v2.13.3/argocd-linux-amd64 -O /usr/bin/argocd chmod +x /usr/bin/argocd [root@master01 ~]# kubectl get svc -n argocd argocd-server NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE argocd-server NodePort 172.168.227.98 \u003cnone\u003e 80:32322/TCP,443:30534/TCP 71m 登录argocd\n[root@master01 ~]# argocd login argocd-server FATA[0000] dial tcp: lookup argocd-server on 192.168.0.1:53: no such host [root@master01 ~]# argocd login 172.168.227.98 WARNING: server certificate had error: tls: failed to verify certificate: x509: cannot validate certificate for 172.168.227.98 because it doesn't contain any IP SANs. Proceed insecurely (y/n)? y Username: admin Password: 'admin:login' logged in successfully Context '172.168.227.98' updated 更新密码\nargocd account update-password 部署一个demo\n将名称空间从default 切换到 argocd\nkubectl config set-context --current --namespace=argocd 部署应用\nhttps://github.com/argoproj/argocd-example-apps.git\nargocd app create guestbook \\ --repo https://gitee.com/mingtian66/argocd-example-apps.git \\ --path guestbook \\ --dest-server https://kubernetes.default.svc \\ --dest-namespace test 查看app 状态，当前应用并未部署。处于OutOfSync 状态\nargocd app get guestbook 执行部署,该命令从git 仓库获取清单并执行kubectl apply 动作\nargocd app sync guestbook 销毁app\nargocd app delete guestbook 通过ui界面操作\n参考 docs|Release\n","categories":["kubernetes"],"description":"k8s argoCD\n","excerpt":"k8s argoCD\n","ref":"/kubernetes/argoCD.html","tags":["kubernetes"],"title":"ArgoCD"},{"body":"二进制安装 wget https://github.com/prometheus/alertmanager/releases/download/v0.26.0/alertmanager-0.26.0.linux-amd64.tar.gz mkdir /data/alertmanager/{bin,conf/templates,data} -p tar xf alertmanager-0.26.0.linux-amd64.tar.gz mv alertmanager-*.linux-amd64/alertmanager /data/alertmanager/bin/ mv alertmanager-*.linux-amd64/amtool /data/alertmanager/bin/ mv alertmanager-*.linux-amd64/alertmanager.yml /data/alertmanager/conf/ chown -R prometheus:prometheus /data/alertmanager 如果需要使用反向代理时启动参数新增： --web.external-url=\"http://127.0.0.1:9003/alert\"\ntee /usr/lib/systemd/system/alertmanager.service \u003c\u003c'EOF' [Unit] Description=alertmanager service https://prometheus.io/ After=network.target [Service] ExecStartPre=/data/alertmanager/bin/amtool check-config /data/alertmanager/conf/alertmanager.yml ExecStart=/data/alertmanager/bin/alertmanager \\ --config.file=/data/alertmanager/conf/alertmanager.yml \\ --storage.path=/data/alertmanager/data \\ --data.retention=120h \\ --web.external-url=http://prometheus.pytc.com/alert User=prometheus [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable alertmanager --now systemctl status alertmanager prometheus配置\nalerting: alertmanagers: - static_configs: - targets: - 127.0.0.1:9093 path_prefix: /alert 告警接收器 webhook\n通过webhook 接收后可以放入数据库、消息队列等.\n创建alertmanager配置\ntee /data/alertmanager/conf/alertmanager.yml \u003c\u003cEOF global: route: receiver: 'webhook' group_by: ['alertname','cluster'] group_wait: 30s group_interval: 5m receivers: - name: 'webhook' webhook_configs: - url: 'http://127.0.0.1:5001/' EOF 编写一个webhook接收告警 webhook: python golang tee app.py\u003c\u003cEOF\rimport json\rfrom flask import Flask, request, jsonify\r# import smtplib app = Flask(__name__)\r@app.route('/', methods=['POST'])\rdef alert_webhook():\r# 获取post 请求传递是所有信息\rdata=request.get_data()\rprint(json.loads(data))\rreturn jsonify({'status': 'ok'})\rif __name__ == '__main__':\rapp.run(host='0.0.0.0', port=5001, debug=True)\rEOF\rpython3 app.py\rpackage main\rimport (\r\"fmt\"\r\"io\"\r\"net/http\"\r)\rfunc handlePostRequest(w http.ResponseWriter, r *http.Request) {\rbody, _ := io.ReadAll(r.Body)\rdefer r.Body.Close()\rfmt.Println(string(body))\r//fmt.Fprint(w, \"POST request received\")\r}\rfunc main() {\rhttp.HandleFunc(\"/\", handlePostRequest)\rhttp.ListenAndServe(\":5001\", nil)\r}\r触发一条告警\ncurl -X POST -H \"Content-Type: application/json\" -d '[ { \"labels\": { \"alertname\": \"cpuHigh\", \"instance\": \"node01:9100\", \"job\": \"node_exporter\", \"severity\": \"warning\" }, \"annotations\": { \"description\": \"node01:9100 of job node_exporter has been used cpu \u003e85 more than 5 minutes. (current value: 89.99%)\", \"summary\": \"Instance node01:9100 cpu usage more than 85%\" }, \"generatorURL\": \"http://prometheus.pytc.com/prom\" } ]' \"http://127.0.0.1:9093/alert/api/v2/alerts\" alertmanager 接收到了一条告警信息\nwebhook接收到告警信息\n[Warning]\n细心的朋友发现generatorurl 和 externalurl 显示异常，可以通过修改启动命令调整。对于运行在容器中尤为有用\n./prometheus --config.file=prometheus.yml --web.external-url=http://x.x.x.x:9090 ./alertmanager --config.file=alertmanager.yml --web.external-url=http://x.x.x.x:9093 { \"receiver\": \"webhook\", \"status\": \"firing\", \"alerts\": [ { \"status\": \"firing\", \"labels\": { \"alertname\": \"cpuHigh\", \"instance\": \"node01:9100\", \"job\": \"node_exporter\", \"severity\": \"warning\" }, \"annotations\": { \"description\": \"node01:9100 of job node_exporter has been used cpu \u003e85 more than 5 minutes. (current value: 89.99%)\", \"summary\": \"Instance node01:9100 cpu usage more than 85%\" }, \"startsAt\": \"2025-01-12T16:04:33.208058295+08:00\", \"endsAt\": \"0001-01-01T00:00:00Z\", \"generatorURL\": \"http://127.0.0.1\", \"fingerprint\": \"d2a2fb28d1488138\" } ], \"groupLabels\": { \"alertname\": \"cpuHigh\" }, \"commonLabels\": { \"alertname\": \"cpuHigh\", \"instance\": \"node01:9100\", \"job\": \"node_exporter\", \"severity\": \"warning\" }, \"commonAnnotations\": { \"description\": \"node01:9100 of job node_exporter has been used cpu \u003e85 more than 5 minutes. (current value: 89.99%)\", \"summary\": \"Instance node01:9100 cpu usage more than 85%\" }, \"externalURL\": \"http://:9093/alert\", \"version\": \"4\", \"groupKey\": \"{}:{alertname=\\\"cpuHigh\\\"}\", \"truncatedAlerts\": 0 } email\ntee /data/alertmanager/conf/alertmanager.yml \u003c\u003cEOF # 全局定义 global: # The smarthost smtp_smarthost: 'smtp.qq.com:465' # from who send email smtp_from: '810654947@qq.com' smtp_auth_username: '810654947@qq.com' smtp_auth_password: 'kqaexaxpbrbdbajd' # smtp_require_tls: false # 告警媒介 receivers: - name: 'email' email_configs: - to: '810654947@qq.com' send_resolved: true - name: 'webhook' webhook_configs: - url: 'http://192.168.0.20:5001/' # 告警路由 route: # 根路由，任何子路由不能匹配的路由都会发送到根路由 receiver: 'email' group_by: ['alertname','cluster'] group_wait: 30s group_interval: 5m routes: - receiver: 'webhook' matchers: - alertname=~\"disk*\" # 告警模版 templates: - '/data/alertmanager/conf/templates/*.tmpl' EOF 告警模版支持html格式的渲染\n格式示例一：\ntee /data/alertmanager/conf/templates/email.tmpl \u003c\u003cEOF {{ define \"email.default.html\" }} {{ range .Alerts }} {{ if eq .Status \"firing\" }} \u003cspan style=\"background-color:red\"\u003e[{{.Status}}]\u003c/span\u003e| {{ .Annotations.summary }} \u003cbr\u003e 告警级别: {{ .Labels.severity }} 级\u003cbr\u003e 告警名称: {{ .Labels.alertname }} \u003cbr\u003e 告警主机: {{ .Labels.instance }} \u003cbr\u003e 告警详情: {{ .Annotations.description }} \u003cbr\u003e 触发时间: {{ .StartsAt.Format \"2006-01-02 15:04:05\" }} \u003cbr\u003e {{ else if eq .Status \"resolved\" }} \u003cspan style=\"background-color:green\"\u003e[{{.Status}}]\u003c/span\u003e {{ .Annotations.summary }} 已恢复正常\u003cbr\u003e 告警级别: {{ .Labels.severity }} 级\u003cbr\u003e 告警名称: {{ .Labels.alertname }} \u003cbr\u003e 告警主机: {{ .Labels.instance }} \u003cbr\u003e 告警详情: {{ .Annotations.description }} \u003cbr\u003e 恢复时间: {{ .StartsAt.Format \"2006-01-02 15:04:05\" }} \u003cbr\u003e {{ end }} {{ end }} {{ end }} EOF 模版效果\n格式示例二：\ntee /data/alertmanager/conf/templates/email.tmpl \u003c\u003cEOF {{ define \"email.default.html\" }} \u003ctable border=1\u003e \u003cth\u003e告警来源\u003c/th\u003e \u003cth\u003e告警级别\u003c/th\u003e \u003cth\u003e告警名称\u003c/th\u003e \u003cth\u003e告警主机\u003c/th\u003e \u003cth\u003e告警主题\u003c/th\u003e \u003cth\u003e告警详情\u003c/th\u003e \u003cth\u003e触发时间\u003c/th\u003e {{ range .Alerts }} \u003ctr\u003e \u003ctd\u003eprometheus_alert\u003c/td\u003e \u003ctd\u003e{{ .Labels.severity }}\u003c/td\u003e \u003ctd\u003e{{ .Labels.alertname }} \u003c/td\u003e \u003ctd\u003e{{ .Labels.instance }}\u003c/td\u003e \u003ctd\u003e{{ .Annotations.summary }}\u003c/td\u003e \u003ctd\u003e{{ .Annotations.description }}\u003c/td\u003e \u003ctd\u003e{{ .StartsAt.Format \"2006-01-02 15:04:05\" }}\u003c/td\u003e \u003c/tr\u003e {{ end }} \u003c/table\u003e {{ end }} EOF 模版效果\nwechart\ntee /data/alertmanager/conf/alertmanager.yml \u003c\u003cEOF global: # The smarthost smtp_smarthost: 'smtp.qq.com:465' # from who send email smtp_from: '810654947@qq.com' smtp_auth_username: '810654947@qq.com' smtp_auth_password: 'kqaexaxpbrbdbajd' # smtp_require_tls: false route: receiver: 'wechat' group_by: ['alertname','cluster'] group_wait: 30s group_interval: 5m receivers: - name: 'email' email_configs: - to: '810654947@qq.com' send_resolved: true - name: 'wechat' wechat_configs: # 企业ID。我的企业--企业信息中查看 - corp_id: 'wwa5d04854e39e7784' # 应用ID。应用管理 agent_id: '1000002' # 应用secret。应用管理 api_secret: 'Rc8nQbF8EiCvO1JP19S9vceTI4LJaNt-j-H1pCBba2U' # 发送给谁 to_user: \"@all\" # 发送给哪个部门。部门ID， 在通讯录中查看 to_party: '2' templates: - '/data/alertmanager/conf/templates/*.tmpl' EOF tee /data/alertmanager/conf/templates/wechat.tmpl \u003c\u003cEOF {{ define \"wechat.default.message\" }} 状态: {{ .Status }} {{ range .Alerts }} ==============监控报警============== 告警程序: prometheus_alert 告警级别: {{ .Labels.severity }} 告警名称: {{ .Labels.alertname }} 告警主机: {{ .Labels.instance }} 告警主题: {{ .Annotations.summary }} 告警详情: {{ .Annotations.description }} 触发事件: {{ .StartsAt.Format \"2006-01-02 15:04:05\" }} {{ end }} {{ end }} EOF dingtalk\ntee /data/alertmanager/conf/alertmanager.yml \u003c\u003cEOF global: # The smarthost smtp_smarthost: 'smtp.qq.com:465' # from who send email smtp_from: '810654947@qq.com' smtp_auth_username: '810654947@qq.com' smtp_auth_password: 'kqaexaxpbrbdbajd' # smtp_require_tls: false route: receiver: 'dingtalk' group_by: ['alertname','cluster'] group_wait: 30s group_interval: 5m receivers: - name: 'email' email_configs: - to: '810654947@qq.com' send_resolved: true - name: 'wechat' wechat_configs: # 企业ID。我的企业--企业信息中查看 - corp_id: 'wwa5d04854e39e7784' # 应用ID。应用管理 agent_id: '1000002' # 应用secret。应用管理 api_secret: 'Rc8nQbF8EiCvO1JP19S9vceTI4LJaNt-j-H1pCBba2U' # 发送给谁 to_user: \"@all\" # 发送给哪个部门。部门ID， 在通讯录中查看 to_party: '2' - name: 'dingtalk' webhook_configs: - url: 'http://127.0.0.1:8060/dingtalk/webhook1/send' templates: - '/data/alertmanager/conf/templates/*.tmpl' EOF 安装钉钉webhook\nwget https://raw.githubusercontent.com/timonwong/prometheus-webhook-dingtalk/main/web/ui/react-app/src/pages/PlaygroundDemoAlert.json # 测试prometheus-webhook-dingtalk 是否可以使用 [root@master prometheus]# curl -X POST -d'@PlaygroundDemoAlert.json' http://10.4.7.10:8060/dingtalk/webhook1/send OK mkdir /data/dingtalk/{bin,conf,conf/templates} -p tee /data/dingtalk/conf/config.yml \u003c\u003cEOF templates: - \"/prometheus-webhook-dingtalk/templates/*.tmpl\" targets: webhook1: url: https://oapi.dingtalk.com/robot/send?access_token=2a8703f74d0960dd8effc6b846c7473520363e4c171fbdfa044b586df3e29be0 # secret for signature secret: SEC2475a4ecdfae996614c6310b18bf948ce95b4c5df306cf75b2842b7a061bba43 message: text: '{{ template \"ding.link.content\" . }}' EOF tee /data/dingtalk/bin/start.sh \u003c\u003cEOF docker rm -f dingtalk docker run --name dingtalk -d \\ --restart=always \\ -p8060:8060 \\ -v /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime \\ -v /data/dingtalk/conf/config.yml:/prometheus-webhook-dingtalk/config.yml \\ -v /data/dingtalk/conf/templates:/prometheus-webhook-dingtalk/templates \\ timonwong/prometheus-webhook-dingtalk:v2.0.0 \\ --config.file=/prometheus-webhook-dingtalk/config.yml \\ --web.enable-ui \\ --web.enable-lifecycle \\ --log.level=debug EOF http://192.168.0.161:8060/ui 格式示例一：\n修改后重启webhook\ntee /data/dingtalk/conf/templates/dingtalk.tmpl \u003c\u003cEOF {{ define \"ding.link.content\" }} 状态: {{ .Status }} {{ range .Alerts }} ==============监控报警============== 告警程序: prometheus_alert 告警级别: {{ .Labels.severity }} 告警名称: {{ .Labels.alertname }} 告警主机: {{ .Labels.instance }} 告警主题: {{ .Annotations.summary }} 告警详情: {{ .Annotations.description }} 触发事件: {{ .StartsAt.Format \"2006-01-02 15:04:05\" }} {{ end }} {{ end }} EOF 格式示例二：\n修改后重启webhook\ntee /data/dingtalk/conf/templates/dingtalk.tmpl \u003c\u003cEOF {{ define \"ding.link.content\" }} 状态: {{ .Status }} {{ range .Alerts }} ==============监控报警============== | 告警程序 | 告警级别 | 告警名称 | 告警主机 | 告警主题 | 告警详情 | 告警时间 | | ---------------- | ---------------------- | ----------------------- | ---------------------- | -------------------------- | ------------------------------ | -------------------------------------------- | | prometheus_alert | {{ .Labels.severity }} | {{ .Labels.alertname }} | {{ .Labels.instance }} | {{ .Annotations.summary }} | {{ .Annotations.description }} | {{ .StartsAt.Format \"2006-01-02 15:04:05\" }} | {{ end }} {{ end }} EOF 格式示例三：\n修改后重启webhook\ntee /data/dingtalk/conf/templates/dingtalk.tmpl \u003c\u003cEOF {{ define \"ding.link.content\" }} ![](https://img1.baidu.com/it/u=1546227440,2897989905\u0026fm=253\u0026fmt=auto\u0026app=138\u0026f=JPEG?w=889\u0026h=500) {{ range .Alerts }} ## 【{{ .Status }}】 --- \u003e【告警主题】\u003cbr\u003e {{ .Annotations.summary }} --- \u003e【告警描述】\u003cbr\u003e {{ .Annotations.description }} --- \u003e【触发时间】{{ .StartsAt.Format \"2006-01-02 15:04:05\" }} --- [查看详情](http://baidu.com) --- {{ end }} {{ end }} EOF message\n高可用 集群间通过 gossip协议 管理成员和检测成员故障，集群成员默认通过9094 通讯，客户端通过9093 与alertmanager 通讯。\n集群成员间并不存在主节点，所有节点对等，alertmanager.yml 配置文件内容相同，即使是3个节点的集群中2个成员都宕机也不影响功能\n节点1：\ntee /usr/lib/systemd/system/alertmanager.service \u003c\u003cEOF [Unit] Description=alertmanager service https://prometheus.io/ After=network.target [Service] ExecStartPre=/data/alertmanager/bin/amtool check-config /data/alertmanager/conf/alertmanager.yml ExecStart=/data/alertmanager/bin/alertmanager \\ --config.file=/data/alertmanager/conf/alertmanager.yml \\ --storage.path=\"/data/alertmanager/data\" \\ --data.retention=120h \\ --web.listen-address=\":9093\" \\ --cluster.listen-address=\"0.0.0.0:9094\" \\ --cluster.peer 10.4.7.251:9094 \\ --cluster.peer 10.4.7.252:9094 \\ --cluster.peer 10.4.7.253:9094 User=prometheus [Install] WantedBy=multi-user.target EOF 节点2：\ntee /usr/lib/systemd/system/alertmanager.service \u003c\u003cEOF [Unit] Description=alertmanager service https://prometheus.io/ After=network.target [Service] ExecStartPre=/data/alertmanager/bin/amtool check-config /data/alertmanager/conf/alertmanager.yml ExecStart=/data/alertmanager/bin/alertmanager \\ --config.file=/data/alertmanager/conf/alertmanager.yml \\ --storage.path=\"/data/alertmanager/data\" \\ --data.retention=120h \\ --web.listen-address=\":9093\" \\ --cluster.listen-address=\"0.0.0.0:9094\" \\ --cluster.peer 10.4.7.251:9094 \\ --cluster.peer 10.4.7.252:9094 \\ --cluster.peer 10.4.7.253:9094 User=prometheus [Install] WantedBy=multi-user.target EOF 节点3：\ntee /usr/lib/systemd/system/alertmanager.service \u003c\u003cEOF [Unit] Description=alertmanager service https://prometheus.io/ After=network.target [Service] ExecStartPre=/data/alertmanager/bin/amtool check-config /data/alertmanager/conf/alertmanager.yml ExecStart=/data/alertmanager/bin/alertmanager \\ --config.file=/data/alertmanager/conf/alertmanager.yml \\ --storage.path=\"/data/alertmanager/data\" \\ --data.retention=120h \\ --web.listen-address=\":9093\" \\ --cluster.listen-address=\"0.0.0.0:9094\" \\ --cluster.peer 10.4.7.251:9094 \\ --cluster.peer 10.4.7.252:9094 \\ --cluster.peer 10.4.7.253:9094 User=prometheus [Install] WantedBy=multi-user.target EOF 部署在k8s环境 https://github.com/turnbullpress/prometheusbook-code/blob/master/12-13/alertmanager.yml\napiVersion: v1 kind: Service metadata: name: alertmanager-webui namespace: monitoring labels: app: alertmanager component: core annotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9093\" external-dns.alpha.kubernetes.io/hostname: alertmanager.quicknuke.com. spec: type: LoadBalancer ports: - port: 9093 name: metrics selector: app: alertmanager component: core --- apiVersion: v1 kind: Service metadata: name: alertmanager namespace: monitoring labels: app: alertmanager component: core spec: ports: - port: 9093 name: cluster type: ClusterIP clusterIP: None selector: app: alertmanager component: core --- apiVersion: apps/v1beta2 kind: StatefulSet metadata: name: alertmanager namespace: monitoring labels: app: alertmanager component: core spec: updateStrategy: type: RollingUpdate replicas: 3 selector: matchLabels: app: alertmanager component: core serviceName: alertmanager template: metadata: labels: app: alertmanager component: core spec: containers: - name: alertmanager image: quay.io/prometheus/alertmanager:master imagePullPolicy: IfNotPresent command: - \"sh\" - \"-c\" args: - /bin/alertmanager --config.file=/etc/alertmanager/config.yml --web.listen-address=0.0.0.0:9093 --cluster.listen-address=0.0.0.0:8001 --storage.path=/alertmanager --cluster.peer=\"alertmanager-0.alertmanager.monitoring.svc:8001\" --cluster.peer=\"alertmanager-1.alertmanager.monitoring.svc:8001\" --cluster.peer=\"alertmanager-2.alertmanager.monitoring.svc:8001\" --log.level=debug ports: - containerPort: 9093 name: web protocol: TCP - containerPort: 8001 name: cluster protocol: TCP livenessProbe: httpGet: path: /api/v1/status port: web scheme: HTTP failureThreshold: 10 readinessProbe: failureThreshold: 10 httpGet: path: /api/v1/status port: web scheme: HTTP initialDelaySeconds: 3 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 3 volumeMounts: - name: alertmanager-config-volume mountPath: /etc/alertmanager/ - name: alertmanager-data-volume mountPath: /alertmanager/ volumes: - name: alertmanager-config-volume configMap: name: alertmanager-server-conf - name: alertmanager-data-volume emptyDir: {} ","categories":["prometheus","alertmanager"],"description":"\ninstall|alertmanager|prometheus\r\n\r\n","excerpt":"\ninstall|alertmanager|prometheus\r\n\r\n","ref":"/prometheus/alertmanager/install.html","tags":["prometheus","alertmanager","install"],"title":"部署alertmanager"},{"body":"\n二进制安装 添加用户\nuseradd prometheus --system -s /sbin/nologin 挂载磁盘\n# 创建一个名为data的thinpool vgName=centos lvcreate -L 100G --thinpool data ${vgName} # 创建名为prometheus的lv lvcreate -n prometheus --virtualsize 500G --thinpool data ${vgName} # 格式化 mkfs.ext4 /dev/${vgName}/prometheus # 创建prometheus目录 mkdir /data/prometheus -p # 开机自启动挂载 echo \"$(blkid /dev/mapper/${vgName}-prometheus|awk '{print $2}') /data/prometheus ext4 defaults,noatime,nodelalloc 0 0\" \u003e\u003e/etc/fstab # 挂载验证 mount -a df -hT /data/prometheus/ 下载长期支持版\nRelease Date End of support Prometheus 2.37 2022-07-14 2023-07-31 Prometheus 2.45 2023-06-23 2024-07-31 Prometheus 2.53 2024-06-16 2025-07-31 version=2.53.0 wget https://github.com/prometheus/prometheus/releases/download/v${version}/prometheus-${version}.linux-amd64.tar.gz tar xf prometheus-${version}.linux-amd64.tar.gz -C /data mkdir /data/prometheus/{bin,conf,conf/rules,data,log} -p cp /data/prometheus-${version}.linux-amd64/prometheus /data/prometheus/bin/ cp /data/prometheus-${version}.linux-amd64/promtool /data/prometheus/bin/ cp /data/prometheus-${version}.linux-amd64/prometheus.yml /data/prometheus/conf cp -a /data/prometheus-${version}.linux-amd64/console* /data/prometheus/conf chown -R prometheus:prometheus /data/prometheus* /data/prometheus ├── bin │ ├── prometheus │ └── promtool ├── conf │ └── prometheus.yml ├── data └── log 4 directories, 3 files 通过systemd管理服务\n--storage.tsdb.retention.time 替代了--storage.tsdb.retention\n提供远程写入到prometheus的功能:\n老版本 --enable-feature=remote-write-receiver\n新版本 --web.enable-remote-write-receiver\ntee /usr/lib/systemd/system/prometheus.service \u003c\u003c'EOF' [Unit] Description=promethues service https://prometheus.io/ After=network.target [Service] ExecStartPre=/data/prometheus/bin/promtool check config /data/prometheus/conf/prometheus.yml ExecStart=/data/prometheus/bin/prometheus \\ --web.listen-address=0.0.0.0:9090 \\ --config.file=/data/prometheus/conf/prometheus.yml \\ --web.read-timeout=5m \\ --web.max-connections=10 \\ --storage.tsdb.retention.time=15d \\ --storage.tsdb.retention.size=100GB \\ --storage.tsdb.path=/data/prometheus/data \\ --query.max-concurrency=20 \\ --query.timeout=2m \\ --web.console.templates=/data/prometheus/conf/consoles \\ --web.console.libraries=/data/prometheus/conf/console_libraries \\ --web.enable-lifecycle \\ --web.enable-admin-api \\ --web.external-url=/prom ExecReload=/usr/bin/curl -s -X POST 127.0.0.1:9090/-/reload User=prometheus [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable prometheus --now systemctl status prometheus ","categories":["prometheus","监控"],"description":"\n安装|prometheus\r\n\r\n","excerpt":"\n安装|prometheus\r\n\r\n","ref":"/prometheus/install.html","tags":["prometheus","安装"],"title":"install"},{"body":"简介 Go 语言最初由谷歌公司的 Robert Griesemer、Ken Thompson 和 Rob Pike 于 2007年开始设计发明\n开发环境安装 数据类型 基本数据类型： int/float 、string 、bool\n复合数据类型：指针、数组、切片、集合、结构体、接口、channel、函数\n流程控制语句 if、swith、for 、goto\n包 社区通用项目结构\n常用的包\n并发编程 框架 ","categories":"","description":"\ngolang 文档中心\r\n","excerpt":"\ngolang 文档中心\r\n","ref":"/docs/code/golang/","tags":"","title":""},{"body":"windows环境安装 GO111MODULE=on GOPATH=d:\\gopath GOPROXY=https://goproxy.cn GOROOT=D:\\go go version go env 在linux环境安装 # centos 启动桌面环境 yum groupinstall \"X Window System\" yum groupinstall -y \"GNOME Desktop\" init 5 # ubuntu server 启动桌面环境 sudo apt install ubuntu-desktop init 5 安装golang\n#https://studygolang.com/dl wget https://golang.google.cn/dl/go1.19.6.linux-amd64.tar.gz tar xf go1.19.6.linux-amd64.tar.gz -C /opt/ mkdir /home/pc/golang -p root@golang:~# ls /opt/go/ api codereview.cfg doc LICENSE PATENTS README.md src VERSION bin CONTRIBUTING.md lib misc pkg SECURITY.md test cat \u003e\u003e/etc/profile \u003c\u003cEOF export GOROOT=/opt/go export GOPATH=/opt/gopath export PATH=\\$GOROOT/bin:$PATH export GO111MODULE=\"on\" export GOPROXY=\"https://goproxy.cn\" EOF source /etc/profile go version go env 安装vscode\nhttps://vscode.download.prss.microsoft.com/dbazure/download/stable/019f4d1419fbc8219a181fab7892ebccf7ee29a2/code-stable-x64-1709077346.tar.gz 启动vscode\n/opt/VSCode-linux-x64/code --no-sandbox sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc sudo sh -c 'echo -e \"[code]\\nname=Visual Studio Code\\nbaseurl=https://packages.microsoft.com/yumrepos/vscode\\nenabled=1\\ngpgcheck=1\\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc\" \u003e /etc/yum.repos.d/vscode.repo' yum check-update sudo yum install code vscode golang插件安装\nctrl + shift + p 输入 install/Update Tools 选择所有提示安装的插件 demo mkdir -p lvminfo/{utils,cmd} go mod init wangendao.com/lvminfo lvminfo/ ├── cmd │ └── main.go\t# 入口文件 ├── go.mod └── utils\t# 内部包存放目录 └── getinfo.go // utils/getinfo.go package utils import \"os/exec\" func Getlvinfo() []byte { lvs := exec.Command(\"lvs\") result, _ := lvs.Output() return result } // cmd/main.go package main import ( \"fmt\" \"wangendao.com/lvminfo/utils\" ) func main() { a := utils.Getlvinfo() fmt.Println(string(a)) } ","categories":["golang"],"description":"\ngo|开发环境\r\n\r\n","excerpt":"\ngo|开发环境\r\n\r\n","ref":"/golang/install.html","tags":["golang","环境设置"],"title":"环境安装"},{"body":" 内容概要 安装、配置hugo\u0026docsy环境 初始化站点: 开发者工具 初始化站点 go version 1.18+ nodejs 1.16+ hugo_extended version 0.110.0+ git hugo new site my-new-site --format yaml cd my-new-site git init hugo mod init github.com/me/my-new-site hugo mod get github.com/google/docsy@v0.11.0 npm install -D autoprefixer postcss-cli postcss # hugo server cat \u003e hugo.yaml \u003c\u003cEOL enableEmoji: true # 支持emoji module: proxy: direct hugoVersion: extended: true min: 0.73.0 imports: - path: github.com/google/docsy disable: false markup: goldmark: renderer: unsafe: true # 支持markdown 中html css的解析 cjk: # 中文配置 enable: true eastAsianLineBreaks: true # 按中文换行规则 escapedSpace: true # 保留空格转义 EOL 初始化顶级菜单 hugo.yaml\nmenu: main: - name: 关于 weight: 100 url: '/about' pre: \u003ci class=\"fa-solid fa-house\"\u003e\u003c/i\u003e # 图标 https://fontawesome.com/ - name: 文档 weight: 200 url: '/docs' pre: \u003ci class=\"fa-solid fa-book\"\u003e\u003c/i\u003e - name: 博客 weight: 300 url: '/blog' pre: \u003ci class=\"fa-solid fa-blog\"\u003e\u003c/i\u003e - weight: 400 url: 'https://github.com/google/docsy/' pre: \u003ci class='fa-brands fa-github'\u003e\u003c/i\u003e 增加内容页面 网站content目录结构\ncontent ├─_index.md ├─about.md ├─blog | |_ _index.md └─docs |- _index.md ├─golang | |- _index.md | |_ package | |_ _index.md └─hugo |_ _index.md _index.md文件至关重要，负责初始化目录结构。以下是相对于content目录下创建的_index.md文件内容:\n索引文件配置: _index.md docs/_index.md docs/hugo/_index.md docs/golang/_index.md --- title: \"HOME\" --- --- title: \"docs\" --- --- title: \"\" linkTitle: \"hugo\" date: 2025-05-09 simple_list: true weight: 3 description: \u003e hugo 文档中心 icon: fa-solid fa-screwdriver-wrench --- --- title: \"\" linkTitle: \"golang\" date: 2025-05-09 simple_list: true weight: 5 description: \u003e golang 文档中心 icon: fa-brands fa-golang --- 新增加第一个页面 content/docs/hugo/hugo.md\nhugo new content docs/hugo/hugo.md -k blog --- title: \"建站\" linkTitle: \"建站\" date: 2025-05-09 toc_hide: false #隐藏左侧菜单 hide_summary: true #隐藏描述，在内容页面的front matter中并没有看出来效果。_index.md 可以清楚的看到效果 weight: 3 description: \u003e 使用hugo \u0026 docsy 构建站点 --- 《请书写你的内容》 在章节菜单中例如content/docs 下，默认会展开所有内容，通过以下参数对该区域进行控制:\nhugo.yaml\nparams: ui: sidebar_menu_compact: true #只展开当前章节，其他兄弟章节折叠 ul_show: 1 #展开侧边栏中一级内容，嵌套更深的菜单项初始化处于折叠状态 sidebar_menu_foldable: true #允许手动点击父菜单项实现展开/折叠 sidebar_cache_limit: 1000 #侧边栏生成时的缓存条目限制，加快渲染 为了简化页面,可以禁用面包屑导航的\nparams: ui: breadcrumb_disable: true #Docsy 主题默认会在页面顶部（位于标题下方）生成一个面包屑导航，显示当前页面的层级路径，例如: hugo/建站。此配置用于禁用 taxonomy_breadcrumb_disable: true #例如 tags 或 categories 的列表页或详情页）默认也会显示面包屑导航. 此配置用于禁用 页面设置，同样可以在页面 front matter 中设置页面的图标\n位置：_index.md 或 \u003c具体文档\u003e.md\n--- icon: fa-solid fa-screwdriver-wrench # 图标 https://fontawesome.com/ --- 本地搜索 hugo.yaml\nparams: offlineSearch: true offlineSearchSummaryLength: 200 offlineSearchMaxResults: 25 Mermaid 渲染 hugo.yaml\nparams: mermaid: version: 10.9.0 示例：\nsequenceDiagram\rautonumber\rDocsy user-\u003e\u003eDiscussion board: Ask question\rDiscussion board-\u003e\u003eCommunity member: read question\rloop Different strategies\rCommunity member-\u003e\u003eTest instance: Investigate issue raised\rend\rNote right of Community member: After hours of investigation:\rTest instance--\u003e\u003eCommunity member: Come up with solution\rCommunity member--\u003e\u003eDiscussion board: Propose solution\rDiscussion board--\u003e\u003eDocsy user: check proposed solution\rDocsy user-\u003e\u003eDiscussion board: Mark question as resolved\rDocsy user-\u003e\u003eDocsy user: Being happy flowchart TD\rB[\"fa:fa-twitter for peace\"]\rB--\u003eC[fa:fa-ban forbidden]\rB--\u003eD(fa:fa-spinner)\rB--\u003eE(A fa:fa-camera-retro perhaps?) Warning 由于hugo(hugo v0.146.7)版本问题，引入了try方法处理错误，原有的语法不在兼容，因此需要覆盖模版配置: layouts\\partials\\scripts\\mermaid.html\n{{ $version := .Site.Params.mermaid.version | default \"latest\" -}} {{ $cdnurl := printf \"https://cdn.jsdelivr.net/npm/mermaid@%s/dist/mermaid.esm.min.mjs\" $version -}} {{ with try (resources.GetRemote $cdnurl) }} {{ with .Err -}} {{ errorf \"Could not retrieve mermaid script from CDN. Reason: %s.\" . -}} {{ end -}} {{ else -}} {{ errorf \"Invalid Mermaid version %s, could not retrieve this version from CDN.\" $version -}} {{ end -}} \u003cscript type=\"module\" async\u003e import mermaid from \"{{ $cdnurl }}\"; (function ($) { if ($('.mermaid').length == 0) { mermaid.initialize({ startOnLoad: false }); return; } var params = {{ with .Site.Params.mermaid }}{{ . | jsonify | safeJS }}{{ else }}{}{{- end }}; // Site params are stored with lowercase keys; lookup correct casing // from Mermaid default config. var norm = function (defaultConfig, params) { var result = {}; for (const key in defaultConfig) { const keyLower = key.toLowerCase(); if (defaultConfig.hasOwnProperty(key) \u0026\u0026 params.hasOwnProperty(keyLower)) { if (typeof defaultConfig[key] === \"object\") { result[key] = norm(defaultConfig[key], params[keyLower]); } else { result[key] = params[keyLower]; } } } return result; }; var settings = norm(mermaid.mermaidAPI.defaultConfig, params); settings.startOnLoad = true; if ($('html[data-bs-theme=\"dark\"]').length) { settings.theme = 'dark'; } mermaid.initialize(settings); // Handle light/dark mode theme changes const lightDarkModeThemeChangeHandler = function (mutationsList, observer) { for (const mutation of mutationsList) { if (mutation.type === 'attributes' \u0026\u0026 mutation.attributeName === 'data-bs-theme') { // Mermaid doesn't currently support reinitialization, see // https://github.com/mermaid-js/mermaid/issues/1945. Until then, // just reload the page. location.reload(); } } }; const observer = new MutationObserver(lightDarkModeThemeChangeHandler); observer.observe(document.documentElement, { attributes: true, attributeFilter: ['data-bs-theme'] }); // observer.disconnect(); })(jQuery); \u003c/script\u003e 外观设置 hugo.yaml开启light/dark 模式的切换\nparams: ui: showLightDarkModeMenu: true hugo.yaml支持打印页面\noutputs: section: - HTML - RSS - print 新标签页打开外部链接 位置：layouts/_default/_markup/render-link.html\n\u003ca href=\"{{ .Destination | safeURL }}\"{{ with .Title}} title=\"{{ . }}\"{{ end }}{{ if strings.HasPrefix .Destination \"http\" }} target=\"_blank\"{{ end }}\u003e{{ .Text }}\u003c/a\u003e 评论系统 评论系统：Waline | Waline\n","categories":["hugo"],"description":"\n使用hugo \u0026 docsy 构建站点\r\n\r\n","excerpt":"\n使用hugo \u0026 docsy 构建站点\r\n\r\n","ref":"/hugo/build-site.html","tags":["hugo","建站"],"title":"hugo 工具"},{"body":"包的声明:\n同一目录下的文件必须属于同一个包 package 声明的包名应该和所在目录名称一致。允许不一致 包可以嵌套 同包下的函数不需要导入和直接使用 init() 函数和 main() 函数是golang 保留函数，不能有参数和返回值。init() 可以在任意包中，并且一个包下允许存在多个init()函数main() 函数只能被定义在main包中。 包的导入格式：\n导包方法: 标准导入 自定义别名 省略导入格式 匿名导入 package main import \"fmt\" func main() { fmt.Println(\"hello world\") } package main import f \"fmt\" func main() { f.Println(\"hello world\") } package main import . \"fmt\" func main() { Println(\"hello world\") } 初始化函数通常在这个包执行main()前执行初始化动作，一个包可以包含0或多个init()函数 常见库 库名 类型 功能 fmt 格式化 I/O 提供格式化输入输出函数（如 Printf、Scanf 等） os 系统交互 操作系统功能接口（文件、进程、环境变量等） filepath 文件路径处理 跨平台路径操作（路径拼接、分析、规范化等） io 基础 I/O 定义 Reader/Writer 接口及流式数据操作 bufio 缓冲 I/O 带缓冲的读写操作（纠正自 iobufer，原库名无效） time 时间处理 时间获取、格式化、计算与时区转换 strings 字符串处理 字符串分割、替换、查找等操作 log 日志管理 基础日志记录（支持输出到文件、终端等） encoding/json 数据序列化 JSON 编码与解码 gopkg.in/yaml.v3 数据序列化（第三方） YAML 解析与生成（需手动导入第三方库） net/http 网络通信 HTTP 客户端/服务端实现（构建 Web 应用） flag 命令行工具 解析命令行参数与选项 context 并发控制 跨协程传递请求上下文（超时、取消信号等） sync 并发控制 互斥锁、条件变量、并发安全工具（Mutex/WaitGroup） runtime 系统交互 访问 Go 运行时信息（GC、协程监控等） syscall 系统交互 直接调用操作系统底层接口 path/filepath 文件路径处理 跨平台文件路径解析（已补全标准写法） net 网络通信 底层网络协议（TCP/UDP）与 DNS 解析 net/http/pprof 性能监控 集成性能分析工具（CPU/内存/协程分析） reflect 元编程 运行时类型反射与动态操作 testing 测试框架 单元测试与基准测试 errors 错误处理 错误包装与自定义错误类型 compress/gzip 压缩解压 GZIP 格式压缩与解压 github.com/spf13/cobra 命令行工具（第三方） 强大的 CLI 应用框架（替代 flag 的高级选择） github.com/spf13/viper 配置管理（第三方） 多格式配置读取（JSON/YAML/ENV 等） github.com/sirupsen/logrus 日志管理（第三方） 结构化日志记录（支持 Hook 和多种输出格式） github.com/gin-gonic/gin Web 框架（第三方） 高性能 HTTP Web 框架（路由、中间件等） github.com/gorilla/websocket 网络通信（第三方） WebSocket 协议实现（客户端/服务端） github.com/go-redis/redis 数据存储（第三方） Redis 客户端库 github.com/stretchr/testify 测试工具（第三方） 断言库与 Mock 工具（增强 testing 功能） github.com/prometheus/client_golang 监控（第三方） Prometheus 指标暴露与收集（运维监控必备） github.com/docker/docker 容器交互（第三方） Docker 引擎 API 客户端（容器管理） gorm.io/gorm ORM 框架（第三方） 数据库 ORM 操作（支持 MySQL/PostgreSQL 等） ","categories":["golang"],"description":"库|golang\n","excerpt":"库|golang\n","ref":"/golang/package/package.html","tags":["golang","package"],"title":"包管理"},{"body":"","categories":"","description":"\nhugo 文档中心\r\n","excerpt":"\nhugo 文档中心\r\n","ref":"/docs/hugo/","tags":"","title":""},{"body":"graph LR\rsubgraph \"【主配置文件】\"\rA\u003efilebeat.yml]\rend\rsubgraph filebeat\rB[filebeat] end\rsubgraph output\rC[output]\rend\rsubgraph path\rD[path]\rend\rsubgraph logging\rE[logging]\rend\rsubgraph setup\rF[setup]\rend\rsubgraph monitoring\rG[monitoring]\rend\rA -..-\u003e|\"所有输入、通用设置、全局处理器、模块等[\u003cfont color=red\u003e必须\u003c/font\u003e]\"|B\rA -..-\u003e|\"日志事件发送到的位置[\u003cfont color=red\u003e必须\u003c/font\u003e]\"|C\rA -..-\u003e|\"安装目录、数据目录、日志目录等路径 \"|D\rA -..-\u003e|\"Filebeat 自身日志的级别、文件轮转、格式等\"|E\rA -..-\u003e|\"在 ES 里创建索引模板、ILM 策略、Kibana 仪表板\"|F\rA -..-\u003e|\" Filebeat 自身的指标送进 ES/Logstash，供 Stack Monitoring 查看\"|G ","categories":"","description":"","excerpt":"graph LR\rsubgraph \"【主配置文件】\"\rA\u003efilebeat.yml]\rend\rsubgraph filebeat\rB[filebeat] end\rsubgraph output\rC[output]\rend\rsubgraph path\rD[path]\rend\rsubgraph logging\rE[logging]\rend\rsubgraph setup\rF[setup]\rend …","ref":"/Observability/logs/filebeat/config/","tags":"","title":"config"},{"body":"","categories":"","description":"","excerpt":"","ref":"/Observability/logs/filebeat/","tags":"","title":"filebeat"},{"body":" kafka版本: 0.9+\npython版本：2.7+\n安装：pip install kafka-python==2.2.15\n文档：https://kafka-python.readthedocs.io/en/master/apidoc/KafkaProducer.html\nAPI: 生产者 消费者 管理kafka bootstrap_servers 数据类型 str|list\nKafka 集 群 的 连 接 字 符 串,至少填写kafka 集群中的一个节点。\n示例：\nfrom kafka import KafkaProducer producer = KafkaProducer(bootstrap_servers='192.168.0.151:9092,192.168.0.152:9092,192.168.0.153:9092') # producer = KafkaProducer(bootstrap_servers=['192.168.0.151:9092','192.168.0.152:9092','192.168.0.153:9092']) for i in range(100): producer.send('foobar', b'some_message_bytes') producer.flush() # 强制把缓冲区消息发出去 producer.close() # 优雅关闭 acks\n指定了必须要有多少个副本收到消息，才会向生产者返回写入成功\nacks=0\t生产者不等待kafka server 确认写入成功 acks=1\t生产者等待leader副本写入成功,生产者就会收到一个来自kafka服务器的成功响应 acks='all' 或 acks=-1 生产者等待所有副本写入成功，生产者才会收到一个来自kafka服务器的成功响应 示例：\nfrom kafka import KafkaProducer producer = KafkaProducer( bootstrap_servers='192.168.0.151:9092,192.168.0.152:9092,192.168.0.153:9092', acks=1 ) for i in range(100): producer.send('foobar', b'some_message_bytes') producer.flush() producer.close() retries | retry_backoff_ms 生产者向broker 发送消息时，broker 可以返回一个成功响应码或者一个错误响应码。错误响应码可以 分为两种，一种是在重试之后可以解决的，还有一种是无法通过重试解决的。例如，如果broker 返回的是 LEADER_NOT_AVAILABLE 错误，生产者可以尝试重新发送消息。也许在这个时候一个新的首领被选举出来了，那么这次发送就会成功。\nretries 定义最大重试次数\nretry_backoff_ms 定义重试的间隔，默认值100\n示例：\nfrom kafka import KafkaProducer producer = KafkaProducer( bootstrap_servers='192.168.0.151:9092,192.168.0.152:9092,192.168.0.153:9092', acks=1, retries=3, retry_backoff_ms=1000 ) for i in range(100): producer.send('foobar', b'some_message_bytes') producer.flush() producer.close() enable_idempotence\n开启该功能需要设置 acks=-1或 acks='all'\n当开启acks 和 retries 后可能造成生产者重复发送数据到kafka，可以通过开启幂等性进行优化。默认为false\n示例：\nfrom kafka import KafkaProducer producer = KafkaProducer( bootstrap_servers='192.168.0.151:9092,192.168.0.152:9092,192.168.0.153:9092', acks=-1, retries=3, retry_backoff_ms=1000, enable_idempotence=True ) for i in range(100): producer.send('foobar', b'some_message_bytes') producer.flush() producer.close() max_in_flight_requests_per_connection 【前提】\nacks='all' 或 acks=-1 reties=3 不能小于1 生产者在收到broker响应之前可以继续发送多少条消息，但是在开启reties 功能后需要将max_in_flight_requests_per_connection=1 设置为1，从而保证消息的顺序。默认值5（此功能在于提高吞吐量）\n示例：\nfrom kafka import KafkaProducer producer = KafkaProducer( bootstrap_servers='192.168.0.151:9092,192.168.0.152:9092,192.168.0.153:9092', acks=-1, retries=3, retry_backoff_ms=1000, enable_idempotence=True, max_in_flight_requests_per_connection=1 ) for i in range(100): producer.send('foobar', b'some_message_bytes') producer.flush() producer.close() compression_type 指定了消息被发送给 broker 之前使用哪一种压缩算法进行压缩，有效的压缩算法包括‘gzip’, ‘snappy’, ‘lz4’, ‘zstd’ or None。默认值为None\n示例：\nfrom kafka import KafkaProducer producer = KafkaProducer( bootstrap_servers='192.168.0.151:9092,192.168.0.152:9092,192.168.0.153:9092', acks=-1, retries=3, retry_backoff_ms=1000, enable_idempotence=True, max_in_flight_requests_per_connection=1, compression_type='gzip' ) for i in range(100): producer.send('foobar', b'some_message_bytes') producer.flush() producer.close() batch_size | linger_ms 当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。KafkaProducer 会在批次填满或 linger_ms 达到上限时把批次发送出去\nbatch_size 参数指定了一个批次可以使用的内存大小，按照字节数计算。当批次被填满，批次里的所有消息会被一次性发送出去。\nlinger_ms 指定了生产者发送批次的时间间隔\n示例：\nfrom kafka import KafkaProducer producer = KafkaProducer( bootstrap_servers='192.168.0.151:9092,192.168.0.152:9092,192.168.0.153:9092', acks=-1, retries=3, retry_backoff_ms=1000, enable_idempotence=True, max_in_flight_requests_per_connection=1, compression_type='gzip', batch_size=10240, linger_ms=200 ) for i in range(100): producer.send('foobar', b'some_message_bytes') producer.flush() producer.close() key_serializer|value_serializer kafka只接受字节数据，生产者执行序列化，消费者对应反序列化\nfrom kafka import KafkaProducer import json producer = KafkaProducer( bootstrap_servers='192.168.0.151:9092,192.168.0.152:9092,192.168.0.153:9092', acks=-1, retries=3, retry_backoff_ms=1000, enable_idempotence=True, max_in_flight_requests_per_connection=1, compression_type='gzip', batch_size=10240, linger_ms=200, key_serializer=lambda k: json.dumps(k).encode('utf-8') if k else None, value_serializer=lambda v: json.dumps(v).encode('utf-8') if v else None ) for i in range(100): producer.send('foobar', 'some_message_bytes') producer.flush() producer.close() max_block_ms 该参数用来设置生产者内存缓冲区的大小，生产者用它缓冲要发送到服务器的消息。如果应用程序发送消息的速度超过发送到服务器的速度，会导致生产者空间不足。这个时候，send() 方法调用要么被阻塞，要么抛出异常。max.block.ms表示在抛出异常之前可以阻塞一段时间 默认60000\nclient_id 可以是任意的字符串，服务器会用它来识别消息的来源\nrequest_timeout_ms 生产者在发送数据时等待服务器返回响应的时间 ，默认 30000\nmax_request_size 能发送的单个消息的最大值。小于等于broker中 message.max.bytes 配置，否则可能被broker拒绝\nmax_request_size | receive_buffer_bytes | send_buffer_bytes TCP socket 接收和发送数据包的缓冲区大小。\n如果它们被设为 -1，就使用操作系统的默认值。如果生产者或消费者与 broker 处于不同的数据中心，那么可以适当增大这些值，因为跨数据中心的网络一般都有比较高的延迟和比较低的带宽。\nbootstrap_servers 数据类型 str|list\nKafka 集 群 的 连 接 字 符 串,至少填写kafka 集群中的一个节点。\n示例：\nfrom kafka import KafkaConsumer counsumer = KafkaConsumer(bootstrap_servers='192.168.0.151:9092,192.168.0.152:9092,192.168.0.153:9092') # producer = KafkaConsumer(bootstrap_servers=['192.168.0.151:9092','192.168.0.152:9092','192.168.0.153:9092']) client_id\n数据类型 str\n可以是任意字符串，broker 用它来标识从客户端发送过来的消息，通常被用在日志、度量指标和配额里。\n示例：\nfrom kafka import KafkaConsumer consumer = KafkaConsumer( bootstrap_servers='192.168.0.151:9092', client_id='consumer-1' ) group_id\n数据类型 str\n标识消费者组名称\n示例：\nfrom kafka import KafkaConsumer consumer = KafkaConsumer( bootstrap_servers='192.168.0.151:9092', client_id='consumer-1', group_id='group-1' ) enable_auto_commit | auto_commit_interval_ms\nenable_auto_commit 数据类型 bool\n指定了消费者是否自动提交偏移量，默认值是 true。为了尽量避免出现重复数据和数据丢失，可以把它设为 false，由自己控制何时提交偏移量。如果把它设为 true，还可以通过配置 auto_commit_interval_ms\t属性来控制提交的频率。\nauto_commit_interval_ms 自动提交的间隔 默认值5000（5s）\n示例：\nfrom kafka import KafkaConsumer consumer = KafkaConsumer( bootstrap_servers='192.168.0.151:9092', client_id='consumer-1', group_id='group-1', enable_auto_commit=True ) auto_offset_reset\n可选类型 'earliest'、 'latest'\n指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下（因消费者长时间失效，包含偏移量的记录已经过时并被删除）该作何处理。它的默认值是 latest\n示例：\nfrom kafka import KafkaConsumer consumer = KafkaConsumer( bootstrap_servers='192.168.0.151:9092', client_id='client-1', group_id='group-1', enable_auto_commit=True, auto_offset_reset='earliest' ) key_deserializer | value_deserializer\nAny callable that takes a raw message key and returns a deserialized key.\nkey的反序列化，与生产者中key_serializer 对应\n示例：\nfrom kafka import KafkaConsumer consumer = KafkaConsumer( bootstrap_servers='192.168.0.151:9092', client_id='client-1', group_id='group-1', enable_auto_commit=True, auto_offset_reset='earliest', key_deserializer=lambda m: json.loads(m.decode('utf-8')) if m else None, value_deserializer=lambda m: json.loads(m.decode('utf-8')) if m else None ) fetch_min_bytes | fetch_max_wait_ms 目的是在性能和延迟之间找到一个均衡\nfetch_min_bytes broker 在收到消费者的数据请求时，如果可用的数据量小于 fetch_min_bytes 指定的大小，那么它会等到有足够的可用数据时才把它返回给消费者。该参数为了减少网络io\nfetch_max_wait_ms 指定broker最大等待多少ms将响应数据发送给consumer，默认是 500ms。例如：fetch_min_bytes被设为 1MB，fetch_max_wait_ms被设为 100ms, 那么 Kafka 在收到消费者的请求后，要么返回 1MB 数据，要么在 100ms 后返回所有可用的数据，就看哪个条件先得到满足。\n示例：\nfrom kafka import KafkaConsumer consumer = KafkaConsumer( bootstrap_servers='192.168.0.151:9092', client_id='client-1', group_id='group-1', enable_auto_commit=True, auto_offset_reset='earliest', key_deserializer=lambda m: json.loads(m.decode('utf-8')) if m else None, value_deserializer=lambda m: json.loads(m.decode('utf-8')) if m else None, fetch_min_bytes=1024000, fetch_max_wait_ms=100 ) session_timeout_ms | heartbeat_interval_ms 用于检测消费组中消费者是否断开，是否需要执行在均衡\nsession_timeout_ms 指定了消费者在被认为死亡之前可以与服务器断开连接的时间，默认是 3s。如果消费者没有session_timeout_ms 指定的时间内发送心跳给群组协调器，就被认为已经死亡，协调器就会触发再均衡，把它的分区分配给群组里的其他消费者。\nheartbeat_interval_ms 指定了 poll() 方法向协调器发送心跳的频率\n,一般需要同时修改这两个属性通常 heartbeat_interval_ms = 1/3 * session_timeout_ms\n示例：\nfrom kafka import KafkaConsumer consumer = KafkaConsumer( bootstrap_servers='192.168.0.151:9092', client_id='client-1', group_id='group-1', enable_auto_commit=True, auto_offset_reset='earliest', key_deserializer=lambda m: json.loads(m.decode('utf-8')) if m else None, value_deserializer=lambda m: json.loads(m.decode('utf-8')) if m else None, fetch_min_bytes=1024000, fetch_max_wait_ms=100, session_timeout_ms=3, heartbeat_interval_ms=1 ) max_poll_records\n属性用于控制单次调用 call() 方法能够返回的记录数量，可以帮你控制在轮询里需要处理的数据量。默认500\n示例：\nfrom kafka import KafkaConsumer consumer = KafkaConsumer( bootstrap_servers='192.168.0.151:9092', client_id='client-1', group_id='group-1', enable_auto_commit=True, auto_offset_reset='earliest', key_deserializer=lambda m: json.loads(m.decode('utf-8')) if m else None, value_deserializer=lambda m: json.loads(m.decode('utf-8')) if m else None, fetch_min_bytes=1024000, fetch_max_wait_ms=100, session_timeout_ms=3, heartbeat_interval_ms=1, max_poll_records=500 ) partition_assignment_strategy\n根据给定的消费者和主题，决定哪些分区应该被分配给哪个消费者\nRange\n该策略会把主题的若干个连续的分区分配给消费者。假设消费者 C1 和消费者 C2 同时\n订阅了主题 T1 和主题 T2，并且每个主题有 3 个分区。那么消费者 C1 有可能分配到这\n两个主题的分区 0 和分区 1，而消费者 C2 分配到这两个主题的分区 2。因为每个主题\n拥有奇数个分区，而分配是在主题内独立完成的，第一个消费者最后分配到比第二个消\n费者更多的分区。只要使用了 Range 策略，而且分区数量无法被消费者数量整除，就会\n出现这种情况。\nRoundRobin\n该策略把主题的所有分区逐个分配给消费者。如果使用 RoundRobin 策略来给消费者 C1\n和消费者 C2 分配分区，那么消费者 C1 将分到主题 T1 的分区 0 和分区 2 以及主题 T2\n的分区 1，消费者 C2 将分配到主题 T1 的分区 1 以及主题 T2 的分区 0 和分区 2。一般\n来说，如果所有消费者都订阅相同的主题（这种情况很常见），RoundRobin 策略会给所\n有消费者分配相同数量的分区（或最多就差一个分区）。\n示例：\nfrom kafka import KafkaConsumer from kafka.coordinator.assignors.roundrobin import RoundRobinPartitionAssignor from kafka.coordinator.assignors.range import RangePartitionAssignor consumer = KafkaConsumer( bootstrap_servers='192.168.0.151:9092', client_id='client-1', group_id='group-1', enable_auto_commit=True, auto_offset_reset='earliest', key_deserializer=lambda m: json.loads(m.decode('utf-8')) if m else None, value_deserializer=lambda m: json.loads(m.decode('utf-8')) if m else None, fetch_min_bytes=1024000, fetch_max_wait_ms=100, session_timeout_ms=3, heartbeat_interval_ms=1, max_poll_records=500, partition_assignment_strategy=[RoundRobinPartitionAssignor,RangePartitionAssignor] ) receive.buffer.bytes | send.buffer.bytes\nsocket 在读写数据时用到的 TCP 缓冲区也可以设置大小。如果它们被设为 -1，就使用操\n作系统的默认值。如果生产者或消费者与 broker 处于不同的数据中心内，可以适当增大这\n些值，因为跨数据中心的网络一般都有比较高的延迟和比较低的带宽。\n示例：\nfrom kafka import KafkaConsumer from kafka.coordinator.assignors.roundrobin import RoundRobinPartitionAssignor from kafka.coordinator.assignors.range import RangePartitionAssignor consumer = KafkaConsumer( bootstrap_servers='192.168.0.151:9092', client_id='client-1', group_id='group-1', enable_auto_commit=True, auto_offset_reset='earliest', key_deserializer=lambda m: json.loads(m.decode('utf-8')) if m else None, value_deserializer=lambda m: json.loads(m.decode('utf-8')) if m else None, fetch_min_bytes=1024000, fetch_max_wait_ms=100, session_timeout_ms=30000, heartbeat_interval_ms=2000, max_poll_records=500, partition_assignment_strategy=[RoundRobinPartitionAssignor,RangePartitionAssignor], receive_buffer_bytes=-1, send_buffer_bytes=-1 ) # 订阅topic consumer.subscribe(['test']) try: while True: # 拉取topic数据 msg=consumer.poll(timeout_ms=5000, max_records=1) for k,v in msg.items(): for m in v: print(m.value) consumer.commit_async() # time.sleep(10) except KeyboardInterrupt: pass finally: consumer.close() max_partition_fetch_bytes\ncreate_topics\nfrom kafka import KafkaAdminClient from kafka.admin import NewTopic # admin_client = KafkaAdminClient(bootstrap_servers='192.168.0.151:9092') # 创建 kubeevents 和 kubeaudit 多个topic admin_client.create_topics([ NewTopic(name='kubeevents', num_partitions=1, replication_factor=3), NewTopic(name='kubeaudit', num_partitions=3, replication_factor=3), ]) 创建topic时手动指定分区所在的broker,replica_assignments\nfrom kafka import KafkaAdminClient from kafka.admin import NewTopic from kafka.errors import TopicAlreadyExistsError # admin_client = KafkaAdminClient(bootstrap_servers='192.168.0.151:9092') # {0: [1, 2, 0]} 0表示partition 0 ,[1,2,0]对应brokerid 其中列表中第一个id为主分片 try: admin_client.create_topics([ NewTopic(name='kubeevents', num_partitions=1, replication_factor=3), NewTopic(name='kubeaudit', num_partitions=-1, replication_factor=-1,replica_assignments={0: [1, 2, 0], 1: [0, 1, 2], 2: [2, 0, 1]}), ]) except TopicAlreadyExistsError: pass ","categories":["python"],"description":"库|python\n","excerpt":"库|python\n","ref":"/python/package/kafka.html","tags":["python","kafka"],"title":"kafka"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\njdk版本：openjdk 11\nzookeeper版本: 3.4.14\nkafka版本：3.3.2\n2011年初，美国领英公司（LinkedIn）开源了一款基础架构软件，以奥地利作家弗兰兹·卡夫卡（Franz Kafka）的名字命名，之后 LinkedIn将其贡献给 Apache基金会，随后该软件于2012年10月完成孵化并顺利晋升为Apache顶级项目——这便是大名鼎鼎的Apache Kafka\n安装部署 作为高吞吐的数据流引擎，磁盘和网络性能对kafka影响最大。24C 、32GB 、1TB 7200转sas盘2块、1gb/s 带宽\n安装依赖\n安装jdk 和 zookeeper参考 部署zookeeper 部署安装【elk建议Kafka 版本 0.8.2.0+】\n下载软件\n#\tkafaka_\u003cscala版本\u003e-\u003ckafka版本\u003e.tgz\twget https://archive.apache.org/dist/kafka/3.3.2/kafka_2.13-3.3.2.tgz tar xf kafka_2.13-3.3.2.tgz -C /opt ln -sv /opt/{kafka_2.13-3.3.2,kafka} mkdir /opt/kafka/data 修改配置文件\n配置kafka: 单节点配置 集群配置 kafka_svc=kafka-1:9092 broker_id=0 zookeeper_svc=zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181 tee /opt/kafka/config/server.properties \u003c\u003cEOF broker.id=${broker_id} # 尽量使用域名而不是ip listeners=PLAINTEXT://${kafka_svc} advertised.listeners=PLAINTEXT://${kafka_svc} log.dirs=/opt/kafka/data/kafka1-logs zookeeper.connect=${zookeeper_svc} zookeeper.connection.timeout.ms=18000 delete.topic.enable=true unclean.leader.election.enable=false EOF 同一集群中保证broker_id唯一，同时修改 listeners 和 advertised.listeners\n启动服务\ntee /usr/lib/systemd/system/kafka.service \u003c\u003c'EOF' [Unit] Description=zookeeper service Documentation=https://kafka.apache.org/ After=network.target [Service] Type=forking Environment=JAVA_HOME=/opt/jdk ExecStart=/opt/kafka/bin/kafka-server-start.sh -daemon /opt/kafka/config/server.properties ExecStop=/opt/kafka/bin/kafka-server-stop.sh /opt/kafka/config/server.properties [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable kafka --now 功能验证\n# 创建topics /opt/kafka/bin/kafka-topics.sh \\ --bootstrap-server localhost:9092 \\ --create --topic app-log # 查看topics /opt/kafka/bin/kafka-topics.sh \\ --bootstrap-server localhost:9092 \\ --list # 查看topics详情 /opt/kafka/bin/kafka-topics.sh \\ --bootstrap-server localhost:9092 \\ --describe \\ --topic app-log # 向topics产生一个事件 echo -e \"This is my first event\\nThis is my second event\" |\\ /opt/kafka/bin/kafka-console-producer.sh \\ --bootstrap-server localhost:9092 \\ --topic app-log # 从头消费topics事件 /opt/kafka/bin/kafka-console-consumer.sh \\ --bootstrap-server localhost:9092 \\ --topic app-log \\ --from-beginning #This is my first event #This is my second event # 删除topic /opt/kafka/bin/kafka-topics.sh \\ --bootstrap-server localhost:9092 \\ --delete --topic app-log # 创建指定分区和副本数量的topic /opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 \\ --create --topic app-log \\ --replication-factor 2 \\ --partitions 2 #/opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic app-log Topic: app-log TopicId: sJH_EE1KT-aYppxi8Eq6qg PartitionCount: 2 ReplicationFactor: 2 Configs: unclean.leader.election.enable=false Topic: app-log Partition: 0 Leader: 2 Replicas: 2,1 Isr: 2,1 Topic: app-log Partition: 1 Leader: 1 Replicas: 1,0 Isr: 1,0 压测\n# 生产者压测脚本 kafka-producer-perf-test # 消费者压测脚本 kafka-consumer-perf-test ","categories":["kafka"],"description":"\n*如何快速启动一个kafka服务*\r\n\r\n","excerpt":"\n*如何快速启动一个kafka服务*\r\n\r\n","ref":"/database/kafka/install.html","tags":["zookeeper","kafka"],"title":"部署kafka"},{"body":"","categories":["serviceless"],"description":"\n微服务治理\r\n","excerpt":"\n微服务治理\r\n","ref":"/kubernetes/serviceless/nacos.html","tags":["nacos"],"title":"nacos"},{"body":"部署安装 实例名称 mysql-01 示例使用全局变量引用\nexport clsName='mysql-01' 添加用户\nuseradd -s /sbin/nologin -u 3306 -M mysql 安装mysql依赖\n# libaio提供 Linux 原生异步 I/O（AIO）支持，允许程序发起非阻塞的磁盘读写操作，提高高并发场景下的 I/O 性能 yum install -y libaio libaio-devel ncurses ncurses-devel cpanminus mysql软件安装，示例安装在/opt/mysql目录下\n软件安装: 二进制安装 编译安装 软件下载\nwget https://downloads.mysql.com/archives/get/p/23/file/mysql-5.7.40-linux-glibc2.12-x86_64.tar.gz tar xf mysql-5.7.40-linux-glibc2.12-x86_64.tar.gz -C /opt 安装到/opt/mysql下\nln -svf /opt/{mysql-5.7.40-linux-glibc2.12-x86_64,mysql} ln -svf /opt/mysql-5.7.40-linux-glibc2.12-x86_64/bin/mysql /usr/bin/mysql 软件下载,5.7.5之后的版本需要boost\nwget https://downloads.mysql.com/archives/get/p/23/file/mysql-boost-5.7.40.tar.gz tar xf mysql-boost-5.7.40.tar.gz -C /opt cd /opt/mysql-5.7.40 安装编译依赖\nyum install -y cmake libaio-devel gcc-c++ perl-devel cpanminus ncurses-devel cmake . -DCMAKE_INSTALL_PREFIX=/opt/mysql \\ -DMYSQL_DATADIR=/data/instances/${clsName}/data \\ -DMYSQL_UNIX_ADDR=/data/instances/${clsName}/mysql.sock \\ -DDEFAULT_CHARSET=utf8 \\ -DDEFAULT_COLLATION=utf8_general_ci \\ -DWITH_EXTRA_CHARSETs=all \\ -DWITH_INNOBASE_STORAGE_ENGINE=1 \\ -DWITH_FEDERATED_STORAGE_ENGINE=1 \\ -DWITH_BLACKHOLE_STORAGE_ENGINE=1 \\ -DWITH_EXAMPLE_STORAGE_ENGINE=1 \\ -DWITH_ZLIB=system \\ -DWITH_SSL=system \\ -DENABLED_LOCAL_INFILE=1 \\ -DWITH_EMBEDDED_SERVER=1 \\ -DENABLE_DOWNLOADS=1 \\ -DWITH_DEBUG=0 \\ -DWITH_BOOST=boost make -j 4 \u0026\u0026 make install ln -svf /opt/mysql/bin/mysql /usr/bin/mysql 初始化\n# 构建目录结构 mkdir -p /data/instances/${clsName}/{data,binlog,logs,relay_log,conf} chown -R mysql:mysql /data/instances/${clsName} 配置文件 mysql从5.7开始支持 GTID(Global Transaction Identifiers)，开启gtid功能配置如下： gtid-mode=on enforce-gtid-consistency=on cat \u003e/data/instances/${clsName}/conf/my.cnf\u003c\u003cEOF [mysqld] performance_schema=ON server_id=1921680152 port=3306 character-set-server=utf8mb4 basedir=/opt/mysql datadir=/data/instances/${clsName}/data/ pid-file=/data/instances/${clsName}/mysql.pid socket=/data/instances/${clsName}/mysql.sock log_error=/data/instances/${clsName}/logs/mysql-error.log slow_query_log_file=/data/instances/${clsName}/logs/mysql_slow_query.log slow_query_log=on long_query_time=1 log_timestamps=SYSTEM binlog_format=row log-bin=/data/instances/${clsName}/binlog/log_bin log-bin-index=/data/instances/${clsName}/binlog/binlog.index gtid-mode=on enforce-gtid-consistency=on relay_log=/data/instances/${clsName}/relay_log relay_log_index=/data/instances/${clsName}/relaylog/relay-bin.index relay_log_recovery=on default_authentication_plugin=mysql_native_password master_info_repository=table relay_log_info_repository=table EOF 初始化数据库 查看初始化密码 cat /data/instances/${clsName}/logs/mysql-error.log /opt/mysql/bin/mysqld --defaults-file=/data/instances/${clsName}/conf/my.cnf --user=mysql --initialize 启动服务\ntee /usr/lib/systemd/system/mysql.service \u003c\u003c'EOF' [Unit] Description=mysql service https://dev.mysql.com/doc/refman/5.7/en After=network.target [Service] ExecStart=/opt/mysql/bin/mysqld_safe \\ --defaults-file=/data/instances/mysql-01/conf/my.cnf \\ --pid-file=/data/instances/mysql-01/mysql.pid ExecReload=/bin/kill -HUP $MAINPID User=mysql # 设置最大文件描述符 LimitNOFILE=1024 # 设置CPU使用率限制为50% CPUQuota=50% # 设置内存限制为1G MemoryLimit=1G [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable mysql --now systemctl status mysql 创建用户\n--- 修改root密码 ALTER USER 'root'@'localhost' identified by 'li\u003c)\u003c#jyi9S)2024'; --- 超管用户 create user 'root'@'%' identified by 'li\u003c)\u003c#jyi9S)2024'; grant all privileges on *.* to 'root'@'%' WITH GRANT OPTION ; --- 管理用户 create user 'rdsAdmin'@'%' identified by 'li\u003c)\u003c#jyi9S)2024'; grant all privileges on *.* to 'rdsAdmin'@'%' WITH GRANT OPTION ; --- 复制用户 create user 'rdsRpl'@'%' identified by 'li\u003c)\u003c#jyi9S)2024'; GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'rdsRpl'@'%'; flush privileges; ","categories":["mysql"],"description":"mysql 5.7版本二进制安装和编译安装\n","excerpt":"mysql 5.7版本二进制安装和编译安装\n","ref":"/mysql/install5.7.html","tags":["安装mysql"],"title":"install mysql5.7"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\nfilebeat版本：7.17.29\nelasticsearch版本： 7.15.0\nfilebeat.inputs 定义了如何定义和处理日志事件，这里列出常用的input类型\ngraph LR\rsubgraph \"【主配置文件】\"\rA\u003efilebeat.yml]\rend\rsubgraph filebeat\rB[filebeat] end\rsubgraph inputs\rC[inputs]\rend\rA -..-\u003e|\"所有输入、通用设置、全局处理器、模块等[\u003cfont color=red\u003e必须\u003c/font\u003e]\"|B\rB --\u003e C\rC--\u003e log/filestream/container\rC--\u003e journald\rC --\u003esyslog log|filestream|Container log plugin deprecated in 7.16.0 ,use filestream\ncontainer可以看作是log的别名语法完全兼容\npaths [必选参数，[]string]日志路径\nfilebeat.inputs: - type: log enabled: true # 启用该input，默认true id: my-filestream-id # 每一个input必须使用唯一的id，用于filebeat 追踪文件的状态 paths: - /var/log/messages - /var/log/*.log output.console: pretty: true encoding [非必须，string]日志事件解码方式\nfilebeat.inputs: - type: log enabled: true # 启用该input id: my-filestream-id # 每一个input必须使用唯一的id，用于filebeat 追踪文件的状态 paths: - /var/log/messages - /var/log/*.log encoding: utf8 output.console: pretty: true exclude_lines [非必须，string]通过正则排除不需要采集的日志行\nfilebeat.inputs: - type: log enabled: true # 启用该input id: my-filestream-id # 每一个input必须使用唯一的id，用于filebeat 追踪文件的状态 paths: - /var/log/messages - /var/log/*.log encoding: utf8 exclude_lines: # 排除空行和以# 号开头的日志行 - \"^$\" - \"^#.*\" output.console: pretty: true include_lines [非必须，string]采集符合正则规则的日志行\nfilebeat.inputs: - type: log enabled: true # 启用该input id: my-filestream-id # 每一个input必须使用唯一的id，用于filebeat 追踪文件的状态 paths: - /var/log/messages - /var/log/*.log encoding: utf8 exclude_lines: # 排除空行和以# 号开头的日志行 - \"^$\" - \"^#.*\" include_lines: # 仅仅采集错误日志 - \".*(ERR|err).*\" output.console: pretty: true multiline [非必须，map]多行匹配\n日志格式：\ncat \u003e\u003e/var/log/1.log\u003c\u003c'EOF'\r[beat-logstash-some-name-832-2015.11.28] IndexNotFoundException[no such index]\rat org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.resolve(IndexNameExpressionResolver.java:566)\rat org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:133)\rat org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:77)\rat org.elasticsearch.action.admin.indices.delete.TransportDeleteIndexAction.checkBlock(TransportDeleteIndexAction.java:75)\rEOF 如果input类型为log，则书写格式为：\n格式一 格式二 filebeat.inputs: - type: log enabled: true # 启用该input id: my-filestream-id # 每一个input必须使用唯一的id，用于filebeat 追踪文件的状态 paths: - /var/log/1.log encoding: utf8 multiline: # 示例中：把不是 [ 开头视为多行日志，并将不是[ 开头的行追加到前一行的后后面。 type: pattern # 支持pattern 和 count pattern: '^\\[' negate: true # true表示否定模式，即不与pattern 匹配的内容被视为多行，默认为false。 match: after output.console: pretty: true filebeat.inputs: - type: log enabled: true # 启用该input id: my-filestream-id # 每一个input必须使用唯一的id，用于filebeat 追踪文件的状态 paths: - /var/log/1.log encoding: utf8 multiline.type: pattern multiline.pattern: '^\\[' multiline.negate: true multiline.match: after output.console: pretty: true 如果input类型为filestream，则书写格式为：\nfilebeat.inputs: - type: filestream enabled: true # 启用该input id: my-filestream-id # 每一个input必须使用唯一的id，用于filebeat 追踪文件的状态 paths: - /var/log/1.log encoding: utf8 parsers: - multiline: # 示例中：把不是 [ 开头视为多行日志，并将不是[ 开头的行追加到前一行的后后面。 type: pattern # 支持pattern 和 count pattern: '^\\[' negate: true # true表示否定模式，即不与pattern 匹配的内容被视为多行，默认为false。 match: after output.console: pretty: true json [非必须，map]json格式的日志解析为json 键值对\n如果input类型为log，则书写格式为： 如果input类型为filestream，则书写格式为： filebeat.inputs: - type: log enabled: true # 启用该input id: my-filestream-id # 每一个input必须使用唯一的id，用于filebeat 追踪文件的状态 paths: - /var/log/1.log encoding: utf8 json.keys_under_root: true json.message_key: message multiline.type: pattern multiline.pattern: '^\\[' multiline.negate: true multiline.match: after output.console: pretty: true filebeat.inputs: - type: filestream enabled: true # 启用该input id: my-filestream-id # 每一个input必须使用唯一的id，用于filebeat 追踪文件的状态 paths: - /var/log/1.log encoding: utf8 parsers: - multiline: # 示例中：把不是 [ 开头视为多行日志，并将不是[ 开头的行追加到前一行的后后面。 type: pattern # 支持pattern 和 count pattern: '^\\[' negate: true # true表示否定模式，即不与pattern 匹配的内容被视为多行，默认为false。 match: after - ndjson: keys_under_root: true message_key: message output.console: pretty: true harvester_buffer_size [非必须，int] 单个harvester 可以使用的最大buffer。单位bytes\nfilebeat.inputs: - type: log enabled: true # 启用该input id: my-filestream-id # 每一个input必须使用唯一的id，用于filebeat 追踪文件的状态 paths: - /var/log/messages - /var/log/*.log encoding: utf8 exclude_lines: # 排除空行和以# 号开头的日志行 - \"^$\" - \"^#.*\" include_lines: # 仅仅采集错误日志 - \".*(ERR|err).*\" harvester_buffer_size: 16384 # 默认值 16384 = 16k output.console: pretty: true max_bytes [非必须，int] 单条日志的最大bytes。 对于多行日志特别有用\nfilebeat.inputs: - type: log enabled: true # 启用该input id: my-filestream-id # 每一个input必须使用唯一的id，用于filebeat 追踪文件的状态 paths: - /var/log/messages - /var/log/*.log encoding: utf8 exclude_lines: # 排除空行和以# 号开头的日志行 - \"^$\" - \"^#.*\" include_lines: # 仅仅采集错误日志 - \".*(ERR|err).*\" harvester_buffer_size: 16384 # 默认值 16384 = 16k max_bytes: 10485760 # 默认值 10485760 = 10MB output.console: pretty: true tags [非必须，[]string] 为日志文件添加tag 便于kibana 和 logstash 等进行筛选\nfilebeat.inputs: - type: log enabled: true # 启用该input id: my-filestream-id # 每一个input必须使用唯一的id，用于filebeat 追踪文件的状态 paths: - /var/log/messages - /var/log/*.log encoding: utf8 exclude_lines: # 排除空行和以# 号开头的日志行 - \"^$\" - \"^#.*\" include_lines: # 仅仅采集错误日志 - \".*(ERR|err).*\" harvester_buffer_size: 16384 # 默认值 16384 = 16k max_bytes: 10485760 # 默认值 10485760 = 10MB tags: - \"nginx\" output.console: pretty: true fields|fields_under_root [非必须，map] 为日志文件添加自定义字段\nfilebeat.inputs: - type: log enabled: true # 启用该input id: my-filestream-id # 每一个input必须使用唯一的id，用于filebeat 追踪文件的状态 paths: - /var/log/messages - /var/log/*.log encoding: utf8 exclude_lines: # 排除空行和以# 号开头的日志行 - \"^$\" - \"^#.*\" include_lines: # 仅仅采集错误日志 - \".*(ERR|err).*\" harvester_buffer_size: 16384 # 默认值 16384 = 16k max_bytes: 10485760 # 默认值 10485760 = 10MB tags: - \"nginx\" fields: # 为document信息添加额外的信息 env: dev fields_under_root: true # 把fields 中添加的信息至于document信息的顶级 output.console: pretty: true journald 该功能处于实验阶段，7.17下载 之后版本中支持\ninclude_matches [非必须，[]map{}] 收集指定journald日志\n收集服务日志\nfilebeat.inputs: - type: journald id: consul.service include_matches: - _SYSTEMD_UNIT=consul.service - type: journald id: filebeat.service include_matches: - _SYSTEMD_UNIT=filebeat.service 收集内核日志\nfilebeat.inputs: - type: journald id: iptables include_matches: - _TRANSPORT=kernel processors: - drop_event: when.not.regexp.message: '^iptables' Syslog 是一个轻量的syslog 服务，功能上与linux rsyslog 提供的tcp/udp 服务相同。常用于收集网络设备日志\n","categories":["ELK"],"description":"\nfilebeat配置文件之inputs\r\n","excerpt":"\nfilebeat配置文件之inputs\r\n","ref":"/elk/filebeat/config/inputs.html","tags":["input","filebeat"],"title":"filebeat_inputs"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\nelasticsearch版本： 7.15.0、7.17.27\n【注意事项】| doc\nelasticsearch 依赖jdk 环境，如果无法判断使用哪个版本jdk。可以下载/国内下载镜像带有jdk版本的elasticsearch。\nelasticsearch默认监听9200（提供restfull风格的服务） 和 9300 （集群内部选举和数据同步）\nelasticsearch 属于io密集型应用，尽可能把数据目录单独挂载出来。并使用高性能磁盘。\n关闭防火墙、selinux、swap。并时间同步。\n集群部署 节点名称 ip elasticsearch集群名 node-1 192.168.0.151 cluster-alerts node-2 192.168.0.152 cluster-alerts node-3 192.168.0.153 cluster-alerts 集群配置文件: 节点一 节点二 节点三 tee /data/elk/instance1/config/elasticsearch.yml \u003c\u003c'EOF' path: data: /data/elk/instance1/data logs: /data/elk/instance1/log node: name: node-1 master: true # 节点可以是master角色 data: true # 节点可以是data角色 discovery: # 启动时向哪些主机进行宣告，进行集群选举，集群内的主机都要填写上。包括自己 seed_hosts: [\"192.168.0.151\",\"192.168.0.152\",\"192.168.0.153\"] cluster: name: cluster-alerts initial_master_nodes: [\"192.168.0.151\",\"192.168.0.152\",\"192.168.0.153\"] #初始化时哪些节点可以被选举为master ，集群内的主机可以都填写上，也可以填写部分主机。支持使用 `node.name` 或者 `ip地址` routing: allocation: cluster_concurrent_rebalance: 16 # 集群内同时启动的数据任务个数，默认是2个 node_concurrent_recoveries: 16 # 添加或删除节点及负载均衡时并发恢复的线程个数，默认4个 node_initial_primaries_recoveries: 16 # 初始化数据恢复时，并发恢复线程的个数，默认4个 network: host: 0.0.0.0 tcp: keep_alive: true no_delay: true http: port: 9200 max_content_length: 200mb cors: # 允许跨域请求9200 enabled: true allow-origin: \"*\" gateway: # 当集群进行数据恢复时必须大于指定数量的节点在线，才可以进行数据恢复。通常设置为 (集群节点数量/2+1) recover_after_nodes: 2 transport: tcp: compress: true bootstrap: # 是否在程序启动时立即分配指定大小内存给elasticsearch进程 memory_lock: false ingest: # 关闭geoip 组件下载 geoip: downloader: enabled: false EOF tee /data/elk/instance1/config/elasticsearch.yml \u003c\u003c'EOF' path: data: /data/elk/instance1/data # 等同path.data: /data/elk/instance1/data，其他依次类推 logs: /data/elk/instance1/log node: name: node-2 master: true # 节点可以是master角色 data: true # 节点可以是data角色 discovery: # 启动时向哪些主机进行宣告，进行集群选举，集群内的主机都要填写上。包括自己 seed_hosts: [\"192.168.0.151\",\"192.168.0.152\",\"192.168.0.153\"] cluster: name: cluster-alerts initial_master_nodes: [\"192.168.0.151\",\"192.168.0.152\",\"192.168.0.153\"] #初始化时哪些节点可以被选举为master ，集群内的主机可以都填写上，也可以填写部分主机。支持使用 `node.name` 或者 `ip地址` routing: allocation: cluster_concurrent_rebalance: 16 # 集群内同时启动的数据任务个数，默认是2个 node_concurrent_recoveries: 16 # 添加或删除节点及负载均衡时并发恢复的线程个数，默认4个 node_initial_primaries_recoveries: 16 # 初始化数据恢复时，并发恢复线程的个数，默认4个 network: host: 0.0.0.0 tcp: keep_alive: true no_delay: true http: port: 9200 max_content_length: 200mb cors: # 允许跨域请求9200 enabled: true allow-origin: \"*\" gateway: # 当集群进行数据恢复时必须大于指定数量的节点在线，才可以进行数据恢复。通常设置为 (集群节点数量/2+1) recover_after_nodes: 2 transport: tcp: compress: true bootstrap: # 是否在程序启动时立即分配指定大小内存给elasticsearch进程 memory_lock: false ingest: # 关闭geoip 组件下载 geoip: downloader: enabled: false EOF tee /data/elk/instance1/config/elasticsearch.yml \u003c\u003c'EOF' path: data: /data/elk/instance1/data # 等同path.data: /data/elk/instance1/data，其他依次类推 logs: /data/elk/instance1/log node: name: node-3 master: true # 节点可以是master角色 data: true # 节点可以是data角色 discovery: # 启动时向哪些主机进行宣告，进行集群选举，集群内的主机都要填写上。包括自己 seed_hosts: [\"192.168.0.151\",\"192.168.0.152\",\"192.168.0.153\"] cluster: name: cluster-alerts initial_master_nodes: [\"192.168.0.151\",\"192.168.0.152\",\"192.168.0.153\"] #初始化时哪些节点可以被选举为master ，集群内的主机可以都填写上，也可以填写部分主机。支持使用 `node.name` 或者 `ip地址` routing: allocation: cluster_concurrent_rebalance: 16 # 集群内同时启动的数据任务个数，默认是2个 node_concurrent_recoveries: 16 # 添加或删除节点及负载均衡时并发恢复的线程个数，默认4个 node_initial_primaries_recoveries: 16 # 初始化数据恢复时，并发恢复线程的个数，默认4个 network: host: 0.0.0.0 tcp: keep_alive: true no_delay: true http: port: 9200 max_content_length: 200mb cors: # 允许跨域请求9200 enabled: true allow-origin: \"*\" gateway: # 当集群进行数据恢复时必须大于指定数量的节点在线，才可以进行数据恢复。通常设置为 (集群节点数量/2+1) recover_after_nodes: 2 transport: tcp: compress: true bootstrap: # 是否在程序启动时立即分配指定大小内存给elasticsearch进程 memory_lock: false ingest: # 关闭geoip 组件下载 geoip: downloader: enabled: false EOF 如果可以查看到node信息，则集群节点创建成功\ncurl 127.0.0.1:9200/_cat/nodes 集群选举失败 Nov 25 16: 12: 10 node-2 elasticsearch[8964]: [2025-11-25T16: 12: 10,131][WARN][o.e.c.c.ClusterFormationFailureHelper] [node-2] master not discovered or elected yet, an election requires at least 2 nodes with ids from [VBlfeDBuQ0aKmj6sXYYDCA, YqHqBfh_TtSeyx8ToGC7cA, lsEs2XMTTRmteL7yxEiuOg], have only discovered non-quorum [{node-2}{YqHqBfh_TtSeyx8ToGC7cA}{qOajkkUJQH6ks4hwaZ02Lw}{192.168.0.152}{192.168.0.152: 9300}{cdfhilmrstw}]; discovery will continue using [192.168.0.151: 9300,192.168.0.153: 9300] from hosts providers and [{node-2}{YqHqBfh_TtSeyx8ToGC7cA}{qOajkkUJQH6ks4hwaZ02Lw}{192.168.0.152}{192.168.0.152: 9300}{cdfhilmrstw}] from last-known cluster state; node term 4, last-accepted version 1272 in term 4 解决方法：\n检查9300集群通讯端口是否互访正常。例如`telnet 192.168.0.152 9300` 关闭selinux 和 firewalld ","categories":["ELK"],"description":"\n部署集群版本elasticsearch\r\n","excerpt":"\n部署集群版本elasticsearch\r\n","ref":"/elasticsearch/cluster.html","tags":["elasticsearch"],"title":"集群版本安装"},{"body":" 服务 建议节点数 依赖 端口 zookeeper 3 jdk 2181/2888/3888 kafka 3 jdk 9092 elasticsearch 3 jdk 9200/9300 filebeat go语言开发，没有依赖 logstash java 9600 Kibana 无 5601 graph LR\rsubgraph Prod\rPod1[node] --\u003e|日志| FB1[Filebeat]\rPod2[node] --\u003e|日志| FB2[Filebeat]\rend\rsubgraph Test\rPod3[node] --\u003e|日志| FB3[Filebeat]\rPod4[node] --\u003e|日志| FB4[Filebeat]\rend\rFB1 --\u003e|topic: prod| Kafka\rFB2 --\u003e|topic: prod| Kafka\rFB3 --\u003e|topic: test| Kafka\rFB4 --\u003e|topic: test| Kafka\rKafka --\u003e|topic: prod| Logstash\rKafka --\u003e|topic: test| Logstash\rLogstash --\u003e|index| Elasticsearch\rElasticsearch --\u003e|Index-pattern| Kibana demo: 目录结构 docker-compose.yml conf/ ├── build │ └── kafka.dockerfile ├── conf │ ├── filebeat.yml │ ├── logstash.conf │ └── server.properties └── docker-compose.yml # build/kafka.dockerfile FROM openjdk:11 LABEL mainter=\"1209233066@qq.com\" ARG version=3.3.2 RUN wget https://archive.apache.org/dist/kafka/${version}/kafka_2.13-${version}.tgz \u0026\u0026\\ tar xf kafka_2.13-${version}.tgz -C /opt \u0026\u0026\\ ln -sv /opt/kafka_2.13-${version} /opt/kafka EXPOSE 9092 CMD /opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties # docker-compose.yml version: \"3\" networks: app-tier: driver: bridge services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0 networks: - app-tier environment: - discovery.type=single-node - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" deploy: resources: limits: cpus: \"2.00\" memory: 4G kibana: image: docker.elastic.co/kibana/kibana:7.17.0 networks: - app-tier environment: - \"ELASTICSEARCH_HOSTS=http://elasticsearch:9200\" - \"I18N_LOCALE=zh-CN\" ports: - \"5601:5601\" depends_on: - elasticsearch deploy: resources: limits: cpus: \"2.00\" memory: 2G filebeat: image: docker.elastic.co/beats/filebeat:7.17.0 networks: - app-tier command: filebeat -e -c /usr/share/filebeat/filebeat.yml volumes: - \"./conf/filebeat.yml:/usr/share/filebeat/filebeat.yml\" zookeeper: image: zookeeper:3.4.14 networks: - app-tier kafka: build: context: ./build dockerfile: kafka.dockerfile networks: - app-tier command: /opt/kafka/bin/kafka-server-start.sh /etc/kafka/server.properties volumes: - \"./conf/server.properties:/etc/kafka/server.properties\" depends_on: - zookeeper logstash: image: docker.elastic.co/logstash/logstash:7.17.0 networks: - app-tier volumes: - \"./conf/logstash.conf:/etc/logstash/logstash.conf\" command: logstash -f /etc/logstash/logstash.conf deploy: resources: limits: cpus: \"1.00\" memory: 1G # conf/server.properties input { kafka { bootstrap_servers =\u003e \"kafka:9092\" topics =\u003e [\"sys_log\"] } } output { elasticsearch { hosts =\u003e [\"10.4.7.10:9200\"] index =\u003e \"sys_log\" } } # conf/server.properties broker.id=0 listeners = PLAINTEXT://kafka:9092 log.dirs=/tmp/kafka-logs offsets.topic.replication.factor=1 transaction.state.log.replication.factor=1 zookeeper.connect=zookeeper:2181 # conf/filebeat.yml filebeat.inputs: - type: log enabled: true paths: - /var/log/messages - /var/log/*.log # 用于output.kafka 动态选择topic fields: log_topic: sys_log - type: log enabled: true paths: - /var/log/nginx/*.log fields: log_topic: nginx_log fields: nginx: true output.kafka: # initial brokers for reading cluster metadata hosts: [\"kafka:9092\"] # message topic selection + partitioning topic: '%{[fields.log_topic]}' partition.round_robin: reachable_only: false required_acks: 1 compression: gzip # 超过的日志流将会被丢失 max_message_bytes: 1000000 ","categories":"","description":"","excerpt":" 服务 建议节点数 依赖 端口 zookeeper 3 jdk 2181/2888/3888 kafka 3 jdk 9092 elasticsearch 3 jdk 9200/9300 filebeat go语言开发，没有依赖 logstash java 9600 Kibana 无 5601 graph LR\rsubgraph Prod\rPod1[node] --\u003e|日志| …","ref":"/Observability/logs/","tags":"","title":"ELK"},{"body":"\n目前coredns1.12.1已集成了30+内置插件，同时也支持用户在编译时引入更多的外部插件(类似nginx)，插件在corefile中的位置不影响插件的执行顺序（plugin.cfg 文件定义顺序决定）。./coredns --plugins 查看当前coredns支持哪些插件\nhealth https://coredns.io/plugins/health/\n语法 提供一个http://0.0.0.0:8080/health 的接口检查coredns是否就绪。主要关注coredns进程本身,通常使用在livenessProbe .:53 { health localhost:8080 { lameduck 5s #当CoreDNS收到终止信号（如 Pod 被删除或重启时），会先进入lameduck状态 5 秒，在这 5 秒内/health检查仍然会返回 200 OK， /ready 不会返回 OK。 #作用：给负载均衡器（如 kube-proxy、Service、Ingress）时间把流量切走，避免请求丢失。 } } ready https://coredns.io/plugins/ready/\n语法 提供一个http://0.0.0.0:8181/ready 的接口,当所有plugins都就绪是返回200,如果某个plugin不可用时返回503。可以用于readinessProbe .:53 { health localhost:8080 { lameduck 5s } ready localhost:8181 } reload https://coredns.io/plugins/reload/\n语法 定期检查 Corefile 是否发生变化，通过读取并计算其 SHA512 校验和来实现自动重新加载\n# INTERVAL和JITTER是Go语言时间持续时间。默认的INTERVAL是30s，默认的JITTER是15s # INTERVAL 的最小值为2s， JITTER的最小值为 1s # 如果 JITTER大于INTERVAL的一半，它将被设置为INTERVAL的一半。 reload [INTERVAL] [JITTER] .:53 { health localhost:8080 { lameduck 5s } ready localhost:8181 reload } log https://coredns.io/plugins/log/\n语法 记录日志，支持对日志格式的定制 .:53 { health localhost:8080 { lameduck 5s } ready localhost:8181 reload log } errors https://coredns.io/plugins/errors/\n语法 查询处理过程中遇到的任何错误都会被打印到标准输出 .:53 { health localhost:8080 { lameduck 5s } ready localhost:8181 reload log errors } prometheus https://coredns.io/plugins/metrics/\n语法 暴露一组prometheus格式的指标 .:53 { health localhost:8080 { lameduck 5s } ready localhost:8181 reload log errors prometheus :9153 } 主要指标\n指标解释 指标 版本信息 coredns_build_info{version, revision, goversion} 开启的插件 coredns_plugin_enabled{server, zone, view, name} 99%查询响应时长 histogram_quantile(0.99,coredns_dns_request_duration_seconds_bucket) reload失败次数 coredns_reload_failed_total 最后重启时间 coredns_hosts_reload_timestamp_seconds 健康检查失败次数 coredns_health_request_failures_total 缓存命中率 coredns_cache_hits_total/coredns_dns_requests_total trace https://coredns.io/plugins/trace/\n语法 链路追踪 docker run -d -p 9411:9411 openzipkin/zipkin .:53 { health localhost:8080 { lameduck 5s } ready localhost:8181 reload log errors prometheus :9153 trace zipkin } forward https://coredns.io/plugins/forward/\n语法 转发dns查询到上游dns服务器\n# FROM 定义要转发的域，TO可以是上游dnsip或/etc/resolv.conf文件 forward FROM TO... { except IGNORED_NAMES... force_tcp prefer_udp expire DURATION max_fails INTEGER tls CERT KEY CA tls_servername NAME policy random|round_robin|sequential health_check DURATION [no_rec] [domain FQDN] max_concurrent MAX next RCODE_1 [RCODE_2] [RCODE_3...] } .:53 { health localhost:8080 { lameduck 5s } ready localhost:8181 reload log errors prometheus :9153 trace zipkin forward . 223.5.5.5 114.114.114.114 { expire 10s } } cache https://coredns.io/plugins/cache/\n语法 缓存从后端（上游、数据库等）获取到的数据，默认 3600s\ncache [TTL] [ZONES...] { success CAPACITY [TTL] [MINTTL] denial CAPACITY [TTL] [MINTTL] prefetch AMOUNT [[DURATION] [PERCENTAGE%]] serve_stale [DURATION] [REFRESH_MODE] servfail DURATION disable success|denial [ZONES...] keepttl } .:53 { health localhost:8080 { lameduck 5s } ready localhost:8181 reload log errors prometheus :9153 trace zipkin forward . 223.5.5.5 114.114.114.114 { expire 10s } cache 60 } hosts https://coredns.io/plugins/hosts/\n语法 提供自定义dns解析的能力，默认5s扫描一次文件的变动\nhosts [FILE [ZONES...]] { [INLINE] ttl SECONDS no_reverse reload DURATION fallthrough [ZONES...] } 样例1 样例2 .:53 { health localhost:8080 { lameduck 5s } ready localhost:8181 reload log errors prometheus :9153 trace zipkin forward . 223.5.5.5 114.114.114.114 { expire 10s } cache 60 hosts { # 使用本机的/etc/hosts文件 1.2.3.4 zero-dew.cn # 扩展定义其他地址解析 1.2.3.5 zero-dew.com ttl 600 reload 30s fallthrough } } .:53 { health localhost:8080 { lameduck 5s } ready localhost:8181 reload log errors prometheus :9153 trace zipkin forward . 223.5.5.5 114.114.114.114 { expire 10s } cache 60 hosts /etc/test.host { # 使用/etc/test.host 文件中定义的地址解析 1.2.3.4 zero-dew.cn # 扩展定义其他地址解析 1.2.3.5 zero-dew.com ttl 600 reload 30s fallthrough } } 监控指标\ncoredns_hosts_entries{} DNS条目数量 coredns_hosts_reload_timestamp_seconds{} 最近重载时间 kubernetes https://coredns.io/plugins/kubernetes/\n语法 动态提供svc、pod 等dns解析能力\nkubernetes [ZONES...] { endpoint URL tls CERT KEY CACERT kubeconfig KUBECONFIG [CONTEXT] namespaces NAMESPACE... labels EXPRESSION pods POD-MODE endpoint_pod_names ttl TTL noendpoints fallthrough [ZONES...] ignore empty_service } .:53 { health localhost:8080 { lameduck 5s } ready localhost:8181 reload log errors prometheus :9153 trace zipkin forward . 223.5.5.5 114.114.114.114 { expire 10s } cache 60 hosts { # 使用本机的/etc/hosts文件 1.2.3.4 zero-dew.cn # 扩展定义其他地址解析 1.2.3.5 zero-dew.com ttl 600 reload 30s fallthrough } kubernetes cluster.local in-addr.arpa ip6.arpa { kubeconfig /root/.kube/kubeconfig # coredns 运行在k8s外 pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } } dig @192.168.0.106 -p 53 a kubernetes.default.svc.cluster.local +short # 反向解析 dig @192.168.0.106 -p 53 -x 172.168.0.1 ","categories":["coredns"],"description":"\nplugin|kubernetes\r\n","excerpt":"\nplugin|kubernetes\r\n","ref":"/kubernetes/plugin.html","tags":["coredns","plugin"],"title":"基础插件"},{"body":"mongo入门bilibili\n","categories":"","description":"","excerpt":"mongo入门bilibili\n","ref":"/docs/database/mongodb/","tags":"","title":""},{"body":"\nnginx 进程关系： nginx 由master和worker进程组成，master进程负责管理work进程，worker进程负责处理具体的任务.\n配置文件结构：\ngraph LR A[nginx.conf] B[global] C[http] C1[upstream] C2[server] C21[location] C211[if] D[stream] D1[upstream] D2[server] A -.-\u003e|worker进程配置| B A -.-\u003e|http服务配置| C C -.-\u003e C1 C -.-\u003e C2 C2 -.-\u003e C21 C21 -.-\u003e C211 D -.-\u003e D1 D -.-\u003e D2 A -.- |4层反向代理|D subgraph 反向代理和http服务 C1 C2 end subgraph 7层服务配置 C end subgraph 4层反向代理 D1 D2 end subgraph 4层服务配置 D end subgraph 全局配置 B end subgraph 主配置文件 A end 配置模板 Details # 进程模式启动 daemon on; # master worker 模式运行 master_process on; # 错误日志设置 日志路径 日志记录级别（debug info notice warn error crit alert emerg） error_log logs/error.log error; user nginx; worker_processes 4; worker_cpu_affinity 0001 0010 0100 1000; #worker_processes 8; #worker_cpu_affinity 0001 0010 0100 1000 0001 0010 0100 1000; events { # 批量建立连接 multi_accept on; # 对指定客户端执行debug,需要在编译时 --with-debug,否则不生效 debug_connection 127.0.0.1; debug_connection 192.168.0.0/24; # 使用epoll i/o模型 use epoll; # 单个进程最大连接数 worker_connections 15000; } # 指定work进程打开的最大文件数，可设置为系统ulimit -HSn的结果 worker_rlimit_nofile 65535; # 4层代理 stream { log_format main '$remote_addr [$time_local] ' '$protocol $status $bytes_sent $bytes_received ' '$session_time \"$upstream_addr\" ' '\"$upstream_bytes_sent\" \"$upstream_bytes_received\" \"$upstream_connect_time\"'; upstream mysql { server 172.16.100.10:3306 weight=1 max_fails=3 fail_timeout=30s; server 172.16.100.11:3306 weight=1 max_fails=3 fail_timeout=30s; server 172.16.100.12:3306 weight=1 max_fails=3 fail_timeout=30s; } server { listen 3306; proxy_connect_timeout 2s; proxy_timeout 900s; proxy_pass mysql; access_log /dev/stdout main; } } http { include mime.types; default_type application/octet-stream; server_tokens off; # 指定字符集 charset utf-8; #sendfile参数控制文件高效传输模式，同时将tcp_nopush和tcp_nodelay设置为on防止网络阻塞 sendfile on; tcp_nopush on; #在keepalive开启才有效 tcp_nodelay on; #设置客户端保持会话的时间 keepalive_timeout 60; #设置客户端请求头读取超时时间，如果超过这个时间，客户端还没有发送任何数据，nginx将返回\"request time out(408)\"错误 client_header_timeout 15; #设置客户端请求主体读取超时时间，如果超过这个时间，客户端还没有发送任何数据，nginx将返回\"request time out(408)\"错误，默认时长60 send_timeout 60; ################################ #开启压缩 gzip on; #压缩对象的最小大小（小于1k不压缩） gzip_min_length 1k; #压缩缓冲区，申请4个单位为16k的内存空间作为缓冲区 gzip_buffers 4 16k; #压缩版本(默认1.1，前端为squid2.5时使用1.0)用于设置识别http协议版本， gzip_http_version 1.0; #压缩比例 1压缩比最小 9压缩比最大 gzip_comp_level 6; #指定压缩的类型，\"text/html\"类型总是被压缩.类型cat mime.types gzip_types text/plain application/x-javascript text/css application/xml #告诉前端缓存服务器不要解压，到客户端时才解压.发送vary:Accept_Enconding 响应头字段，从而通知接收方我做了gzip 压缩 gzip_vary on; # nginx做反向代理时，按照后端web服务器的压缩策略设置gzip 参数 gzip_proxied any; # 关闭对ie6 的压缩 gzip_disable \"msie6\"; server_names_hash_max_size 1024; server_names_hash_bucket_size 512; # 七层代理 upstream backend { # backup 当其他主机失败后使用该主机，不能用在 hash ip_hash 和random 调度算法中 # down 当前主机不可用，当使用ip_hash 中标记为down不会影响hash值 server 172.16.100.10:8080 weight=1 max_fails=3 fail_timeout=30s; server 172.16.100.11:8080 weight=1 max_fails=3 fail_timeout=30s; server 172.16.100.12:8080 weight=1 max_fails=3 fail_timeout=30s; server 172.16.100.13:8080 backup; server 172.16.100.14:8080 backup; } server { # 设置X-Real-IP头，让服务端看到客户端的真实IP地址。 proxy_set_header X-Real-IP $remote_addr; # 设置X-Forwarded-For头，添加原始请求的IP地址 proxy_set_header X-Forwarded-For $remote_addr; # 设置代理请求的Host头，使用请求的原始主机名 proxy_set_header Host $host; # 设置客户端请求的最大body大小为50MB client_max_body_size 50m; # 设置客户端请求body的缓冲区大小为256KB client_body_buffer_size 256k; location /backend { proxy_pass http://backend; } } server { listen 80; server_name localhost; location / { root html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } } 参数解释 cup亲和力绑定\n位置：全局 global 指令下 配置示例:\nworker_processes 4; worker_cpu_affinity 0001 0010 0100 1000; #worker_processes 8; #worker_cpu_affinity 0001 0010 0100 1000 0001 0010 0100 1000; 事件模型优化\n位置：全局 global 中events指令下\n推荐： 在linux中使用epoll模型； 在freebsd中使用kqueue模型； 在solaris中使用/dev/poll的模式； 在windows中使用icop模型\n配置示例:\nevents { use epoll; #单个进程最大连接数 worker_connections 2048; } 单个worker可以打开的最大文件描述符\n位置：全局 global 指令下 配置示例:\n#指定work进程打开的最大文件数，可设置为系统ulimit -HSn的结果 worker_rlimit_nofile 65535; ","categories":["nginx"],"description":"nginx.conf|nginx\n","excerpt":"nginx.conf|nginx\n","ref":"/nginx/conf.html","tags":["nginx.conf"],"title":"配置nginx"},{"body":"\nnginx 是一个高性能的HTTP和反向代理服务器，本章主要介绍nginx的http服务器配置。\n基于ip的多虚拟机 Details worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; # 服务一 server { listen 192.168.0.106:80; server_name localhost; location / { root html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } # 服务二 server { listen 192.168.0.236:80; server_name localhost; location / { root html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } } 基于端口的多虚拟机 Details worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; # 服务一 server { listen 80; server_name localhost; location / { root html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } # 服务二 server { listen 81; server_name localhost; location / { root html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } } 基于域名的多虚拟机 Details worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; # 服务一 server { listen 80; server_name docs.zero-dew.com; location / { root html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } # 服务二 server { listen 80; server_name blog.zero-dew.com; location / { root html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } } 基于用户名密码的简单认证 Details 生成用户名和密码\necho user01:`openssl passwd 456` \u003e\u003e/opt/nginx/passwd echo user02:`openssl passwd 456` \u003e\u003e/opt/nginx/passwd worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; server_name localhost; location / { root html; index index.html index.htm; #### auth_basic \"Restricted Content\"; auth_basic_user_file passwd; #### } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } } 基于SSL的HTTPS服务 Details 生成证书\nmkdir /opt/nginx/conf/ssl cd /opt/nginx/conf/ssl openssl genrsa -out server.key 2048 openssl req -new -key server.key -out server.csr -subj \"/C=CN/ST=GD/L=SZ/O=zero-dew.com/CN=docs.zero-dew.com\" openssl x509 -req -in server.csr -out server.crt -signkey server.key -days 3650 worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; server_name docs.zero-dew.com; # 所有80端口的请求都重定向到443端口 location / { return 301 https://$host$request_uri; } } server { listen 443 ssl; server_name docs.zero-dew.com; # 指定SSL证书和私钥的位置 ssl_certificate ssl/server.crt; ssl_certificate_key ssl/server.key; # SSL会话超时时间 ssl_session_timeout 10m; # 指定SSL密码套件，使用安全的加密套件 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; # 指定支持的TLS协议版本 ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # 优先使用服务器指定的密码套件 ssl_prefer_server_ciphers on; # 配置HTTPS下的访问规则 location / { root html; index index.html index.htm; } } } 配置指令解释 隐藏版本号 位置： http 指令下 配置示例:\nserver_tokens off; 开启高效文件传输模式 Nginx的sendfile指令用于控制文件传输的高效模式，同时将tcp_nopush和tcp_nodelay设置为on防止网络阻塞。\n位置：http 、server 或 location 指令下\n配置示例:\nsendfile on; tcp_nopush on; tcp_nodelay on; #在keepalive开启才有效 设置连接超时时间 位置：http 、server 指令下 配置示例:\n#设置客户端保持会话的时间 keepalive_timeout 60; #设置客户端请求头读取超时时间，如果超过这个时间，客户端还没有发送任何数据，nginx将返回\"request time out(408)\"错误 client_header_timeout 15; #设置客户端请求主体读取超时时间，如果超过这个时间，客户端还没有发送任何数据，nginx将返回\"request time out(408)\"错误，默认时长60 send_timeout 60; nginx gzip压缩功能\n位置：http 、server 指令下 配置示例:\n#开启压缩 gzip on; #压缩对象的最小大小（小于1k不压缩） gzip_min_length 1k; #压缩缓冲区，申请4个单位为16k的内存空间作为缓冲区 gzip_buffers 4 16k; #压缩版本(默认1.1，前端为squid2.5时使用1.0)用于设置识别http协议版本， gzip_http_version 1.0; #压缩比例 1压缩比最小 9压缩比最大 gzip_comp_level 6; #指定压缩的类型，\"text/html\"类型总是被压缩.类型cat mime.types gzip_types text/plain application/x-javascript text/css application/xml #告诉前端缓存服务器不要解压，到客户端时才解压.发送vary:Accept_Enconding 响应头字段，从而通知接收方我做了gzip 压缩 gzip_vary on; # nginx做反向代理时，按照后端web服务器的压缩策略设置gzip 参数 gzip_proxied any; # 关闭对ie6 的压缩 gzip_disable \"msie6\"; 浏览器缓存功能 位置：http 、server location 指令下 配置示例:\nlocation ~.*\\.(gif|jpg|png|bmp|swf)$ { expires 3650d; } location ~.*\\.(js|css)?$ { expires 30d; } location ~(roboots.txt) { log_not_found off; expires 7d; break; } 客户端请求的最大大小 位置：http 、server location 指令下 配置示例:\n#超过这个值提示 413错误 ，值0=\u003e不控制 client_max_body_size 16M; root\n位置：全局 http, server, location 指令下 配置示例:\nlocation / { root /var/www/html; } server\n位置：全局 http 指令下 配置示例:\nserver {} listen\n位置：全局 server 指令下 配置示例:\nlisten 127.0.0.1:8000; listen 127.0.0.1; listen 8000; listen *:8000; listen localhost:8000; server_name\n位置：全局 server 指令下 Nginx使用哈希表来快速查找匹配的server块，而这两个参数分别控制哈希表的总容量和每个桶的大小。max_size是哈希表的总槽位数，而bucket_size是每个槽位占用的内存大小\n位置： http 指令下\n报错提示 nginx: [warn] could not build optimal server_names_hash, you should increase either server_names_hash_max_size: 512 or server_names_hash_bucket_size: 64; ignoring server_names_hash_bucket_size 配置示例:\nserver { listen 443 ssl; # 支持多个 server_name test1.go.dev test2.go.dev *.python.dev test.dev.*; # 为了提高匹配server_name的速度，nginx将server_name hash后存储,这里设置么个hash桶占用内存的大小 server_names_hash_bucket_size 128; # 该值越大，hash冲突的可能性越小 server_names_hash_max_size 512; #必须2的幂次方 } alias\n位置：全局 location 指令下\n语法：alias file-path|directory-path;\n配置示例:\n# 例如请求 http://127.0.0.1/test/nginx.conf =\u003e /etc/nginx/conf/nginx.conf location /conf { aliase /etc/nginx/conf/; } # 例如请求 http://127.0.0.1/test/nginx.conf =\u003e /etc/nginx/conf/nginx.conf location ~ ^/test/(\\w+)\\.(\\w+)$ { aliase /etc/nginx/$2/$1.$2; } index\n位置：全局 server, location 指令下 配置示例:\nlocation / { root /var/www/html; index index.html index.php; } error_page\n位置：全局 http server location if 指令下 配置示例:\nlocation / { error_page 404 /404.html; error_page 502 503 504 /50x.html; error_page 403 http://127.0.0.1/forbidden.html; # 如果重定向后仍然与原来的相同，可以使用 如下格式修改状态码 error_page 404 = 200 /empty.html; # 如果不想修改状态码只是想让重定向到另一个location 可以这样写 error_page 404 = @fetch; } location @fetch { proxy_pass http://other; } autoindex\n模块：ngx_http_autoindex_module\n位置：全局 http, server, location 指令下\n配置示例:\nlocation / { autoindex on; } location指令详解\n所属模块：ngx_http_core_module\n位置：server、location\n语法：location [=|~|~*|^~|@] /uri/ { ... }\n匹配动作 解释 优先级 = 精确匹配 1 ^~ 匹配以xx开头,忽略正则 2 /html/ 匹配目录 3 ~ 匹配正则 区分大小写 4 ~* 匹配正则 不区分大小写 4 / 默认规则 5 @ 内部的重定向 在location 最经常使用的参数为proxy_pass，接下来参照下图解释proxy_pass中路径拼接规则 location /foo { proxy_pass http://host/bar; } 如果 location 后面没有 /，Nginx 会把请求的 URI 不变地拼接到 proxy_pass 后的地址。\nlocation /foo/ { proxy_pass http://host/bar/; } 如果 location 后面有 /，Nginx 会把匹配到的前缀 /foo/ 去掉，然后把剩下的 URI 拼接到 proxy_pass 后的地址\n示例：\nflowchart LR A(nginx) B(prometheus) C(alertmanager) D(grafana) z\u003e浏览器] --\u003e A A -..-\u003e|/prom| B A -..-\u003e|/alert| C A -..-\u003e|/grafana| D style A fill:#f9f,stroke:#333,stroke-width:2px style B fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5 5 style C fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5 5 style D fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5 5 location匹配不以/结尾 location匹配以/结尾(推荐) 修改访问url 正确配置 访问 /prom/graph 时，Nginx 会把 /prom/graph 直接拼接到 http://192.168.0.161:9090/prom 后面，变成http://192.168.0.161:9090/prom/graph location /prom { proxy_pass http://192.168.0.161:9090/prom; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } 错误配置，显示404 访问 /prom/graph 时，Nginx 会把 /prom/graph 直接拼接到 http://192.168.0.161:9090/prom/ 后面，变成http://192.168.0.161:9090/prom//graph location /prom { proxy_pass http://192.168.0.161:9090/prom/; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } 正确配置 访问 /prom/graph 时，Nginx 会把 /prom/ 去掉，剩下 graph，拼接到 http://192.168.0.161:9090/prom/ 后面，变成http://192.168.0.161:9090/prom/graph location /prom/ { proxy_pass http://192.168.0.161:9090/prom/; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } 错误配置，显示404 访问 /prom/graph 时，Nginx 会把 /prom/ 去掉，剩下 graph，拼接到 http://192.168.0.161:9090/prom 后面，变成http://192.168.0.161:9090/promgraph location /prom/ { proxy_pass http://192.168.0.161:9090/prom; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } 默认情况下，nginx会将请求转发到后端的服务器上，而不会将请求的路径进行修改。如果需要将请求的路径进行修改，可以使用 proxy_redirect 指令。\nlocation = /abc { return 301 /abc/; } location /abc/ { proxy_pass http://192.168.0.161:9090/prom/; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_redirect /prom/ /abc/; } ","categories":["nginx"],"description":"http服务器|nginx\n","excerpt":"http服务器|nginx\n","ref":"/nginx/http.html","tags":["http服务器"],"title":"http服务器"},{"body":"\n文本处理 mkdir, touch, mktemp, ln, tar, gzip, zip, unzip, grep, sed, awk, tr, sort, uniq, more, less, dd, echo, printf, tail, tailf, head, stat, wc, cat, tac, nl, tee, cut, source, read, seq, find, xargs, exec, expr, install, test, eval, base64, openssl, cfssl 系统命令 top/htop, vmstat, sar, iostat, ionice, taskset, uptime, journalctl, lsmem, lscpu, crontab, at, hostname, halt, init, chkconfig, ntpdate, chronyd, ps, pstree, pgrep, pidof, kill, killall, runlevel, free, dmidecode, date, yum, rpm, getent, info, man, lz, sz 网络管理 curl, wget, dig, nc, tcpdump, nmap, telnet, ip, ifconfig, route, ifup, ifdown, brctl 磁盘管理 df, du, fdisk, parted,mount, showmount, lsblk, blkid, fsdump date\n查看时区: date -R 获取当前时间时间戳: date +%s 时间戳格式化为时间格式: date -d @1674108177 +\"%F %H:%m%S\" 时间格式格式化为时间戳: date -d \"2023-01-19 14:01:57\" +%s yum\n列出软件包可以用的版本: yum search --showduplicates docker 查看软件包信息: yum info docker rpm\n查看rpm包是否安装: rpm -qa docker\n列出软件包相关文件: rpm -ql docker\ngetent\n# grep ^root /etc/passwd getent passwd root read\n-p 指定提示符 -t 指定超时时间 -s 隐藏输入内容 -u 读取文件描述符 -n 读取指定字符后自动回车 例如： read -n 1 -p \"将在你输入第一个字符后自动回车：\" REPLY 默认接受键盘的输入的变量名 #read -t 30 -p \"please input firstname:\" firstname #echo $firstname #read -t 30 -p \"please input firstname:\" #echo $REPLY stat\n获取文件的权限信息: stat -c %A /etc/services 获取文件的权限信息: stat -c %a /etc/services 获取文件的大小: stat -c %s /etc/services echo\necho -e 转义 \\a 发出警告声； \\b 删除前一个字符； \\c 最后不加上换行符号； \\f 换行但光标仍旧停留在原来的位置； \\n 换行且光标移至行首； \\r 光标移至行首，但不换行； \\t 插入tab； \\v 与\\f相同； \\\\ 插入\\字符； \\nnn 插入nnn（八进制）所代表的ASCII字符； printf\n优势就在于格式化处理字符串\n格式化字符 转义字符 对齐和精度 格式符 描述 %s 字符串 %f 浮点格式 %b 转义字符会被转义 %c ASCII字符，显示对应参数的第一个字符 %d, %i 十进制整数 %o 不带正负号的八进制值 %u 不带正负号的十进制值 %x 不带正负号的十六进制值，用a到f表示10至15 %X 不带正负号的十六进制值，用A到F表示10至15 %% 表示‘%’本身 转义字符 描述 \\a 警告字符，常为ASCII的BEL字符 \\b 后退 \\c 不显示输出结果中任何结尾的换行字符 \\f 换页（formfeed） \\n 换行 \\r 回车 \\t 水平制表符 \\v 垂直制表符 \\ “\\”本身 \\ddd 表示1到3位数八进制值得字符串，仅在字符格式串中有效 \\0ddd 表示1到3位数八进制值得字符串 对齐和精度控制格式符 描述 %7s 当前替换符对应的输出长度为7个字符宽，不足补齐，超出也正常显示 %-7s “-”，表示左对齐，不加时，是右对齐 %+5d 正数会自动变为+num %12.3f “.3”表示保留小数点后三位，%f默认是小数点后6位 %12.5d “.5”表示整数的长度，不足用0补齐 printf \"%s\\n\" {1..20} printf \"%10s\\n\" {1..20} printf \"%-10s\\n\" {1..20} printf \"%2.2f\\n\" {1..20} printf \"%2d\\n\" {1..20} printf \"(%d)\\n\" {1..20} sort\n按照ASCII 排序: sort /etc/hosts 排序并去除重复行: sort -u /etc/hosts 按照ASCII 逆序: sort -r /etc/hosts 按照数字排序: sort -n /etc/hosts 按照月份排序: sort -M /etc/hosts 按照指定分隔符分割后，指定列排序: sort -t \":\" -k 3 -n /etc/passwd uniq\n查看文件内容等同cat: uniq /etc/hosts 只显示重复行: uniq -d /etc/hosts 去重重复行显示: uniq -u /etc/hosts 显示重复行出现的次数: uniq -c /etc/hosts wc\n统计文本函数: wc -l /etc/hosts 统计文本字数: wc -w /etc/hosts 统计文本字节数: wc -c /etc/hosts 统计文本中最长一行包含的字节数: wc -L /etc/hosts seq\n语法\t命令格式seq [OPTION]... FIRST INCREMENT LAST\n从2开始到10结尾步长为1：seq 2 10 从2开始到10结尾步长为2: seq 2 2 10 从2开始到10结尾步长为2，等宽显示: seq -w 2 2 10 从2开始到10结尾步长为2，等宽显示.修改分隔符为空格: seq -s \" \" -w 2 2 10 cut\n语法： cut OPTION... [FILE]\n按照字符串截取: echo \"http://127.0.0.1/#/zh-cn/bash\"|cut -c 1-4,8-10 按照字符串截取,1- 表示从第一个字符到结尾: echo \"http://127.0.0.1/#/zh-cn/bash\"|cut -c 1- 按照字符串截取,-5 表示从第一个字符到结尾: echo \"http://127.0.0.1/#/zh-cn/bash\"|cut -c -5 按照分隔符截取: echo \"http://127.0.0.1/#/zh-cn/bash\"|cut -d '/' -f1,2,3 tr\ntranslate or delete characters\n删除:\techo {a..z}{1..10}|tr -d [0-9]\n替换:\techo {a..z}{1..10}|tr [a-z] [A-Z]\nfind\nfind [!]-type [!]--maxdepth levels [!]-name -o 或 -a and -mtime +-7 -ctime +-7 -atime +-7 -perm -user \u003cusername\u003e -group \u003cgroupname\u003e find / -type f -name curl -or -name wget -perm 755 find /etc -name \"host*\" -print find /etc -name \"host*\" -print0 find / -nouser -a -nogroup # 查找时排除指定文件夹./rules_file find . -path \"./rules_file\" -prune -o -print expr\n计算字符串长度: expr length \"hello world\" 模式匹配 'STRING : REGEX' 执行模式匹配。两端参数会转换为字符格式，且第二个参数被视为正则表达式(GNU基本正则)，它默认会隐含前缀\"^\"。随后将第一个参数和正则模式做匹配。 如果匹配成功，且REGEX使用了'\\('和'\\)'，则此表达式返回匹配到的，如果未使用'\\('和'\\)'，则返回匹配的字符数。 如果匹配失败，如果REGEX中使用了'\\('和'\\)'，则此表达式返回空字符串，否则返回为0。 只有第一个'\\(...\\)'会引用返回的值；其余的'\\(...\\)'只在正则表达式分组时有意义。 在正则表达式中，'\\+'，'\\?'和'\\|'分表代表匹配一个或多个，0个或1个以及两端任选其一的意思 [root@allinone ~]# expr \"423-46-7\" : \"\\([0-9]\\{,3\\}-[0-9]\\{,3\\}-[0-9]\\{,3\\}\\)\" 423-46-7 [root@allinone ~]# expr \"423-46-7\" : \"[0-9]\\{,3\\}-[0-9]\\{,3\\}-[0-9]\\{,3\\}\" 8 mktemp\n创建临时的文件：mktemp 创建临时的命名文件: mktemp -t test.XXXXXXXXXX 创建一个临时目录 mktemp -d 创建一个临时命名目录 mktemp -d test.XXXXXXXXXX\ninstall\n是mkdir 、copy 、chown、chmod等命令的复合体\n创建目录，并授予权限。所有者nginx,所属组nginx 权限7777 类型为目录: install -o nginx -g nginx -m 7777 -d /tmp/456 拷贝文件： install -m 755 /etc/hosts /tmp base64\nbase64加密: echo \"123\"|base64 base64解码 : echo $(echo 123|base64)|base64 -d\ntest\n参数 代表意义 -e 该【文件名】是否存在（常用） -f 该【文件名】是否存在且为文件（file）（常用） -d 该【文件名】是否存在且为目录（directory） -b 该【文件名】是否存在且为 block device 设备 -c 该【文件名】是否存在且为 character device 设备 -S 该【文件名】是否存在且为 socket 文件 -p 该【文件名】是否存在且为 FIFO (pipe) 文件 -L 该【文件名】是否存在且为一个链接文件 -r 检测该文件名是否存在且具有【可读】的权限 -w 检测该文件名是否存在且具有【可写】的权限 -x 检测该文件名是否存在且具有【可执行】的权限 -u 检测该文件名是否存在且具有【SUID】的属性 -g 检测该文件名是否存在且具有【SGID】的属性 -k 检测该文件名是否存在且具有【Sticky bit】的属性 -s 检测该文件名是否存在且为【非空文件】 -nt 两个文件之间的比较，如：test file1 -nt file2（newer than）判断 file1 是否比 file2 新 -ot （older than）判断 file1 是否比 file2 旧 -ef 判断 file1 与 file2 是否为同一文件，可用在判断 hard link 的判定上。主要意义在判断两个文件是否均指向同一个 inode -eq 两数值相等（equal） -ne 两数值不等（not equal） -gt n1 大于 n2（greater than） -lt n1 小于 n2（less than） -ge n1 大于等于 n2（greater than or equal） -le n1 小于等于 n2（less than or equal） -z string 判定字符串是否为 0？若 string 为空字符串，则为 true -n string 判定字符串是否非空为 0？若 string 为非空字符串，则为 true str1 = str2 判断 str1 是否等于 str2，若相等，则返回 true str1 != str2 判断 str1 是否不等于 str2，若相等，则返回 false -a [and] 两条件同时成立，例如 test -f file -a -x file，则 file 同时具有 -f 与 x 权限时，返回 true curl\ncurl 是向服务端发送和接收数据的工具。 支持的协议(DICT, FILE, FTP, FTPS, GOPHER, HTTP, HTTPS, IMAP, IMAPS, LDAP,LDAPS, POP3, POP3S, RTMP, RTSP, SCP, SFTP, SMTP, SMTPS, TELNET and TFTP）\n查看服务端响应头信息: curl -I https://www.baidu.com\n获取响应码: curl -o /dev/null -s -w \"%{http_code}\" https://www.baidu.com\n携带证书访问: curl -k --cacert /etc/kubernetes/pki/ca.crt --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key https://127.0.0.1:10250/metrics\n指定请求头信息 curl -k -H \"Authorization: Bearer $TOKEN\" https://\u003cKUBERNETES_API_SERVER\u003e/api/v1/nodes\n指定请求方法, -g, –globoff允许url中存在 {} [] 等特殊字符 : curl -g -X GET http://114.132.158.2:9100/metrics?collect[]=cpu\n通过post发送数据: curl -X POST -H 'content-type: application/json' -d '{\"password\":\"123\"}' http://172.16.100.10:8060/dingtalk/webhook1/send\n文件下载: curl -fSsL https://example.com/file.zip -o /local/path/file.zip\n-f, --fail 如果curl失败时不显示错误输出，主要用于书写脚本 -s, --silent 静默 -L, --location 如果站定已经搬迁，支持重定向 dig\n查询A 记录: dig @114.114.114.114 a baidu.com +short 查询RTP记录 dig @114.114.114.114 -x 1.2.3.4.in-addr.arpa mount\n查看所有系统挂载: mount 挂载定义在/etc/fstab中的所有文件系统: mount -a 挂载定义在/etc/fstab中的指定文件系统 mount -a -t ext4 指定挂载选项: mount -o noatime,rw /dev/sdb /mnt xargs\n举例： [root@test /]# cat 1.txt a b c d e f a b c d e f a b c d e f a b c d e f [root@test /]# cat 1.txt|xargs 对文本格式的重定向 a b c d e f a b c d e f a b c d e f a b c d e f [root@test /]# cat 1.txt|xargs -n6 每行6个重排 a b c d e f a b c d e f a b c d e f a b c d e f [root@test /]# cat 1.txt|xargs|sort \u003e2.txt 对文本格式重定向后默认排序输出到2.txt a b c d e f a b c d e f a b c d e f a b c d e f 举例： [root@test /]# cat 2.txt a b ,c d e f a b ,c d e f a b ,c d e f a b ,c d e f [root@test /]# cat 2.txt|xargs -d\",\" -n2 以，为分隔符每行两个域重排 a b c d e f a b c d e f a b c d e f a b c d e f find /tmp -name 1.log -type f -print0|xargs -0 rm -f 可用于处理文件中含有空格换行的文件 find ./ -name \"*.log\" -type f -print0|xargs -0 rm -f 用于日志的删除 find -type f -name \"*.jpg\" -print|xargs tar zcvf jpg.tar.gz 压缩所有图片文件 cat url-list.txt|xargs wget -c kill\nkill -l 查询所有信号量 kill -s HUP 3940 kill -0 3940 kill -15 crontab\ncrond是执行定时任务的后台程序，systemctl status crond命令确保服务正常运行。\n/etc/crontab 系统定时任务，默认为空\n/etc/cron.d/ 包含系统定时任务\n/var/spool/cron 包含用户创建的定时任务，文件名以用户名命名\n/var/log/cron 包含定时任务执行的日志\n示例：\nSHELL=/bin/bash PATH=/sbin:/bin:/usr/sbin:/usr/bin MAILTO=\"\" */5 * * * * echo 1 \u003e/dev/null 2\u003e\u00261 10/5 * * * * echo 1 \u003e/dev/null 2\u003e\u00261 */5 23,24 * * * echo 1 \u003e/dev/null 2\u003e\u00261 */5 20-24 * * * echo 1 \u003e/dev/null 2\u003e\u00261 at\ntar\n-v, --verbose 显示过程 -f --file -c --creat -z, --gzip gzip压缩方式 -j, --bzip2 bizp压缩方式 -x --extract 取出 -C, --directory DIR 指定解压路径 -t, --list 显示压缩文件列表 --exclude= 压缩但不包含否个文件 -X, --exclude-from FILE 用文件列表的方式排除不需要打包的文件 -Z, --compress, --uncompress 调用compress完成压缩 -N, --after-date DATE, --newer DATE 打包比指定日期新的文件，用于增量打包 -p, --same-permissions, --preserve-permissions 保持属性 extract all protection information -P, --absolute-names 保存根目录的 / don't strip leading '/'s from file names 打包时排除指定文件\ntar --exclude 'conf/input.promtail' --exclude 'conf/input.redis*' -czvf seagullagent.tar.gz conf seagullagent dumpe2fs\ndump ext2/ext3/ext4 的文件系统信息,xfs 文件格式待整理\n# 磁盘 [root@tencent-sh ~]# dumpe2fs /dev/vda1 |grep -i \"inode size\" dumpe2fs 1.42.9 (28-Dec-2013) Inode size: 256 [root@tencent-sh ~]# dumpe2fs /dev/vda1 |grep -i \"block size\" dumpe2fs 1.42.9 (28-Dec-2013) Block size: 4096 ln\n语法：ln [OPTION]src dest\nexample\nln -svf /etc/hosts /tmp/hosts nl\n语法：nl [OPTION] file\nexample\n# 显示行号 nl /etc/hosts ntpdate 在centos6 中我们经常使用的时间同步命令\ntimedatectl set-timezone Asia/Shanghai date -s \"12/12/2010 15:40:30“ hwclock -w */5 * * * * /usr/sbin/ntpdate 10.0.76.177 \u003e/dev/null 2\u003e\u00261 chronyd\ncfssl\n下载地址\n# cfssl 生成证书 wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O /usr/bin/cfssl # cfssl-json 证书格式化成文件 wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O /usr/bin/cfssl-json # cfss-certinfo 查看证书的信息，发证机构，证书有效期 wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -O /usr/bin/cfssl-certinfo cfssl-certinfo -cert file cfssl-certinfo -domain domain_name cfssl-certinfo -domain pinduoduo.com cfssl certinfo -domain jd.com cfssl-certinfo -cert apiserver.pem cfssl-certinfo -domain dashboard.od.com openssl\nSSL： Secure Socket Layer（安全套接层协议)\nTLS： Transport Layer Security（传输层安全协议）\n加密算法\nRSA算法是一个广泛使用的公钥算法。其密钥包括公钥和私钥。它能用于数字签名、身份认证以及密钥交换。RSA密钥长度一般使用1024位或者更高。 DSA (Digital Signature Algorithm)算法是一种公钥算法. 功能\n证书签发 文件加密 数字签名 生成随机数 证书签发\n#自签CA openssl genrsa -out caKey.pem 2048 openssl req -new -key caKey.pem -out ca.csr -subj \"/C=CN/ST=Gd/L=SZ/O=od.com/CN=harbor.od.com\" openssl x509 -req -in ca.csr -out ca-cert.pem -signkey caKey.pem -days 3650 #签署应用key和crt # 1)用户生成自己的私钥； # genrsa Generation of DSA Private Key # gendsa Generation of DSA Private Key openssl genrsa -out serverkey.pem 2048 # 2)构造证书申请文件 openssl req -new -key serverkey.pem -out server.csr -subj \"/C=CN/ST=Gd/L=SZ/O=od.com/CN=harbor.od.com\" # req X.509 Certificate Signing Request (CSR) Management. # -new new request. # 3)用户将证书申请文件提交给CA；CA验证签名，提取用户信息，并加上其他信息（比如颁发者等信息） openssl x509 -req \\ -in server.csr \\ -out server-cert.pem \\ -signkey serverkey.pem \\ -CA ca-cert.pem \\ -CAkey caKey.pem \\ -CAcreateserial \\ -days 3650 # x509 X.509 Certificate Data Management. # -signkey arg - self sign cert with arg # -CAcreateserial - create serial number file if it does not exist # -CA arg - set the CA certificate, must be PEM format. # -CAkey arg - set the CA key, must be PEM format # missing, it is assumed to be in the CA file. 另一种签发方式\n# 跳过ca自签秘钥,如Nginx 中ssl openssl genrsa -out server.key 2048 #-x509 output a x509 structure instead of a cert openssl req -new -x509 -key server.key -out server.crt -subj /C=CN/ST=GD/L=SZ/O=devops/CN=wangendao.xyz -days 3650 -subj\nC：Country ，单位所在国家，为两位数的国家缩写，如： CN 就是中国 ST：State/Province ，单位所在州或省 L：Locality ，单位所在城市 / 或县区 O：Organization ，网站的单位名称 OU：Organization Unit，部门名称，也常常用于显示其他证书相关信息，如证书类型，证书产品名称或身份验证类型或验证内容等 CN：Common Name ，网站的域名; EA：Email Address ，邮箱地址 # 查看证书的信息 # -noout - no certificate output # -text - print the certificate in text form openssl x509 -noout -text -in server.crt # 检查证书CN是否与域名匹配 # -checkhost host - check certificate matches \"host\" openssl x509 -noout -checkhost seafile.example.com -in cert.pem Hostname seafile.example.com does match certificate # 检查证书是否包含对应ip # -checkip ipaddr - check certificate matches \"ipaddr\" [root@hdss7-21 certs]# openssl x509 -checkip 10.4.7.55 -in kubelet.pem IP 10.4.7.55 does NOT match certificate # 检查证书是否过期 # -checkend arg - check whether the cert expires in the next arg seconds # exit 1 if so, 0 if not [root@hdss7-21 ssl]# openssl x509 -checkend 2592000 -in cert.pem Certificate will expire 文件加密\n# 生成私钥 openssl genrsa -out private.pem 2048 # 生成公钥 openssl rsa -in private.pem -pubout -out public.pem # 创建文件 echo 12345678 \u003ea.txt # 加密文件 openssl rsautl -encrypt -in a.txt -inkey public.pem -pubin -out miwen.txt # 解密文件 openssl rsautl -decrypt -in miwen.txt -inkey private.pem -out b.txt 文件摘要（文件md5等验证是否被篡改）\nopenssl dgst -sign private.pem -md5 -out check a.txt 生成随机数\nopenssl rand -base64 10 openssl rand -hex 10 https://www.cnblogs.com/technology178/p/14094375.html https://blog.csdn.net/qq_35014708/article/details/89351248 https://github.com/ssllabs/research/wiki/SSL-and-TLS-Deployment-Best-Practices https://www.cnblogs.com/littleatp/p/5878763.html\n查看主机型号：\n[root@spa00x1 ~]# dmidecode |grep 'Product Name' Product Name: PowerEdge R720 Product Name: 0X6FFV 查看主机序列号: dmidecode |grep 'Serial Number'\n查看操作系统：cat /etc/redhat-release\n查看cpu：/proc/cpuinfo 查看内存：/proc/meminfo free -m\n查看负载: /proc/loadavg load uptime\nnc 网络命令nc\ncentos中的目录\n/etc/rsyslog 日志配置文件 /var/spool/clientmqueue 邮件队列 /etc/motd 添加登录时提示信息 打印ip的方法 ifconfig eth0|awk -F \"[ :]+\" 'NR==2{print $4}' ifconfig eth0|awk -v FS=\"[ :]+\" 'NR==2{print $4}' ifconfig eth0|sed -n 's/^.*addr:\\(.*\\) Bcast.*$/\\1/gp' awk int() [root@seagullcore01-uat-s2 ~]# echo \"1 0.1 2%\"|awk '{print int($2)}' 0 计算字符串长度 计算字符串长度的三种方法 char=$(seq -s \"\\t\" 100) echo ${#char} echo ${char}|wc -m echo $(expr length $char) bash 魔法 #打印连续序列 echo {0..10} #打印偶数数列 echo {0..10..2} #打印组合数列 echo {a..z}{a..z} echo {0..10}{0..10..2} #利用序列定义数组 letter_combos=({a..z}{a..z}) echo ${letter_combos[*]} 时间同步\n配置chrony 配置时区 ln -sf /usr/share/zoneinfo/$TZ /etc/localtime \u0026\u0026 echo $TZ \u003e /etc/timezone 手动时间同步 ","categories":["linux"],"description":"command|linux\n","excerpt":"command|linux\n","ref":"/linux/command.html","tags":["command"],"title":"常用命令"},{"body":"git clone https://github.com/easzlab/kubeasz.git 环境\n主机 ip 资源 kernel master01 192.168.0.243 2c 4g 100GB centos7 3.10 master02 192.168.0.244 2c 4g 100GB centos7 3.10 master03 192.168.0.245 2c 4g 100GB centos7 3.10 时间同步\nchrony 是一个优秀的 NTP 实现，性能比ntp好，且配置管理方便；它既可作时间服务器服务端，也可作客户端。\n安装chronyd服务:\tyum install chrony\n配置时间服务器地址:\n# server \u003c时间服务器\u003e \u003c客户端向服务端发起时间同步的最小间隔，示例中4 表示 2^4=64s\u003e \u003c客户端向服务端发起时间同步的最大间隔，示例中10表示 2^10=1024s\u003e server ntp.aliyun.com minpoll 4 maxpoll 10 iburst\rserver ntp1.aliyun.com minpoll 4 maxpoll 10 iburst\rserver ntp2.aliyun.com minpoll 4 maxpoll 10 iburst\rserver ntp3.aliyun.com minpoll 4 maxpoll 10 iburst\rserver ntp4.aliyun.com minpoll 4 maxpoll 10 iburst\rserver ntp5.aliyun.com minpoll 4 maxpoll 10 iburst\rserver ntp6.aliyun.com minpoll 4 maxpoll 10 iburst\rserver 192.168.0.251 minpoll 4 maxpoll 10 iburst 作为服务端时，允许连接的客户端:\nallow 192.168.0.0/16 启动守护进程：\n客户端监听本机的323/udp 端口，服务端监听 123/udp\nsystemctl enable chronyd --now systemctl status chronyd chronyc 作为客户端命令 查看配置的时间服务器\n追踪查看时间同步的详细信息\n作为服务端，查看当前连接过来的客户端\n关闭firewalld\n关闭 selinux\n使用kubeasz\ndeploy ~]# yum install ansible -y deploy ~]# ansible --version ansible 2.9.27 # 秘钥认证 ssh-keygen -t rsa -P '' -f /root/.ssh/id_rsa # for host in 192.168.0.251 192.168.0.252 192.168.0.253;do sshpass -p pytc@2024 ssh-copy-id $host done git clone https://github.com/easzlab/kubeasz.git cd kubeasz/ git checkout 3.6.5 /etc/kubeasz/ezctl new cluster01 # 是bash 脚本 wget https://github.com/easzlab/kubeasz/releases/download/3.1.1/ezdown # 下载 kubeasz代码、二进制、离线镜像 到/etc/kubeasz bash ezdown -D # 在/etc/kubeasz/clusters/k8s-01/ 下生成集群配置文件和ansible主机模板 k8s-01 是集群名称 cd /etc/kubeasz deploy kubeasz]# ./ezctl new k8s-01 # 集群名称 k8s-01 # 可以但组件安装./ezctl setup k8s-01 01 # 也可以 ./ezctl setup k8s-01 all 全部安装 deploy kubeasz]# ./ezctl setup k8s-01 all kubelete 无法启动，通过docker info 查看得知 kubelet 启动配置 和docker 的cgroup-driver 不一致。修改kubelete 的启动文件后正常\n--cgroup-driver=cgroupfs [root@ceph-deploy kubeasz]# kubectl version Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.0\", GitCommit:\"cb303e613a121a29364f75cc67d3d580833a7479\", GitTreeState:\"clean\", BuildDate:\"2021-04-08T16:31:21Z\", GoVersion:\"go1.16.1\", Compiler:\"gc\", Platform:\"linux/amd64\"} Server Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.0\", GitCommit:\"cb303e613a121a29364f75cc67d3d580833a7479\", GitTreeState:\"clean\", BuildDate:\"2021-04-08T16:25:06Z\", GoVersion:\"go1.16.1\", Compiler:\"gc\", Platform:\"linux/amd64\"} 后面的增删该都是通过 ./ezctl 集群升级\n创建两套完整的集群。 一台一台先升级master，master 升级完。在升级node ","categories":["kubernetes"],"description":"\n构建一套高可用、可扩展的kubernetes生态环境\r\n","excerpt":"\n构建一套高可用、可扩展的kubernetes生态环境\r\n","ref":"/kubernetes/kubernetes_setup/kubeaz.html","tags":["kubeadm","kubeaz"],"title":"kubeaz环境搭建"},{"body":"\nci/cd工具 注册runner runner 是go语言编写，真正执行ci构建的组件，runner不绑定任何端口\n【runner分类】\ninstance类型，所有项目可用 group 类型，组成员可用 project类型，该项目可用 注册nstance类型 runner 安装runner\ncurl -L --output /usr/local/bin/gitlab-runner https://s3.dualstack.us-east-1.amazonaws.com/gitlab-runner-downloads/v15.5.0/binaries/gitlab-runner-linux-386 chmod +x /usr/local/bin/gitlab-runner useradd --comment 'GitLab Runner' --create-home gitlab-runner --shell /bin/bash gitlab-runner install --user=gitlab-runner --working-directory=/home/gitlab-runner gitlab-runner start [root@tidb-dev01-s2 tmp]# gitlab-runner -v Version: 15.5.0 Git revision: 0d4137b8 Git branch: 15-5-stable GO version: go1.18.7 Built: 2022-10-22T23:52:17+0000 OS/Arch: linux/386 注册runner 到gitlab\ngitlab-runner register --url http://192.168.0.247/ --registration-token ADtKhX4s9UfJ6Fnn8vs_ 非交互式注册\ngitlab-runner register \\ --non-interactive \\ --url http://192.168.0.247/ \\ --registration-token \"GR1348941aQM2R2A6-Jgm_xtbnjGN\" \\ --executor \"shell\" \\ --description \"用于构建golang项目\" \\ --tag-list \"linux,go\" \\ --run-untagged=true \\ --locked false \\ --access-level not_protected gitlab-runner register \\ --non-interactive \\ --url http://192.168.0.247/ \\ --registration-token \"GR1348941q6P-P5K4vkyCjNWnMBsm\" \\ --executor \"shell\" \\ --description \"用于构建golang、java项目\" \\ --tag-list \"linux,go,mvn\" \\ --run-untagged=true \\ --locked false \\ --access-level not_protected 注册group类型 runner 安装和注册方式与instance 类型一致，只是其中注册命令中的token 不同\n注册project类型 runner 安装和注册方式与instance 类型一致，只是其中注册命令中的token 不同\n流水线文件 名词： 流水线： 一个.gitlab-ci.yml 是一个流水线 作业： 一个job 是一个作业\ngraph LR z[.gitlab-ci.yml] z -.-\u003eA z -.-\u003eB z -.-\u003eC z -.-\u003eD z -.-\u003eE A[stages] B[variables] C[job] C1[allow_failure] C2[tags] C3[stage] C4[script] C5[when] C6[retry] D[before_script] E[after_script] C --\u003e B C --\u003e|允许失败| C1 C --\u003e C2 C --\u003e C3 C --\u003e C4 C --\u003e |按照上一个job的状态决定是否运行这里和jenkins不同|C5 C --\u003eC6 C --\u003e D C --\u003e E pipeline 变量\n本次提交跳过流水线 语法：\nstages: - build - test - deploy job1: tags: - linux stage: build script: - echo \"123\" job2: tags: - linux stage: .post allow_failure: true script: - echo \"456\" 环境变量\nstages: - build variables: k1: \"v1\" k2: \"v2\" job1: variables: k1: \"v1\" k2: value: \"v2\" description: \"k2 这个变量对应的值是v2,我只是一个描述信息\" tags: - linux stage: build script: - echo \"${k1}\" job 作业\n","categories":["devops"],"description":"cicd|gitlab\n","excerpt":"cicd|gitlab\n","ref":"/devops/gitlab-cicd.html","tags":["cicd","gitlab"],"title":"ci/cd"},{"body":" 依赖 Ruby 2.7, 3.0, 3.1, 3.2 SQLite3 (tested with SQLite 3.11) MySQL (tested with MySQL 8)\nredmine 是一个项目管理工具，使用ruby开发，支持sqlit3/mysql/postgresql 作为后端存储库。\n部署安装 操作系统： CentOS Linux release 7.9.2009 (Core) 软件版本： 5.1.3 ruby版本： 2.7.2\nruby 环境安装\ngem 是ruby的包管理工具，可以理解为python的pip\n设置gem 国内仓库地址：\n# 安装ruby环境 ruby_version='3.1.0' major_minor=${ruby_version%.*} install_ruby(){ if [ ! -f ruby-${ruby_version}.tar.gz ];then curl -q -# https://cache.ruby-lang.org/pub/ruby/${major_minor}/ruby-${ruby_version}.tar.gz -O tar xf ruby-${ruby_version}.tar.gz cd ruby-${ruby_version} ./configure --disable-install-rdoc \u0026\u0026 make -j 8 \u0026\u0026 make install ruby -v gem -v else tar xf ruby-${ruby_version}.tar.gz cd ruby-${ruby_version} ./configure --disable-install-rdoc \u0026\u0026 make -j 8 \u0026\u0026 make install ruby -v gem -v fi } install_bundler(){ # 更新镜像源 gem sources --add https://mirrors.tuna.tsinghua.edu.cn/rubygems/ gem sources --remove https://rubygems.org/ # Bundler是一个Ruby库，用于管理Ruby应用程序的依赖 gem install bundler --no-document --version '\u003c 2' # 设置bundler的国内镜像地址 bundle config set mirror.https://rubygems.org https://mirrors.tuna.tsinghua.edu.cn/rubygems/ # 查看设置是否成功 bundle config get mirror.https://rubygems.org } yum install gdbm-devel readline-devel zlib-devel openssl-devel -y install_ruby install_bundler install_redmine(){ curl -q -# https://www.redmine.org/releases/redmine-5.1.3.tar.gz -O tar xf redmine-5.1.3.tar.gz cd redmine-5.1.3/ echo -e \"production:\\n adapter: sqlite3\\n database: db/redmine.sqlite3\\n\" \u003econfig/database.yml # 安装依赖 bundle install --without development test # 生成secret bundle exec rake generate_secret_token # 生成数据库表结构 bundle exec rake db:migrate RAILS_ENV=\"production\" # useradd redmine -s /usr/sbin/nologin chown -R redmine:redmine files log tmp public/plugin_assets chmod -R 755 files log tmp public/plugin_assets ruby bin/rails server -e production } install_redmine #!/bin/bash # gem 是ruby 的包管理工具，该命令安装了一个包bundler bundle install --without development test bundle exec rake db:migrate RAILS_ENV=\"production\" bundle exec rake generate_secret_token useradd redmine chown -R redmine:redmine files log tmp public/plugin_assets chmod -R 755 files log tmp public/plugin_assets ruby bin/rails server -e production } yum install ncurses ncurses-devel libaio cpanminus -y useradd -s /sbin/nologin -M mysql wget https://downloads.mysql.com/archives/get/p/23/file/mysql-8.0.36-linux-glibc2.12-x86_64.tar.xz tar xf mysql-8.0.36-linux-glibc2.12-x86_64.tar.xz -C /opt/ ln -svf /opt/mysql-8.0.36-linux-glibc2.12-x86_64 /opt/mysql chown -R mysql.mysql /opt/mysql* [root@localhost ~]# /opt/mysql/bin/mysqld --initialize --user=mysql --basedir=/opt/mysql/ --datadir=/opt/mysql/data/ 2024-06-29T10:10:23.688320Z 0 [Warning] [MY-011070] [Server] 'Disabling symbolic links using --skip-symbolic-links (or equivalent) is the default. Consider not using this option as it' is deprecated and will be removed in a future release. 2024-06-29T10:10:23.688422Z 0 [System] [MY-013169] [Server] /opt/mysql/bin/mysqld (mysqld 8.0.36) initializing of server in progress as process 7931 2024-06-29T10:10:23.699452Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started. 2024-06-29T10:10:24.026657Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended. 2024-06-29T10:10:25.693738Z 6 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: GJk1t)H)0Cug /opt/mysql/bin/mysqld_safe --defaults-file=/opt/mysql/my.cnf --datadir=/opt/mysql/data --pid-file=/opt/mysql/mysql.pid /opt/mysql/bin/mysqld_safe --datadir=/opt/mysql/data --pid-file=/opt/mysql/mysql.pid /opt/mysql/bin/mysql -uroot -p'hj2paRhqJl' yum install mysql-devel\rbundle install --without development test create database redmine set utf8 collate utf8_general_ci;\nCreate an empty utf8 encoded database: “redmine” for example\nConfigure the database parameters in config/database.yml for the “production” environment (default database is MySQL)\nInstall the required gems by running: bundle install –without development test\nOnly the gems that are needed by the adapters you’ve specified in your database configuration file are actually installed (eg. if your config/database.yml uses the ‘mysql2’ adapter, then only the mysql2 gem will be installed). Don’t forget to re-run bundle install when you change config/database.yml for using other database adapters.\nIf you need to load some gems that are not required by Redmine core (eg. fcgi), you can create a file named Gemfile.local at the root of your redmine directory. It will be loaded automatically when running bundle install.\nGenerate a session store secret\nRedmine stores session data in cookies by default, which requires a secret to be generated. Under the application main directory run: bundle exec rake generate_secret_token\nAlternatively, you can store this secret in config/secrets.yml: http://guides.rubyonrails.org/upgrading_ruby_on_rails.html#config-secrets-yml\nCreate the database structure\nUnder the application main directory run: bundle exec rake db:migrate RAILS_ENV=“production”\nIt will create all the tables and an administrator account.\nSetting up permissions (Windows users have to skip this section)\nThe user who runs Redmine must have write permission on the following subdirectories: files, log, tmp \u0026 public/plugin_assets.\nAssuming you run Redmine with a user named “redmine”: sudo chown -R redmine:redmine files log tmp public/plugin_assets sudo chmod -R 755 files log tmp public/plugin_assets\nTest the installation by running the Puma web server\nUnder the main application directory run: ruby bin/rails server -e production\nOnce Puma has started, point your browser to http://localhost:3000/ You should now see the application welcome page.\nUse the default administrator account to log in: login: admin password: admin\nGo to “Administration” to load the default configuration data (roles, trackers, statuses, workflow) and to adjust the application settings\n== Database server configuration\nWhen using MySQL with Redmine 5.1.1 or later, it is necessary to change the transaction isolation level from the default REPEATABLE READ to READ_COMMITTED. To modify this setting, either change the database configuration file or alter the settings on your MySQL server.\nTo set the transaction isolation level in the database configuration file, add transaction_isolation variable as below:\nproduction: adapter: mysql2 database: redmine host: localhost […] variables: transaction_isolation: “READ-COMMITTED”\nMore details can be found in https://www.redmine.org/projects/redmine/wiki/MySQL_configuration.\n== SMTP server Configuration\nCopy config/configuration.yml.example to config/configuration.yml and edit this file to adjust your SMTP settings. Do not forget to restart the application after any change to this file.\nPlease do not enter your SMTP settings in environment.rb.\n== References\nhttp://www.redmine.org/wiki/redmine/RedmineInstall http://www.redmine.org/wiki/redmine/EmailConfiguration http://www.redmine.org/wiki/redmine/RedmineSettings http://www.redmine.org/wiki/redmine/RedmineRepositories http://www.redmine.org/wiki/redmine/RedmineReceivingEmails http://www.redmine.org/wiki/redmine/RedmineReminderEmails http://www.redmine.org/wiki/redmine/RedmineLDAP ","categories":["devops"],"description":"\nredmine|gitlab\r\n","excerpt":"\nredmine|gitlab\r\n","ref":"/devops/redmine.html","tags":["redmine"],"title":"redmine"},{"body":"\ndoc|plugin\n安装sonarQube[服务端] 端口 jdk 9000 17 安装jdk\nwget https://download.oracle.com/java/17/archive/jdk-17.0.10_linux-x64_bin.tar.gz tar xf jdk-17.0.10_linux-x64_bin.tar.gz -C /opt 安装sonar\n#https://www.sonarsource.com/products/sonarqube/downloads/ wget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-9.9.6.92038.zip unzip sonarqube-9.9.6.92038.zip -d /opt\rln -svf /opt/{sonarqube-9.9.6.92038,sonarqube} useradd sonar chown -R sonar.sonar /opt/sonarqube* 启动服务\nsu - sonar JAVA_HOME=/opt/jdk-17.0.10 JAVA_JRE=$JAVA_HOME/jre CLASSPATH=$JAVA_HOME/lib:$JAVA_HOME/jre/lib PATH=$JAVA_HOME/bin:$JAVA_JRE/bin:$PATH:. /opt/sonarqube/bin/linux-x86-64/sonar.sh start 登录\nhttp://127.0.0.1:9000\r登录：admin\r密码：admin 插件安装 重启服务后界面展示为中文简体 离线安装插件\ncd /opt/sonarqube/extensions/downloads/ wget https://github.com/xuhuisheng/sonar-l10n-zh/releases/download/sonar-l10n-zh-plugin-9.9/sonar-l10n-zh-plugin-9.9.jar # 重启服务后，downloads/ 下的jar 包会被加载到 plugins/ ls /opt/sonarqube/extensions/downloads/ ls /opt/sonarqube/extensions/plugins/ 安装sonarscaner[客户端] 内置了jre https://docs.sonarsource.com/sonarqube/9.9/analyzing-source-code/scanners/sonarscanner/\nwget https://binaries.sonarsource.com/Distribution/sonar-scanner-cli/sonar-scanner-cli-6.1.0.4477-linux-x64.zip unzip sonar-scanner-cli-6.1.0.4477-linux-x64.zip -d /opt ln -svf /opt/{sonar-scanner-6.1.0.4477-linux-x64,sonar-scanner} export SONAR_SCANNER_HOME=/opt/sonar-scanner export PATH=${SONAR_SCANNER_HOME}/bin:$PATH export SONAR_SCANNER_OPTS=\"-Xmx512m\" [root@192 ~]# sonar-scanner -v 21:37:07.202 INFO Scanner configuration file: /opt/sonar-scanner-6.1.0.4477-linux-x64/conf/sonar-scanner.properties 21:37:07.209 INFO Project root configuration file: NONE 21:37:07.244 INFO SonarScanner CLI 6.1.0.4477 21:37:07.248 INFO Java 17.0.11 Eclipse Adoptium (64-bit) 21:37:07.250 INFO Linux 3.10.0-957.el7.x86_64 amd64 执行代码扫描 参数 解释 sonar.host.url sonarQube的服务器地址 sonar.projectKey sonar.projectName 项目名称 sonar.projectVersion 项目版本 sonar.login sonarQube用户名 sonar.password sonarQube密码 sonar.ws.timeout 超时时间 sonar.projectDescription 项目描述 sonar.links.homepage 超级链接（gitlab） sonar.links.ci 超级链接（jenkins） sonar.sources 代码目录 sonar.sourceEncoding 字符编码 sonar.java.binaries java代码编译后的文件目录 通过命令行扫描 扫描 java 代码\nsonar-scanner \\ -Dsonar.host.url=http://192.168.0.246:9000 \\ -Dsonar.projectKey=seagull-api \\ -Dsonar.projectName=seagull-api \\ -Dsonar.projectVersion=1.1 \\ -Dsonar.login=admin \\ -Dsonar.password=pytc@2024 \\ -Dsonar.ws.timeout=30 \\ -Dsonar.projectDescription='my first project!' \\ -Dsonar.links.homepage=http://192.168.0.246:8076/devops/seagull-core \\ -Dsonar.links.ci=http://192.168.1.200:8080/job/demo-pipeline-service/ \\ -Dsonar.sources=src \\ -Dsonar.sourceEncoding=UTF-8 \\ -Dsonar,java.binaries=target/classes \\ -Dsonar.java.test.binaries=target/test-classes \\ -Dsonar.java.surefire.report=target/surefire-reports 扫描 js 代码\nsonar-scanner \\ -Dsonar.host.url=http://192.168.0.246:9000 \\ -Dsonar.projectKey=seagull-webui \\ -Dsonar.projectName=seagull-webui \\ -Dsonar.projectVersion=1.1 \\ -Dsonar.login=admin \\ -Dsonar.password=pytc@2024 \\ -Dsonar.ws.timeout=30 \\ -Dsonar.projectDescription='seagull-webui' \\ -Dsonar.links.homepage=https://e.gitee.com/bj-pytc/repos/bj-pytc/pcloud-webui/sources \\ -Dsonar.links.ci=http://175.178.65.213:18080/job/seagull-ui/job/seagull-ui/ \\ -Dsonar.sources=src \\ -Dsonar.sourceEncoding=UTF-8 扫描 golang 代码\nsonar-scanner \\ -Dsonar.host.url=http://192.168.0.246:9000 \\ -Dsonar.projectKey=seagull-core \\ -Dsonar.projectName=seagull-core \\ -Dsonar.projectVersion=1.1 \\ -Dsonar.login=admin \\ -Dsonar.password=pytc@2024 \\ -Dsonar.ws.timeout=30 \\ -Dsonar.projectDescription='seagull-core' \\ -Dsonar.links.homepage=https://e.gitee.com/bj-pytc/repos/bj-pytc/pcloud-core/sources \\ -Dsonar.links.ci=http://175.178.65.213:18080/job/seagull-core/job/seagull-core-test/ \\ -Dsonar.sourceEncoding=UTF-8 sonar-scanner \\ -Dsonar.host.url=http://192.168.0.246:9000 \\ -Dsonar.projectKey=seagull-agent \\ -Dsonar.projectName=seagull-agent \\ -Dsonar.projectVersion=1.1 \\ -Dsonar.login=admin \\ -Dsonar.password=pytc@2024 \\ -Dsonar.ws.timeout=30 \\ -Dsonar.projectDescription='seagull-agent' \\ -Dsonar.links.homepage=https://e.gitee.com/bj-pytc/repos/bj-pytc/pcloud-agent/sources \\ -Dsonar.links.ci=http://175.178.65.213:18080/job/seagull-agent/job/seagull-agent/ \\ -Dsonar.sourceEncoding=UTF-8 通过配置文件扫描 文件命名为sonar-project.properties, 并把该文件放置在git代码根目录下，此时只需要执行 sonar-scanner 命令即会加载该配置文件执行扫描\ncat \u003esonar-project.properties \u003c\u003cEOF sonar.host.url=http://192.168.0.246:9000 sonar.projectKey=seagullagent sonar.projectName=seagull-agent sonar.projectVersion=1.1 sonar.login=admin sonar.password=pytc@2024 sonar.ws.timeout=30 sonar.projectDescription='seagull-agent' sonar.links.homepage=https://e.gitee.com/bj-pytc/repos/bj-pytc/pcloud-agent/sources sonar.links.ci=http://175.178.65.213:18080/job/seagull-agent/job/seagull-agent/ sonar.sourceEncoding=UTF-8 EOF 多分支扫描插件 社区版本默认无法支持多分支扫描\nSonarqube配置 下载扫描插件到 /opt/sonarqube/extensions/plugins/：\nwget https://github.com/mc1arke/sonarqube-community-branch-plugin/releases/download/1.14.0/sonarqube-community-branch-plugin-1.14.0.jar -P /opt/sonarqube/extensions/plugins/ 配置sonarqube配置文件：/opt/sonarqube/conf/sonar.properties 添加以下内容：\nsonar.web.javaAdditionalOpts=-javaagent:./extensions/plugins/sonarqube-community-branch-plugin-1.14.0.jar=web sonar.ce.javaAdditionalOpts=-javaagent:./extensions/plugins/sonarqube-community-branch-plugin-1.14.0.jar=ce 重启soanrqube,看到如下信息表示成功 sonar-scanner配置 新增扫描配置 -Dsonar.branch.name=dev\nexport SONAR_SCANNER_HOME=/opt/sonar-scanner\rexport PATH=${SONAR_SCANNER_HOME}/bin:$PATH sonar-scanner \\ -Dsonar.host.url=http://192.168.0.246:9000 \\ -Dsonar.projectKey=seagull-agent \\ -Dsonar.projectName=seagull-agent \\ -Dsonar.projectVersion=1.1 \\ -Dsonar.login=admin \\ -Dsonar.password=pytc@2024 \\ -Dsonar.ws.timeout=30 \\ -Dsonar.projectDescription='seagull-agent' \\ -Dsonar.links.homepage=https://e.gitee.com/bj-pytc/repos/bj-pytc/pcloud-agent/sources \\ -Dsonar.links.ci=http://175.178.65.213:18080/job/seagull-agent/job/seagull-agent/ \\ -Dsonar.sourceEncoding=UTF-8 \\ -Dsonar.branch.name=dev 扫描结果可以看到具体的分支信息 依赖插件安全扫描 jeremylong/DependencyCheck: OWASP dependency-check is a software composition analysis utility that detects publicly disclosed vulnerabilities in application dependencies. (github.com)\ndependency-check/dependency-check-sonar-plugin：将 Dependency-Check 报告集成到 SonarQube 中 (github.com)\nSonarQube集成DependencyCheck - Open-Source Security Architecture (bloodzer0.github.io)\ndependency-check – Internet Access Required (jeremylong.github.io)\n附件 docker 启动 sonarqube_data: 包含数据文件，例如嵌入式 H2 数据库和 Elasticsearch 索引 sonarqube_logs: 包含有关访问、Web 进程、CE 进程和 Elasticsearch 的 SonarQube 日志 sonarqube_extensions: 将包含您安装的所有插件和 Oracle JDBC 驱动程序（如有必要） docker run --rm -p 9000:9000 sonarqube:lts-community docker run --rm \\ -p 9000:9000 \\ -e JDBC_URL=\"jdbc:postgresql://127.0.0.1:5432/sonarqube\" \\ -v sonarqube_conf:/opt/sonarqube/conf \\ -v sonarqube_logs:/opt/sonarqube/logs \\ -v sonarqube_data:/opt/sonarqube/data \\ -v sonarqube_extensions:/opt/sonarqube/extensions \\ sonarqube:8.9.2-community sonarQube api 用途 api 版本 方法 prometheus格式监控 api/monitoring/metrics \u003e=9.3 GET 查找项目 api/projects/search?projects=seagull-agent \u003e=6.3 GET 创建项目 api/projects/create?project=seagull-api\u0026name=seagull-api \u003e=4.0 POST 删除项目 api/projects/delete?project=seagull-agent \u003e=5.2 POST ","categories":["devops"],"description":"\nsonarqube|devops\r\n","excerpt":"\nsonarqube|devops\r\n","ref":"/devops/sonarqube.html","tags":["sonarqube"],"title":"sonarqube"},{"body":"\ngroovy 语法 注释 // 单行注释 /* 多行注释 */ /* * 文档注释 * 这个函数的使用方法 */ 变量 // 变量定义 def name=\"hello world!\" 字符串\nstr=\"中国\" // 字符串拼接 str=str + \"-beijing\" // 字符串分割 split() ,分割后生成数组 printf(\"我来自%s\",str.split(\"-\")[0]) // 首字符大写 capitalize() printf(\"我来自%s\",str.split(\"-\")[1].capitalize()) // 大写 toUpperCase() printf(\"我来自%s\",str.toUpperCase()) // 大写 toLowerCase() printf(\"我来自%s\",str.toLowerCase()) // 字符串长度 str.size() 数字\n数组\n// 数组 a_list=[1,2,3,4,5,6] println a_list.getClass().getName() // 数据类型 java.util.ArrayList for(i in a_list){ println i } // 数据嵌套 b_list=[1,2,3,4,5,6,[\"a\",\"b\"]] // 数组新增元素 b_list.add(\"新成员\") b_list.add(2,\"在索引为2 的位置插入\") println b_list //[1, 2, 在索引为2 的位置插入, 3, 4, 5, 6, [a, b], 新成员] // 通过语法糖 追加元素 b_list \u003c\u003c \"hello world\" println b_list //[1, 2, 在索引为2 的位置插入, 3, 4, 5, 6, [a, b], 新成员, hello world] // 除了for之外遍历元素的方法，${it} 为内置变量 b_list.each{println \"成员：${it}\"} map\n// map a_map=[name:\"张三\"] println a_map.getClass().getName() // java.util.LinkedHashMap // 调用方法和python 很类似 a_map.name a_map[\"name\"] a_map.get(\"name\") // 添加元素 a_map[\"age\"]=18 // 遍历元素 a_map.each{println\"${it},分别获取可以和value: ${it.key}--${it.value}\"} 运算符 赋值运算符： = += -= /= %=\n算数运算符： + - * / % ++ --\n关系运算符： == != \u003e \u003c \u003e= \u003c=\n逻辑运算符： \u0026\u0026 || !\n位运算符： \u0026 | ^ ~\n范围运算符：\ndef a=1..5 a.getClass().getName() //类型: groovy.lang.IntRange a.get(1) // 按照索引取值 for while if switch for\nfor(def i=0;i\u003c5;i++){ println i } for in\ndef a=\"123\" // a 是一个可迭代对象，和python类似 for(i in a){ println i } times\n// 循环0 到9 10.times{ i -\u003e println i } info = [ [\"id\": 1, \"name\": \"张三\"], [\"id\": 2, \"name\": \"李四\"] ] // 格式1： for (i in info) { println i.name } newinfo=[] for (i in 0..info.size() - 1) { newinfo \u003c\u003c info[i].name } // 格式2 for(i=0;i\u003cinfo.size();i++){ println info[i] } while\ndef conut=0 while(conut \u003c5){ println \"${conut}\" conut++ } if\ndef score=0 if (score\u003c60){ println \"再接再厉\" }else{ print \"恭喜你\" } def score=60 if (score\u003c60){ println \"再接再厉\" }else if(score==60){ println \"运气不错\" }else if(score\u003e60 \u0026\u0026 score\u003c=100){ println \"你太厉害了\" } switch\ndef type = \"linux\" switch(type) { case \"linux\": println \"操作系统为linux\" break case \"windows\": println \"操作系统为windows\" break default: println \"其他操作系统\" break } 函数 def sum(){ println \"不带参数的函数\" } sum() def sum(num1,num2){ println \"带参数的函数\\n \" println num1 + num2 } sum(1,2) def sum(num1=1,num2=2){ println \"带参数的函数,设置了默认值\\n \" println num1 + num2 } sum() def sum(num1=1,num2=2){ println \"带参数的函数,设置了默认值和返回值\\n \" return num1 + num2 } a=sum() println a 异常处理 // 异常处理 // try catch finally try { println name }catch(Exception e){ println e }finally { println \"always\" } ","categories":["devops"],"description":"groovy|jenkins\n","excerpt":"groovy|jenkins\n","ref":"/devops/groovy.html","tags":["jenkins","groovy"],"title":"Groovy"},{"body":"\n环境准备: 主机环境 系统环境 os 主机名 ip cpu 内存 磁盘 网卡 集群 centos7.6 tidb-dev01-s2 192.168.0.223 中控机 centos7.6 tidb-cmb01-s3 192.168.0.105 8 8 200G 千兆 cluster01 centos7.6 tidb-cmb02-s3 192.168.0.106 8 8 200G 千兆 cluster01 centos7.6 tidb-cmb03-s3 192.168.0.107 8 8 200G 千兆 cluster01 centos7.6 tidb-cmb04-s3 192.168.0.108 8 8 200G 千兆 cluster02 centos7.6 tidb-cmb05-s3 192.168.0.140 8 8 200G 千兆 cluster02 centos7.6 tidb-cmb06-s3 192.168.0.162 8 8 200G 千兆 cluster02 必备软件\n软件 版本 目标主机 sshpass 1.06 及以上 中控机 \u0026 集群主机 numactl 2.0.12 及以上 集群主机 tar 任意 集群主机 时间同步\n关闭swap\necho \"vm.swappiness = 0\"\u003e\u003e /etc/sysctl.conf sysctl -p swapoff -a \u0026\u0026 swapon -a 关闭防火墙\n关闭selinux\n关闭透明大页\n[root@tidb-cmb01-s3 ~]# echo never \u003e/sys/kernel/mm/transparent_hugepage/enabled [root@tidb-cmb01-s3 ~]# cat /sys/kernel/mm/transparent_hugepage/enabled # always madvise [never] 关闭数据目录所在磁盘的i/o调度器\n[root@tidb-cmb01-s3 ~]# echo noop \u003e/sys/block/sd[bc]/queue/scheduler [root@tidb-cmb01-s3 ~]# cat /sys/block/sd[bc]/queue/scheduler [noop] deadline cfq 挂载磁盘\nmkfs.ext4 /dev/mapper/data-tidb mount -o noatime -o nodelalloc -t ext4 /dev/mapper/data-tidb /data/ mkdir -p /data/cluster01 chown -R tidb:tidb /data/cluster01 安装tiup工具\ncurl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh source /root/.bash_profile 下载离线镜像\nversion=v5.2.4 #version=v6.5.12 wget https://download.pingcap.org/tidb-community-server-${version}-linux-amd64.tar.gz tar xf tidb-community-server-${version}-linux-amd64.tar.gz 设置本地镜像仓库\ntiup mirror set tidb-community-server-${version}-linux-amd64 # 验证设置是否成功 tiup mirror show 配置部署文件\n# 通过该命令获取基础配置文件 tiup cluster template \u003e /tmp/topology.yaml 检查配置文件语法和集群依赖\nDetails global: user: \"root\" ssh_port: 22 deploy_dir: \"/tidb-deploy\" data_dir: \"/tidb-data\" arch: \"amd64\" monitored: node_exporter_port: 0 blackbox_exporter_port: 0 server_configs: pd: replication.location-labels: [\"host\", \"disk\"] pd_servers: - host: 192.168.0.115 client_port: 2379 peer_port: 2380 deploy_dir: \"/tidb-deploy/pd-2379\" data_dir: \"/tidb-data/pd-2379\" log_dir: \"/tidb-deploy/pd-2379/log\" - host: 192.168.0.115 client_port: 3379 peer_port: 3380 deploy_dir: \"/tidb-deploy/pd-3379\" data_dir: \"/tidb-data/pd-3379\" log_dir: \"/tidb-deploy/pd-3379/log\" - host: 192.168.0.115 client_port: 4379 peer_port: 4380 deploy_dir: \"/tidb-deploy/pd-4379\" data_dir: \"/tidb-data/pd-4379\" log_dir: \"/tidb-deploy/pd-4379/log\" tidb_servers: - host: 192.168.0.115 port: 4000 status_port: 10080 deploy_dir: \"/tidb-deploy/tidb-4000\" log_dir: \"/tidb-deploy/tidb-4000/log\" tikv_servers: - host: 192.168.0.115 port: 20160 status_port: 20180 deploy_dir: \"/tidb-deploy/tikv-20160\" data_dir: \"/tidb-data/tikv-20160\" log_dir: \"/tidb-deploy/tikv-20160/log\" config: server.labels: host: tikv1 disk: disk1 - host: 192.168.0.115 port: 20161 status_port: 20181 deploy_dir: \"/tidb-deploy/tikv-20161\" data_dir: \"/tidb-data/tikv-20161\" log_dir: \"/tidb-deploy/tikv-20161/log\" config: server.labels: host: tikv2 disk: disk2 - host: 192.168.0.115 port: 20162 status_port: 20182 deploy_dir: \"/tidb-deploy/tikv-20162\" data_dir: \"/tidb-data/tikv-20162\" log_dir: \"/tidb-deploy/tikv-20162/log\" config: server.labels: host: tikv3 disk: disk3 monitoring_servers: - host: 192.168.0.114 port: 9090 ng_port: 12020 deploy_dir: /tidb-deploy/prometheus-9090 data_dir: /tidb-data/prometheus-9090 log_dir: /tidb-deploy/prometheus-9090/log tiup cluster check /tmp/topology.yaml --user root -p tiup cluster check /tmp/topology.yaml --apply --user root -p 部署集群\ntiup cluster deploy cluster01 v5.2.4 /tmp/topology.yaml --user root -p 启动集群\n[tidb@tidb-dev01-s2 ~]$ tiup cluster start cluster01 --init Started cluster `cluster01` successfully The root password of TiDB database has been changed. The new password is: 'jS7y!61*gz8MUZ50_%'. Copy and record it to somewhere safe, it is only displayed once, and will not be stored. The generated password can NOT be get and shown again. 查看tiup管理的集群\ntiup cluster list tiup cluster display cluster01 验证功能\nmysql -P 4000 -uroot -p'jS7y!61*gz8MUZ50_%' -h 127.0.0.1 dashboar: http://192.168.0.115:3379/dashboard\n默认用户： root\n默认密码： jS7y!61*gz8MUZ50_%\n销毁集群\ntiup cluster stop cluster01 tiup cluster destroy cluster01 数据备份 tiup install br:v6.5.12 数据迁移 数据迁移概述 | TiDB 文档中心\n数据同步 数据同步概述 | TiDB 文档中心\ntidb工具链 TiDB 工具功能概览 | TiDB 文档中心\n","categories":["tidb"],"description":"install|tidb\n","excerpt":"install|tidb\n","ref":"/tidb/install.html","tags":["install"],"title":"集群安装"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes/etcd/","tags":"","title":"etcd"},{"body":"","categories":"","description":"\njenkins 文档中心\r\n","excerpt":"\njenkins 文档中心\r\n","ref":"/docs/devops/jenkins/","tags":"","title":""},{"body":"配置文件 alertmanager配置文件默认为 alertmanger.yaml主要包含5个顶级字段\ngraph LR\rA[alertmanager.yml]\rB[\u003ca href=#global\u003eglobal\u003c/a\u003e]\rC[\u003ca href=#route\u003eroute\u003c/a\u003e]\rD[\u003ca href=#receivers\u003ereceivers\u003c/a\u003e]\rE[\u003ca href=#template\u003etemplate\u003c/a\u003e]\rF[\u003ca href=#inhibit_rules\u003einhibit_rules\u003c/a\u003e]\rA --\u003e B\rA --\u003e C\rA --\u003e D\rA --\u003e E\rA --\u003e F global\n我们可以像这样什么都不定义\nglobal: global: # The smarthost smtp_smarthost: 'smtp.qq.com:465' # from who send email smtp_from: '810654947@qq.com' smtp_auth_username: '810654947@qq.com' smtp_auth_password: 'kqaexaxpbrbdbajd' # smtp_require_tls: false route\nroute receiver: default # 通过报警规则中的label 来聚合报警 group_by: ['alertname','cluster'] # When a new group of alerts is created, wait at # least 'group_wait' to send the initial notification. group_wait: 30s # 在同一个分组中。第一个报警发出后,等待`group_interval`才会发下一个警报 group_interval: 5m # If an alert has successfully been sent, wait 'repeat_interval' to # resend them repeat_interval: 3h # match 和 match_re 在v1.0版本已经弃用 match: [ \u003clabelname\u003e: \u003clabelvalue\u003e, ... ] # 使用正则匹配标签 match_re: [ \u003clabelname\u003e: \u003cregex\u003e, ... ] # 子路由模块，拥有更高优先级 routes: - receiver: 'database-pager' group_wait: 10s matchers: - service=~\"mysql|cassandra\" - receiver: 'frontend-pager' group_by: [product, environment] matchers: - team=\"frontend\" receivers\nreceivers: - name: 'default' email_configs: - to: 'xxxxx@.com' send_resolved: true template\ninhibit_rules\n# Inhibition rules allow to mute(静音) a alert according to another alert global: ... route: ... inhibit_rules: # 同一alertname的警报，如果存在 `critical` 级别的报警，则抑制 `warning` 级别的警报 - source_matchers: - severity=\"critical\" target_matchers: - severity=\"warning\" # alertname 相等 equal: ['alertname'] # DEPRECATED: Use target_matchers below. # Matchers that have to be fulfilled in the alerts to be muted. target_match: [ \u003clabelname\u003e: \u003clabelvalue\u003e, ... ] # DEPRECATED: Use target_matchers below. target_match_re: [ \u003clabelname\u003e: \u003cregex\u003e, ... ] # A list of matchers that have to be fulfilled by the target # alerts to be muted. target_matchers: [ - \u003cmatcher\u003e ... ] # DEPRECATED: Use source_matchers below. # Matchers for which one or more alerts have to exist for the # inhibition to take effect. source_match: [ \u003clabelname\u003e: \u003clabelvalue\u003e, ... ] # DEPRECATED: Use source_matchers below. source_match_re: [ \u003clabelname\u003e: \u003cregex\u003e, ... ] # A list of matchers for which one or more alerts have # to exist for the inhibition to take effect. source_matchers: [ - \u003cmatcher\u003e ... ] # Labels that must have an equal value in the source and target # alert for the inhibition to take effect. [ equal: '[' \u003clabelname\u003e, ... ']' ] [root@master prometheus]# curl http://10.4.7.10:9093/-/healthy OK [root@master prometheus]# curl http://10.4.7.10:9093/-/ready OK [root@master prometheus]# curl -X POST http://10.4.7.10:9093/-/reload ","categories":["prometheus","alertmanager"],"description":"\nconfig|alertmanager|prometheus\r\n\r\n","excerpt":"\nconfig|alertmanager|prometheus\r\n\r\n","ref":"/prometheus/alertmanager/config.html","tags":["prometheus","alertmanager"],"title":"配置alertmanager"},{"body":"基于prometheus-operator 版本矩阵\nkube-prometheus stack Kubernetes 1.21 Kubernetes 1.22 Kubernetes 1.23 Kubernetes 1.24 Kubernetes 1.25 Kubernetes 1.26 Kubernetes 1.27 release-0.9 ✔ ✔ ✗ ✗ ✗ x x release-0.10 ✗ ✔ ✔ ✗ ✗ x x release-0.11 ✗ ✗ ✔ ✔ ✗ x x release-0.12 ✗ ✗ ✗ ✔ ✔ x x main ✗ ✗ ✗ ✗ x ✔ ✔ 下载安装\ngit clone https://github.com/prometheus-operator/kube-prometheus.git cd kube-prometheus/ git checkout -b release-0.10 # Create the namespace and CRDs, and then wait for them to be availble before creating the remaining resources kubectl create -f manifests/setup 创建monitoring 名称空间，并创建以下crd\nalertmanagerconfigs.monitoring.coreos.com\nalertmanagers.monitoring.coreos.com\npodmonitors.monitoring.coreos.com\nprobes.monitoring.coreos.com\nprometheuses.monitoring.coreos.com\nprometheusagents.monitoring.coreos.com\nprometheusrules.monitoring.coreos.com\nscrapeconfigs.monitoring.coreos.com\nservicemonitors.monitoring.coreos.com\nthanosrulers.monitoring.coreos.com\n# Wait until the \"servicemonitors\" CRD is created. The message \"No resources found\" means success in this context.until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo \"\"; done kubectl create -f manifests/ 验证\nkubectl --namespace monitoring port-forward svc/prometheus-k8s 9090 kubectl --namespace monitoring port-forward svc/alertmanager-main 9093 kubectl --namespace monitoring port-forward svc/grafana 3000 ","categories":["prometheus","监控"],"description":"\n安装|prometheus\r\n\r\n","excerpt":"\n安装|prometheus\r\n\r\n","ref":"/prometheus/prometheus-operator.html","tags":["prometheus","安装"],"title":"operator"},{"body":"doc\nVictoriaMetrics 系统选型，硬件规划，容量规划（磁盘、内存、带宽）\n文件系统： 建议选择ext4\n如果您计划在 ext4 分区上存储超过 1TB 的数据或计划将其扩展到 16TB 以上，则建议将以下选项传\n# -O 64bit：启用64位文件系统特性，这允许文件系统支持大于2TB的文件和大于1EB的文件系统。 # -O huge_file：启用大文件特性，这允许文件系统支持大于2TB的文件。 # -O extent：启用扩展分配特性，这可以提高文件系统的效率，特别是在处理大文件时 # -T huge 选项指定了文件系统的类型 mkfs.ext4 ... -O 64bit,huge_file,extent -T huge 简介 VictoriaMetrics是一个兼容prometheus的监控解决方案和分布式时序数据库。相比于prometheus、thanos具有速度快、资源占用量低、可扩展等特征。\n版本：\n分为企业版和社区版。企业版支持TLS版本，社区版建议升级到最新版本。\n版本 链接 1.110.x https://github.com/VictoriaMetrics/VictoriaMetrics/releases/tag/v1.110.7 1.102.x https://github.com/VictoriaMetrics/VictoriaMetrics/releases/tag/v1.102.20 主要功能:\n时序数据库。长期存储prometheus 指标，直接对接grafana。 统一视图。支持多个promethues 对接到victorametrics中统一查询。 简单易用 启动文件是一个没有任何依赖的go编译文件 配置完全支持命令行参数 支持兼容promQL 的的查询语言 易扩展。微服务开发，集群化部署。支持对单个/多个组件水平扩展。 支持多种方式数据抓取、采集和回填方式： 兼容promethues exporter 格式指标，exporter可以直接对接victoria 支持prometheus 远程写入 兼容prometheus 格式的数据导入doc 支持标签relabeling 优势：\n内存占用更小\nIt uses 10x less RAM than InfluxDB and up to 7x less RAM than Prometheus, Thanos or Cortex when dealing with millions of unique time series (aka high cardinality).\n更高的数据压缩比\nIt provides high data compression, so up to 70x more data points may be stored into limited storage comparing to TimescaleDB according to these benchmarks and up to 7x less storage space is required compared to Prometheus, Thanos or Cortex according to this benchmark.\n组件：\n在VictoriaMetrics 生态中包括以下成员：\nvictoria-metrics : victorias的时序数据库 vmagent：服务发现、指标采集服务 vmalert ：告警和规则评估服务 vmalert-tool: 校验告警和规则语法的工具 vmauth：是一个http代理。提供认证、路由、负载均衡的功能 vmgateway：多租户下提供租户限速服务 vmctl ：数据迁移和合并工具 vmbackup, vmrestore and vmbackupmanager ：数据备份和还原工具 快速开始 部署victoriaMetrics\n功能实现： 启动单实例victoriaMetrics, 并将node-exporter中指标存储到victoria中\n监听端口 8428\n参数解释：\n-storageDataPath数据文件默认存储在当前工作目录中./victoria-metrics-data -retentionPeriod数据保存周期默认30d -promscrape.config.strictParse 必须严格遵守promethues.yaml 规则，主要用于校验-promscrape.config 参数指定的配置文件。默认为true -promscrape.config 指定抓取exporter的配置文件 二进制方式\nwget https://github.com/VictoriaMetrics/VictoriaMetrics/releases/download/v1.79.13/victoria-metrics-linux-amd64-v1.79.13.tar.gz tar xf victoria-metrics-linux-amd64-v1.79.13.tar.gz mkdir /opt/victoria-metrics/{bin,conf,data} -p useradd victoria -s /usr/sbin/nologin cp victoria-metrics-prod /opt/victoria-metrics/bin/ chown -R victoria:victoria /opt/victoria-metrics -storageDataPath=/opt/victoria-metrics/data 数据存储目录\n-retentionPeriod=1d 保留存储的数据。默认31d, 最短1d\ntee /usr/lib/systemd/system/victoria.service \u003c\u003c'EOF' [Unit] Description=victoria service https://victoriametrics.com/ After=network.target [Service] ExecStart=/opt/victoria-metrics/bin/victoria-metrics-prod \\ -storageDataPath=/opt/victoria-metrics/data \\ -retentionPeriod=1d \\ -promscrape.config.strictParse=false \\ -promscrape.config=/etc/victoria-metrics/scrape.yml User=victoria [Install] WantedBy=multi-user.target EOF 采集node-exporter指标\ntee /etc/victoria-metrics/scrape.yml\u003c\u003cEOF scrape_configs: - job_name: pcloudcore honor_labels: true honor_timestamps: true scrape_interval: 30s scrape_timeout: 10s metrics_path: /metrics scheme: http follow_redirects: true enable_http2: true static_configs: - targets: - 127.0.0.1:9100 systemctl daemon-reload systemctl enable victoria --now 截止到这里victoriaMetrics 已经可以正常工作了，访问http://xxxx:8428验证功能\n对接prometheus prometheus版本 v2.12.0+\n[info]\npromethues远程写入会导致promethues内存使用 上浮25%左右\n对于高负载的 Prometheus 实例（每秒 200k+ 个样本），可以应用以下调整：\nqueue_config 字段，用于配置 remote_write 的发送队列:\nmax_samples_per_send: 一个 HTTP 请求中包含的最大样本数，默认值为 0，表示没有限制。如果远程写入端点要求HTTP请求能够包含有限数量的样本，则可以设置此项。当超过指定数量时，Prometheus 会自动将队列中的数据拆分成多个请求来发送。\ncapacity: 发送队列的最大缓存容量。当队列中的样本数量超过此值时，Prometheus 将会阻塞新的写入操作，直到队列中的样本数量下降到可接受的范围内。注意，队列满了时不会丢弃数据，而是会将新写入的数据缓存起来，等待队列有足够容量之后再进行发送。\nmax_shards: 配置发送队列线程池中最大的线程数，默认值为 1。当 remote_write 并发写入的数量较大时，可以将此值调大，以便更快地处理写入请求。\n需要注意的是，队列中的数据可能会占用的内存较大，因此需要根据实际情况仔细考虑合理的配置。在实际使用时，应根据数据规模、写入速度、服务器资源等因素逐步调整参数，以平衡发送队列的性能和稳定性。\nremote_write: - url: http://\u003cvictoriametrics-addr\u003e:8428/api/v1/write queue_config: max_samples_per_send: 10000 capacity: 20000 max_shards: 30 为promethues添加全局标签（多个prometheus向同一victorametrics远程写入数据时）\nglobal: external_labels: datacenter: dc-123 增加远程写入\n# promethues.yml 中开启远程写入 remote_write: - url: http://\u003cvictoriametrics-addr\u003e:8428/api/v1/write 对接grafana\n# 数据源选择prometheus，地址填写 http://\u003cvictoriametrics-addr\u003e:8428 集群架构 集群模式支持单体架构的所有功能，同时支持多租户的功能。考虑到架构复杂度，官方建议指标采集率在 100w/s 时，可以使用单体架构。\n架构图 VictoriaMetrics 集群由以下服务组成：\nvmstorage- 存储原始数据，并返回给定标签过滤器在给定时间范围内查询的数据 vminsert- 接受摄取的数据，并根据指标名称及其所有标签的一致哈希值将其分布在节点之间vmstorage vmselect- 通过从所有配置的节点获取所需的数据来执行传入查询vmstorage 安装部署 [info]\nvictoraMetrics 属于io密集型应用，官方建议使用ext4 文件系统,如果计划存储超过1TB 文件系统格式化时添加一下参数：\nmkfs.ext4 ... -O 64bit,huge_file,extent -T huge\n例如：\nlvcreate -n test -L 1g data\nmkfs.ext4 /dev/data/test -O 64bit,huge_file,extent -T huge\n主机名 ip 服务(端口) victora-dev01-s2 192.168.0.226 |vmstorage(8400 8401 8482)|vminsert(8480) |vmselect(8481) victora-dev02-s2 192.168.0.227 |vmstorage(8400 8401 8482)|vminsert(8480) |vmselect(8481) victora-dev03-s2 192.168.0.228 |vmstorage(8400 8401 8482)|vminsert(8480) |vmselect(8481) 软件下载\npvcreate /dev/sdb vgcreate victora /dev/sdb lvcreate -L 49GB victora -n data mkfs.ext4 /dev/mapper/victora-data base_dir=/data/victoriaMetrics mkdir $base_dir -p echo /dev/mapper/victora-data $base_dir ext4 defaults 0 0 \u003e\u003e/etc/fstab mount -a df -hT ${base_dir} version=v1.93.16 wget https://github.com/VictoriaMetrics/VictoriaMetrics/releases/download/${version}/victoria-metrics-linux-amd64-${version}-cluster.tar.gz mkdir ${base_dir}/{bin,conf,data,log} -p tar xf victoria-metrics-linux-amd64-${version}-cluster.tar.gz -C ${base_dir}/bin 安装vmstorage\nvmstorage 同时监听 8400 8401 8482\n8400 接受 vminsert 的写入\n8401 接受 vmselect 的查询\n8482 对接文件存储\n节点1：\n-retentionPeriod 默认保留期为 1 个月。最短保留期为 24 小时或 1 天\nipaddr=192.168.0.228 tee /etc/systemd/system/vmstorage.service\u003c\u003c'EOF' [Unit] Description=victoriaMetrics vmstorage serveice After=network.target [Service] ExecStart=${base_dir}/bin/vmstorage-prod \\ -storageDataPath ${base_dir}/data \\ -retentionPeriod 10d \\ -httpListenAddr ${ipaddr}:8482 \\ -vminsertAddr ${ipaddr}:8400 \\ -vmselectAddr ${ipaddr}:8401 User=root [Install] WantedBy=multi-user.target EOF 安装vminsert\n监听8480\n-storageNode 支持多个，用逗号分隔\n节点1:\nipaddr=192.168.0.226 storageNodeInsert=192.168.0.226:8400,192.168.0.227:8400,192.168.0.228:8400 tee /etc/systemd/system/vminsert.service \u003c\u003c'EOF' [Unit] Description=victoriaMetrics vminsert serveice After=network.target [Service] ExecStart=${base_dir}/bin/vminsert-prod \\ -storageNode=${storageNodeInsert} \\ -httpListenAddr ${ipaddr}:8480 User=root [Install] WantedBy=multi-user.target EOF 安装vmselect\n监听8481\n-storageNode 支持多个，用逗号分隔\nipaddr=192.168.0.227 storageNodeSelect=192.168.0.226:8401,192.168.0.227:8401,192.168.0.228:8401 tee /etc/systemd/system/vmselect.service\u003c\u003c'EOF' [Unit] Description=victoriaMetrics vmselect serveice After=network.target [Service] ExecStart=${base_dir}/bin/vmselect-prod \\ -storageNode=${storageNodeSelect} \\ -httpListenAddr ${ipaddr}:8481 User=root [Install] WantedBy=multi-user.target EOF 启动服务\nsystemctl daemon-reload systemctl enable vminsert --now \u0026\u0026 systemctl status vminsert systemctl enable vmselect --now \u0026\u0026 systemctl status vmselect systemctl enable vmstorage --now \u0026\u0026 systemctl status vmstorage 读写验证\n# 写入测试 curl -d 'metric_name{foo=\"bar\"} 123' -X POST http://192.168.0.226:8480/insert/1/prometheus/api/v1/import/prometheus # 查询测试 curl http://192.168.0.226:8481/select/1/prometheus/api/v1/query -d 'query=metric_name' # 删除测试 curl -v http://192.168.0.226:8481/delete/1/prometheus/api/v1/admin/tsdb/delete_series -d 'match[]=metric_name' 修改prometheus配置\n对于高负载的 Prometheus 实例（每秒 200k+ 个样本），可以应用以下调整：\nremote_write: - url: http://\u003cvictoriametrics-addr\u003e:8480/insert/0/prometheus queue_config: max_samples_per_send: 10000 capacity: 20000 max_shards: 30 remote_write: #- url: http://\u003cvictoriametrics-host\u003e:8480/insert/\u003ctenantId\u003e/\u003csuffix\u003e # prometheus and - for ingesting data with Prometheus remote write API.prometheus/api/v1/write - url: http://10.4.7.250:8480/insert/0/prometheus - url: http://10.4.7.251:8480/insert/0/prometheus 配置grafana数据源\nvictoraMetrics 与 grafana 存在兼容性问题：\nVictoriaMetrics v1.44 - v1.58.x 与 Grafana 5.0 - 7.5.x 兼容。 VictoriaMetrics v1.59.x - v1.65.x 与 Grafana 5.3 - 8.1.x 兼容（需要输入默认的 Prometheus 插件 URL）。 #http://\u003cvictoriametrics-host\u003e:8480/insert/\u003ctenantId\u003e/\u003csuffix\u003e http://10.4.7.250:8481/select/0/prometheus victoraMetrics ui【非必须】\nhttp://10.4.7.250:8481/select/0/vmui/ 组件暴露的监控指标\n10.4.7.250:8480/metrics 10.4.7.250:8481/metrics 10.4.7.250:8482/metrics vmauth vmauth 提供http的反向代理和用户鉴权认证\n8427\nversion=v1.93.16 wget https://github.com/VictoriaMetrics/VictoriaMetrics/releases/download/${version}/vmutils-linux-amd64-${version}.tar.gz tar xf vmutils-linux-amd64-${version}.tar.gz -C ${base_dir}/bin tee ${base_dir}/conf/vmauth.yml \u003c\u003cEOF unauthorized_user: url_map: - src_paths: - \"/insert/.*\" url_prefix: - \"http://192.168.0.226:8480/\" - \"http://192.168.0.227:8480/\" - \"http://192.168.0.228:8480/\" - src_paths: - \"/select/.*\" - \"/delete/.*\" url_prefix: - \"http://192.168.0.226:8481\" - \"http://192.168.0.227:8481\" - \"http://192.168.0.228:8481\" EOF ipaddr=192.168.0.226 tee /etc/systemd/system/vmauth.service \u003c\u003c'EOF' [Unit] Description=victoriaMetrics vmauth serveice After=network.target [Service] ExecStart=${base_dir}/bin/vmauth-prod -auth.config ${base_dir}/conf/vmauth.yml \\ -configCheckInterval 20s \\ -httpListenAddr \":8427\" User=root [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable vmauth --now \u0026\u0026 systemctl status vmauth # 写入测试 curl -d 'metric_name{foo=\"bar\"} 456' -X POST http://192.168.0.226:8427/insert/1/prometheus/api/v1/import/prometheus # 查询测试 curl http://192.168.0.226:8427/select/1/prometheus/api/v1/query -d 'query=metric_name' curl http://192.168.0.226:8481/select/1/prometheus/api/v1/query -d 'query=metric_name' curl http://192.168.0.227:8481/select/1/prometheus/api/v1/query -d 'query=metric_name' curl http://192.168.0.228:8481/select/1/prometheus/api/v1/query -d 'query=metric_name' # 删除测试 curl -v http://192.168.0.226:8427/delete/1/prometheus/api/v1/admin/tsdb/delete_series -d 'match[]=metric_name' 添加认证： 用户名和密码\ntee ${base_dir}/conf/vmauth.yml \u003c\u003cEOF users: - username: foo password: bar url_map: - src_paths: - \"/insert/.*\" url_prefix: - \"http://192.168.0.226:8480/\" - \"http://192.168.0.227:8480/\" - \"http://192.168.0.228:8480/\" - src_paths: - \"/select/.*\" - \"/delete/.*\" url_prefix: - \"http://192.168.0.226:8481\" - \"http://192.168.0.227:8481\" - \"http://192.168.0.228:8481\" EOF # 写入测试 curl -ufoo:bar -d 'metric_name{foo=\"bar\"} 456' -X POST http://192.168.0.226:8427/insert/1/prometheus/api/v1/import/prometheus # 查询测试 curl -ufoo:bar http://192.168.0.226:8427/select/1/prometheus/api/v1/query -d 'query=metric_name' curl http://192.168.0.226:8481/select/1/prometheus/api/v1/query -d 'query=metric_name' curl http://192.168.0.227:8481/select/1/prometheus/api/v1/query -d 'query=metric_name' curl http://192.168.0.228:8481/select/1/prometheus/api/v1/query -d 'query=metric_name' # 删除测试 curl -ufoo:bar -v http://192.168.0.226:8427/delete/1/prometheus/api/v1/admin/tsdb/delete_series -d 'match[]=metric_name' 添加认证：token认证\ntee ${base_dir}/conf/vmauth.yml \u003c\u003cEOF users: - bearer_token: ABCDEF url_map: - src_paths: - \"/insert/.*\" url_prefix: - \"http://192.168.0.226:8480/\" - \"http://192.168.0.227:8480/\" - \"http://192.168.0.228:8480/\" - src_paths: - \"/select/.*\" - \"/delete/.*\" url_prefix: - \"http://192.168.0.226:8481\" - \"http://192.168.0.227:8481\" - \"http://192.168.0.228:8481\" EOF curl -H \"Authorization: Bearer ABCDEF\" -d 'metric_name{foo=\"bar\"} 789' -X POST http://192.168.0.226:8427/insert/1/prometheus/api/v1/import/prometheus curl -H \"Authorization: Bearer ABCDEF\" http://192.168.0.226:8427/select/1/prometheus/api/v1/query -d 'query=metric_name' curl -H \"Authorization: Bearer ABCDEF\" http://192.168.0.226:8427/select/1/prometheus/api/v1/query -d 'query=metric_name' 集群环境数据高可用 默认数据是hash 后分片存储的，可以在vminsert 中修改参数，设置数据的高可用\n./vminsert-prod --help|gep repli -replicationFactor int Replication factor for the ingested data, i.e. how many copies to make among distinct -storageNode instances. Note that vmselect must run with -dedup.minScrapeInterval=1ms for data de-duplication when replicationFactor is greater than 1. Higher values for -dedup.minScrapeInterval at vmselect is OK (default 1) 监控 ","categories":["prometheus","victoriaMetrics"],"description":"victoriaMetrics|prometheus\n","excerpt":"victoriaMetrics|prometheus\n","ref":"/prometheus/victoriaMetrics/victoriaMetrics.html","tags":["prometheus","victoriaMetrics"],"title":"victoriaMetrics"},{"body":" printf 格式化输出\npackage main import \"fmt\" func main() { // %s %q字符串占位符 fmt.Printf(\"%s\\n\", \"中文\") //中文 fmt.Printf(\"%q\\n\", \"中文\") //\"中文\" // %d 十进制占位符 // %b 二进制占位符 // %q 按照assic 码输出字面值 fmt.Printf(\"%d\\n\", 91) // 91 fmt.Printf(\"%b\\n\", 91) // 1011011 fmt.Printf(\"%q\\n\", 91) // '[' // %f 浮点数占位符 fmt.Printf(\"%.2f\\n\", 3.141592657) // 3.14 // %t 布尔型占位符 fmt.Printf(\"%t\\n\", true) //true // %p 指针类型占位符 0x表示16进制 Name := \"张三\" fmt.Printf(\"%p\\n\", \u0026Name) //0xc000014270 // %v 按照值的默认格式输出 fmt.Printf(\"%v %v %v\\n\", 123, true, \"张三\") //123 true 张三 // %+v 按照值的默认格式输出,结构体会添加字段名 type Persion struct { Name string Age int } var p Persion p.Name = \"张三\" p.Age = 33 fmt.Printf(\"%v\\n%+v\\n\", p, p) // %T 数据类型 fmt.Printf(\"%T\\n\", true) //bool // %% 表示一个% fmt.Printf(\"%.2f%%\\n\", 3.141592657) //3.14% } 向文件中写入内容\nFprint,Fprintf Fprintln\npackage main import ( \"fmt\" \"os\" ) func main() { // os.OpenFile: 这是Go标准库中os包提供的一个函数，用于打开一个文件，并返回一个文件句柄。 /* os.O_CREATE|os.O_WRONLY|os.O_APPEND：这是文件打开的模式，由几个常量进行位或（|）操作得到: os.O_CREATE：如果文件不存在，创建一个新文件。 os.O_WRONLY：以只写方式打开文件。 os.O_APPEND：写入文件时，每次写操作都从文件末尾开始，即追加模式。 */ f, err := os.OpenFile(\"./1.txt\", os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0644) if err != nil { fmt.Println(err) } defer f.Close() fmt.Fprintf(f, \"%.2f\\n\", 3.141592657) } 获取输出的字符串\npackage main import \"fmt\" func main() { // Sprint Sprintf Sprintln 把输出值赋值给变量 name := fmt.Sprint(\"张三\") if name == \"张三\" { fmt.Println(name) } } 键盘输入\npackage main import \"fmt\" func main() { // Scan Scanln Scanf var userName, passWord string num, err := fmt.Scan(\u0026userName, \u0026passWord) if err != nil { fmt.Println(num, err) } // 获取变量 userName 和 passWord 的值 fmt.Printf(\"用户名:%s\\n密码:%s\\n\", userName, passWord) } package main import \"fmt\" func main() { // Scan Scanln Scanf var userName, passWord string num, err := fmt.Scanln(\u0026userName, \u0026passWord) if err != nil { fmt.Println(num, err) } // 获取变量 userName 和 passWord 的值 fmt.Printf(\"用户名:%s\\n密码:%s\\n\", userName, passWord) // root 123 // 用户名:root // 密码:123 } 自定义错误输出\npackage main import \"fmt\" func main() { file := \"/etc/hosts\" err := fmt.Errorf(\"%s文件内容为空\", file) fmt.Println(err) // /etc/hosts文件内容为空 } ","categories":["golang"],"description":"fmt|标准库\n","excerpt":"fmt|标准库\n","ref":"/golang/package/fmt.html","tags":["golang","fmt"],"title":"fmt"},{"body":"shortcode是hugo 内置的一种语法，通过在markdown 中嵌入html 标签，从而扩展markdown 的功能。\n警告标签 color=\"\" 支持 primary、info、warning\n短代码:\n{{% alert title=\"Info\" color=\"info\" %}} This is a info. {{% /alert %}} 渲染效果： Info This is a info. 短代码:\n{{% alert title=\"Warning\" color=\"warning\" %}} This is a warning. {{% /alert %}} 渲染为： Warning This is a warning. 选项卡式窗口 短代码:\n{{\u003c tabpane text=true right=false \u003e}} {{% tab header=\"**OS**:\" disabled=true /%}} {{% tab header=\"windows\" lang=\"en\" %}} 可执行文件名*.exe {{% /tab %}} {{% tab header=\"linux\" lang=\"de\" %}} 一切皆文件 {{% /tab %}} {{\u003c /tabpane \u003e}} 渲染效果： OS: windows linux 可执行文件名*.exe\r一切皆文件\r折叠 短代码:\n{{\u003c details \u003e}} 详细信息 {{\u003c /details \u003e}} 渲染效果： Details print(\"hello world\") ","categories":["hugo"],"description":"\n使用hugo \u0026 docsy 构建站点\r\n\r\n","excerpt":"\n使用hugo \u0026 docsy 构建站点\r\n\r\n","ref":"/hugo/shortcode.html","tags":["hugo","shortcode"],"title":"shortcode"},{"body":"容器引擎 docker vs containerd k8s 部署文档 etcd coredns cni flannel cillum ","categories":"","description":"\nkubernetes 文档中心\r\n\r\n","excerpt":"\nkubernetes 文档中心\r\n\r\n","ref":"/docs/kubernetes/","tags":"","title":""},{"body":"","categories":"","description":"\npython 文档中心\r\n","excerpt":"\npython 文档中心\r\n","ref":"/docs/code/python/","tags":"","title":""},{"body":"\r单引号，双引号，反引号: 区别 示例 写法 类型 转义 多字符 跨行 典型用途 'a' rune ❌ ❌ ❌ 单个字符、码点运算,类型是 rune（int32 别名）； \"abc\" string ✅ ✅ ❌ 普通字符串 raw string ❌ ✅ ✅ 正则/SQL/模板 var ch rune = '中' // 正确，字符字面量，类型是 rune（int32 别名） // var s string = \"hello\\n世界\" // 支持转义：\\n \\t \\u4e2d 等 // sql := ` SELECT * FROM user WHERE id = ? ` 常量 常量(constant)声明格式:\nconst \u003cname\u003e [type] = \u003cvalue\u003e const 是关键字固定不变，表示声明了一个常量 \u003cname\u003e 是常量的名称 [type]该常量存储的数据类型，可省略。constant 仅支持基本数据类型(bool, 数字类型，string)。 value 常量赋值 举例:\n单一常量声明\npackage main import \"fmt\" func main() { // 单一常量声明 const spring int8 = 1 const summer = 2 fmt.Printf(\"spring = %d \\nsummer = %d \\n\", spring, summer) } 批量声明常量\npackage main import \"fmt\" func main() { // 批量声明常量 const ( spring int8 = 1 summer = \"2\" ) fmt.Printf(\"spring = %d \\nsummer = %s \\n\", spring, summer) } iota为常量连续增1赋值\npackage main import \"fmt\" func main() { // iota 连续增量复制 const ( spring int8 = iota summer autumn winter ) fmt.Printf(\"spring = %d \\nsummer = %d \\nautumn = %d\\nwinter = %d\\n\", spring, summer, autumn, winter) } 变量 变量(variable)声明格式:\nvar \u003cname\u003e [type] = [expression] var 为关键字表明声明一个变量 \u003cname\u003e 变量名称 [type] 变量类型，可省略 [expression] 赋值，可省略（不能与【type】一起省略） 举例：\n单一变量声明\npackage main import \"fmt\" func main() { // 先声明后赋值 var name string name = \"张三\" // 类型推断 var age = 33 // 简短声明，只能在函数内部声明 message := `人生没有最晚，只有更晚` fmt.Printf(\"我的名字叫 %s\\n我%d岁开始学习golang\\n%s\\n\", name, age, message) } 批量声明变量\npackage main import \"fmt\" func main() { // 批量声明，数据类型相同的变量 var spring, summer, autumn, winter int spring = 1 fmt.Printf(\"spring -- %d\\nsummer -- %d\\n autumn -- %d\\nwinter -- %d\", spring, summer, autumn, winter) // 批量声明 var ( name string age int address string ) name = \"张三\" age = 33 address = \"中国深圳\" fmt.Printf(\"%s,今年%d,来自%s\\n\", name, age, address) } 全局变量和局部变量\npackage main import \"fmt\" // 全局变量必须使用 var 声明，如果想要在其他包中引用首字母必须大写 var Age = 33 func main() { Age := 32 fmt.Printf(\"%d\\n\", Age) } ","categories":["golang"],"description":"\ngo|常量和变量\r\n\r\n","excerpt":"\ngo|常量和变量\r\n\r\n","ref":"/golang/variable.html","tags":["golang","常量","变量"],"title":"常量和变量"},{"body":"Logstash事件处理流水线有三个阶段：输入→过滤器→ 输出\ninputs\nfile 从文件读取\nsyslog 充当syslog服务器\nredis\nkafka\nbeats 处理来自beats事件\nfilter\ngrok\nmutate\ndrop\nclone\ngeoip\noutput\nelasticsearch\nfile\ngraphite\nstatsd\nCodecs\njson\nmultiline\n排序 代码 仓位建议 核心逻辑 止损位 ① 603977 国泰集团 30% 稀有金属矿+民爆，2026订单刚性，PEG0.45，机构榜干净，再涨30%才触监管 破5日线 ③ 601106 中国一重 10% 核电长逻辑，换手低，适合底仓配置，短线爆发力一般 破20日线 ④ 002046 国机精工 5% 航天故事性感，但拉萨席位对倒严重，只能迷你仓博弈 破首板低点 ","categories":"","description":"","excerpt":"Logstash事件处理流水线有三个阶段：输入→过滤器→ 输出\ninputs\nfile 从文件读取\nsyslog 充当syslog服务器\nredis\nkafka\nbeats 处理来自beats事件\nfilter\ngrok\nmutate\ndrop\nclone\ngeoip\noutput\nelasticsearch\nfile\ngraphite\nstatsd\nCodecs\njson\nmultiline\n排 …","ref":"/Observability/logs/logstash/","tags":"","title":"logstash"},{"body":" kubernetes版本: 1.23.17\npython版本：3.6+\n安装：pip install kubernetes==23.6.0 文档：https://github.com/kubernetes-client/python\nfrom kubernetes import client, config,watch # Configs can be set in Configuration class directly or using helper utility config.load_kube_config(config_file=r'C:\\Users\\pc\\Desktop\\kubeconfing') w = watch.Watch() v1 = client.CoreV1Api() ret = w.stream(v1.list_namespaced_event, 'kube-system', watch=True) for item in ret: print(item['object'].message) ","categories":["python"],"description":"库|python\n","excerpt":"库|python\n","ref":"/python/package/kubernetes.html","tags":["python","kubernetes"],"title":"kubernetes"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\njdk版本：openjdk 11\nzookeeper版本: 3.4.14\nkafka版本：2.13-3.3.2 龙腾出行生产环境使用2.11-2.2.0\n2011年初，美国领英公司（LinkedIn）开源了一款基础架构软件，以奥地利作家弗兰兹·卡夫卡（Franz Kafka）的名字命名，之后 LinkedIn将其贡献给 Apache基金会，随后该软件于2012年10月完成孵化并顺利晋升为Apache顶级项目——这便是大名鼎鼎的Apache Kafka\n核心概念 broker 一个kafka进程实例可以称作一个broker,主要功能是持久化消息以及将消息队列中的消息从发送端传输到消费端 topic主题就好比数据库的表，或者文件系统里的文件夹。一个主题可以有若干个partition，是实现分布式和高可用的关键 producer生产者可能被称为发布者或写入者。 consumer消费者可能被称为订阅者或读者。 offset是一个不断递增的整数值，在创建消息时，Kafka 会把它添加到消息里。在给定的分区里，每个消息的偏移量都是唯一的。消费者把每个分区最后读取的消息偏移量保存在 Zookeeper 或 Kafka 上，如果消费者关闭或重启，它的读取状态不会丢失。 consumer group 为使消费者按照消息顺序消费和避免重复消费，群组保证每个分区只能被一个消费者使用 核心配置 kafka不支持动态调整参数，调整后需要重启broker\nbroker端参数 topic级别参数 jvm参数 os参数 文档\nbroker.id\nBroker唯一标识，不指定时broker随机生成一个\n示例：\nbroker.id = 0 log.dirs\nkafka数据存放位置，可以填写多个。多个使用,间隔\n示例:\nbroker.id = 0 log.dirs = /tmp/kafka-logs,/tmp/kafka-log2 zookeeper.connect\nZooKeeper服务器地址，多个使用,间隔。\n示例：\nbroker.id = 0 log.dirs = /tmp/kafka-logs,/tmp/kafka-log2 zookeeper.connect = 192.168.0.151:2181,192.168.0.152:2181,192.168.0.153:2181 listeners\n监听信息。格式: [协议]://[主机名]:[端口],[[协议]]://[主机名]:[端口]] 示例：\nbroker.id = 0 log.dirs = /tmp/kafka-logs,/tmp/kafka-log2 zookeeper.connect = 192.168.0.151:2181,192.168.0.152:2181,192.168.0.153:2181 # PLAINTEXT表示明文传输 listeners = PLAINTEXT://192.168.0.151:9092 advertised.listeners\n和listeners类似，该参数也是用于发布给clients的监听器，不过该参数主要用于 IaaS 环境，比如云上的机器通常都配有多块网卡（私网网卡和公网网卡）。对于这种机器，用户可以设置该参数绑定公网IP供外部clients使用，然后配置上面的listeners来绑定私网 IP供broker间通信使用\n示例：\nbroker.id = 0 log.dirs = /tmp/kafka-logs,/tmp/kafka-log2 zookeeper.connect = 192.168.0.151:2181,192.168.0.152:2181,192.168.0.153:2181 # PLAINTEXT表示明文传输 listeners = PLAINTEXT://192.168.0.151:9092 advertised.listeners = PLAINTEXT://120.24.171.66:9092 auto.create.topics.enable\n是否允许自动创建topic,默认值true\n示例:\nbroker.id = 0 log.dirs = /tmp/kafka-logs,/tmp/kafka-log2 zookeeper.connect = 192.168.0.151:2181,192.168.0.152:2181,192.168.0.153:2181 # PLAINTEXT表示明文传输 listeners = PLAINTEXT://192.168.0.151:9092 advertised.listeners = PLAINTEXT://120.24.171.66:9092 auto.create.topics.enable = false delete.topic.enable\n是否允许删除topic,默认为true。建议配合acl使用\n示例:\nbroker.id = 0 log.dirs = /tmp/kafka-logs,/tmp/kafka-log2 zookeeper.connect = 192.168.0.151:2181,192.168.0.152:2181,192.168.0.153:2181 # PLAINTEXT表示明文传输 listeners = PLAINTEXT://192.168.0.151:9092 advertised.listeners = PLAINTEXT://120.24.171.66:9092 auto.create.topics.enable = false delete.topic.enable = true log.retention.{hours|minutes|ms}\n控制了消息数据的留存时间。如果同时设置，优先选取ms的设置，minutes次之，hours最后。默认值7天\n示例：\nbroker.id = 0 log.dirs = /tmp/kafka-logs,/tmp/kafka-log2 zookeeper.connect = 192.168.0.151:2181,192.168.0.152:2181,192.168.0.153:2181 # PLAINTEXT表示明文传输 listeners = PLAINTEXT://192.168.0.151:9092 advertised.listeners = PLAINTEXT://120.24.171.66:9092 auto.create.topics.enable = false delete.topic.enable = true log.retention.hours = 168 log.retention.bytes\n空间上控制了消息数据的留存多大的数据。默认值是-1\n示例：\nbroker.id = 0 log.dirs = /tmp/kafka-logs,/tmp/kafka-log2 zookeeper.connect = 192.168.0.151:2181,192.168.0.152:2181,192.168.0.153:2181 # PLAINTEXT表示明文传输 listeners = PLAINTEXT://192.168.0.151:9092 advertised.listeners = PLAINTEXT://120.24.171.66:9092 auto.create.topics.enable = false delete.topic.enable = true log.retention.hours = 168 # 10g log.retention.bytes = 1073741824 log.flush.interval.messages\n写入的消息放置在kafka缓存中，达到指定条记录后写入到磁盘中，这个参数用于控制多少条数据后执行强制刷新\nlog.flush.interval.ms\nlog.flush.scheduler.interval.ms\nunclean.leader.election.enable\n如果我们允许不同步的副本成为首领，那么就要承担丢失数据和出现数据不一致的风险。如果不允许它们成为首领，那么就要接受较低的可用性，因为我们必须等待原先的首领恢复到可用状态。 默认为true\n示例：\nbroker.id = 0 log.dirs = /tmp/kafka-logs,/tmp/kafka-log2 zookeeper.connect = 192.168.0.151:2181,192.168.0.152:2181,192.168.0.153:2181 # PLAINTEXT表示明文传输 listeners = PLAINTEXT://192.168.0.151:9092 advertised.listeners = PLAINTEXT://120.24.171.66:9092 auto.create.topics.enable = false delete.topic.enable = true log.retention.hours = 168 # 10g log.retention.bytes = 1073741824 # unclean.leader.election.enable = false min.insync.replicas\n至少有多少个副本确认写入成功才会给生成者返回写入成功（数据可靠性）\n示例：\nbroker.id = 0 log.dirs = /tmp/kafka-logs,/tmp/kafka-log2 zookeeper.connect = 192.168.0.151:2181,192.168.0.152:2181,192.168.0.153:2181 # PLAINTEXT表示明文传输 listeners = PLAINTEXT://192.168.0.151:9092 advertised.listeners = PLAINTEXT://120.24.171.66:9092 auto.create.topics.enable = false delete.topic.enable = true log.retention.hours = 168 # 10g log.retention.bytes = 1073741824 # unclean.leader.election.enable = false min.insync.replicas = 2 num.network.threads\n默认是3\n示例：\nbroker.id = 0 log.dirs = /tmp/kafka-logs,/tmp/kafka-log2 zookeeper.connect = 192.168.0.151:2181,192.168.0.152:2181,192.168.0.153:2181 # PLAINTEXT表示明文传输 listeners = PLAINTEXT://192.168.0.151:9092 advertised.listeners = PLAINTEXT://120.24.171.66:9092 auto.create.topics.enable = false delete.topic.enable = true log.retention.hours = 168 # 10g log.retention.bytes = 1073741824 # unclean.leader.election.enable = false min.insync.replicas = 2 num.network.threads = 3 num.io.threads\n默认是8\n示例：\nbroker.id = 0 log.dirs = /tmp/kafka-logs,/tmp/kafka-log2 zookeeper.connect = 192.168.0.151:2181,192.168.0.152:2181,192.168.0.153:2181 # PLAINTEXT表示明文传输 listeners = PLAINTEXT://192.168.0.151:9092 advertised.listeners = PLAINTEXT://120.24.171.66:9092 auto.create.topics.enable = false delete.topic.enable = true log.retention.hours = 168 # 10g log.retention.bytes = 1073741824 # unclean.leader.election.enable = false min.insync.replicas = 2 num.network.threads = 3 num.io.threads = 8 num.partitions\ntopic默认分区数\nbroker.id = 0 log.dirs = /tmp/kafka-logs,/tmp/kafka-log2 zookeeper.connect = 192.168.0.151:2181,192.168.0.152:2181,192.168.0.153:2181 # PLAINTEXT表示明文传输 listeners = PLAINTEXT://192.168.0.151:9092 advertised.listeners = PLAINTEXT://120.24.171.66:9092 auto.create.topics.enable = false delete.topic.enable = true log.retention.hours = 168 # 10g log.retention.bytes = 1073741824 # unclean.leader.election.enable = false min.insync.replicas = 2 num.network.threads = 3 num.io.threads = 8 num.partitions=3 default.replication.factor\ntopic 默认副本数量\nbroker.id = 0 log.dirs = /tmp/kafka-logs,/tmp/kafka-log2 zookeeper.connect = 192.168.0.151:2181,192.168.0.152:2181,192.168.0.153:2181 # PLAINTEXT表示明文传输 listeners = PLAINTEXT://192.168.0.151:9092 advertised.listeners = PLAINTEXT://120.24.171.66:9092 auto.create.topics.enable = false delete.topic.enable = true log.retention.hours = 168 # 10g log.retention.bytes = 1073741824 # unclean.leader.election.enable = false min.insync.replicas = 2 num.network.threads = 3 num.io.threads = 8 num.partitions=3 default.replication.factor=2 message.max.bytes\n能够接受的最大消息，默认 977kb\n示例：\nbroker.id = 0 log.dirs = /tmp/kafka-logs,/tmp/kafka-log2 zookeeper.connect = 192.168.0.151:2181,192.168.0.152:2181,192.168.0.153:2181 # PLAINTEXT表示明文传输 listeners = PLAINTEXT://192.168.0.151:9092 advertised.listeners = PLAINTEXT://120.24.171.66:9092 auto.create.topics.enable = false delete.topic.enable = true log.retention.hours = 168 # 10g log.retention.bytes = 1073741824 # unclean.leader.election.enable = false min.insync.replicas = 2 num.network.threads = 3 num.io.threads = 8 num.partitions=3 default.replication.factor=2 message.max.bytes=977 num.partitions\n通过设置指定topic partitions 数量来覆盖broker启动文件中的 num.paritions配置。\n分区数量只能增加不能减少，但如果消息是按照不同的键来写入分区的，那么为已有的主题新增分区就会很困难\nlog.retention.ms\nlog.retention.bytes\n覆盖全局的 log.retention.bytes，每个 topic 设置不同的日志留存尺寸。\nlog.segment.bytes\nlog.segment.ms\nmax.message.bytes\n覆盖全局的 message.max.bytes，即为每个 topic指定不同的最大消息尺寸。\n-Xmx6g -Xms6g -XX:MetaspaceSize=96m -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M -XX:MinMetaspaceFreeRatio=50 -XX:MaxMetaspaceFreeRatio=80 文件描述符 socket缓冲区 使用ext4 或xfs 关闭swap 设置更长的flush时间 ","categories":["kafka"],"description":"\n*剖析kafka架构、核心概念和配置*\r\n\r\n","excerpt":"\n*剖析kafka架构、核心概念和配置*\r\n\r\n","ref":"/database/kafka/config.html","tags":["zookeeper","kafka"],"title":"kafka核心概念和配置文件"},{"body":"","categories":["serviceless"],"description":"\n微服务治理\r\n","excerpt":"\n微服务治理\r\n","ref":"/kubernetes/serviceless/dubbo.html","tags":["dubbo"],"title":"dubbo"},{"body":" redis-shake2.1.2 版本支持 redis 2.x to 6.x 参数\n变量名 可选值（举例） 备注 sourceType standalone sentinel cluster proxy sourceAddress 10.1.1.1:20331;10.1.1.2:20441 sourcePassword targetType targetAddress targetPassword keyExists rewrite none ignore filterKeyWhitelist “abc;bzz” filterKeyWhitelist 和 filterKeyBlacklist 不能同时设置 filterKeyBlacklist “abc;bzz” filterDbWhitelist 0;5;10 filterDbBlacklist 0;5;10 构建镜像 Details 启动脚本entrypoint.sh\n#!/bin/sh set -ex config(){ cat \u003e/etc/redis-shake.conf \u003c\u003cEOF conf.version = 1 id = redis-shake system_profile = 9310 http_profile = 9320 parallel = 32 source.type = ${sourceType:- \"cluster\"} source.address = ${sourceAddress} source.password_raw = ${sourcePassword} source.auth_type = auth target.type = ${targetType:- \"cluster\"} target.address = ${targetAddress} target.password_raw = ${targetPassword} target.auth_type = auth target.db = -1 key_exists = ${keyExists:-\"ignore\"} filter.db.whitelist = ${filterDbWhitelist} filter.db.blacklist = ${filterDbBlacklist} filter.key.whitelist = ${filterKeyWhitelist} filter.key.blacklist = ${filterKeyBlacklist} big_key_threshold = 524288000 metric = true metric.print_log = false sender.size = 104857600 sender.count = 4095 sender.delay_channel_size = 65535 keep_alive = 0 scan.key_number = 50 scan.special_cloud = scan.key_file = qps = 200000 resume_from_break_point = false replace_hash_tag = false EOF } config exec \"$@\" Details Dockerfile.yml\nFROM alpine:3.13 ARG version=2.1.2 RUN sed -i s#dl-cdn.alpinelinux.org#mirrors.tuna.tsinghua.edu.cn#g /etc/apk/repositories \u0026\u0026 \\ apk update \u0026\u0026 \\ apk add vim curl python3 \u0026\u0026 \\ alias python=\"python3\" \u0026\u0026 \\ rm -rf /var/cache/apk/* #wget https://github.com/tair-opensource/RedisShake/releases/download/release-v2.1.2-20220329/release-v2.1.2-20220329.tar.gz COPY entrypoint.sh / COPY redis-shake.linux /usr/bin/redis-shake.linux EXPOSE 9320 ENTRYPOINT [\"/entrypoint.sh\"] 构建镜像\nchmod +x entrypoint.sh docker build . -t redis-shake:2.1.2-alpine 迁移示例 docker环境测试变量解析\ntee env\u003c\u003cEOF sourceType=cluster sourceAddress=127.0.0.1:6379;127.0.0.1:6380;127.0.0.1:6381; sourcePassword=123 targetType=cluster targetAddress= 127.0.0.1:6479;127.0.0.1:6480;127.0.0.1:6481; targetPassword=456 keyExists=rewrite filterKeyWhitelist=\"abc;bzz\" filterKeyBlacklist=\"abc;bzz\" filterDbWhitelist=0;5;10 filterKeyBlacklist=0;5;10 EOF docker run --rm -it --env-file env redis-shak:2.1.2-alpine sh docker run --rm -it --env-file env redis-shak:2.1.2-alpine redis-shake.linux -type=sync -conf=/etc/redis-shake.conf 资源压力测试\n源端（版本5.0） 目标端（版本5.0） 数据量 8分片 3分片 12GB 2.1 在源端生成数据\n# 约12G数据 nodelist=( node01 node02 node03 node04 node05 node06 node07 node08 ) for i in ${nodelist[@]};do redis-cli -a 123 -h $i debug populate 12000000 $i;done 资源使用情况1 资源使用情况2 2.2.1 配置同步\ndocker run -d --net=host --env-file env nosql.registry.io:5000/redis-shak:2.1.2-alpine redis-shake.linux -type=sync -conf=/etc/redis-shake.conf 2.3.1 资源使用情况和同步耗时\n2.2.1 配置同步，限制为 3c 4g\ndocker run -d --cpus=3 -m 4G --net=host --env-file env nosql.registry.io:5000/redis-shak:2.1.2-alpine redis-shake.linux -type=sync -conf=/etc/redis-shake.conf 2.3.1 资源使用情况和同步耗时\n","categories":["redis"],"description":"\nredis-shake2.1.2 docker镜像构建\r\n","excerpt":"\nredis-shake2.1.2 docker镜像构建\r\n","ref":"/redis/redis-shake2.1.2.html","tags":["迁移案例"],"title":"redis-shake2.1.2"},{"body":"","categories":"","description":"\n可观测性工具\r\n","excerpt":"\n可观测性工具\r\n","ref":"/docs/observability/","tags":"","title":""},{"body":" 任务：\n什么是bootstraptoken,在kubernetes中的应用 kube-apiserver 的认证方式有哪些，输出最佳实践 kubeadm 配置文件结构体，支持哪些配置和字段 authentication 认证\nauthorization 授权\n执行kubeadm init 阶段将ClusterConfiguration 配置上传到kube-system名称空间下kubeadm-config configmap中，\n执行kubeadm join、kubeadm reset、 kubeadm upgrade 时读取该配置\nkubeadm config migrate 来转换旧配置文件\nkubeadm config validate 可用于验证配置文件\nkubeadm config images list\nkubeadm config images pull\nkubeadm config print\nkubeadm token create\n配置文件包含5中类型：\napiVersion: kubeadm.k8s.io/v1beta3 kind: InitConfiguration apiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration apiVersion: kubeadm.k8s.io/v1beta3 kind: JoinConfiguration kubeadm config print init-defaults kubeadm config print join-defaults kubeadm config print init-defaults \\ --component-configs KubeProxyConfiguration \\ --component-configs KubeletConfiguration kubeadm/app/apis/kubeadm/types.go\napiVersion: kubeadm.k8s.io/v1beta3 kind: InitConfiguration bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication localAPIEndpoint: advertiseAddress: 192.168.0.161 bindPort: 6443 nodeRegistration: criSocket: /var/run/dockershim.sock imagePullPolicy: IfNotPresent name: seagullcore01-uat-s2 taints: null dryRun: false --- apiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration imageRepository: k8s-gcr.m.daocloud.io kubernetesVersion: 1.23.17 apiServer: timeoutForControlPlane: 4m0s certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: {} etcd: local: dataDir: /var/lib/etcd networking: dnsDomain: cluster.local serviceSubnet: 172.168.0.0/16 scheduler: {} kubeadm/app/apis/config/types.go\n--- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration bindAddress: 0.0.0.0 # kube-proxy 监听的地址 healthzBindAddress: \"\" # /healthz 默认 0.0.0.0:10256 metricsBindAddress: \"\" # /metrics 默认 0.0.0.0:10249 hostnameOverride: \"\" # kube-proxy 实例的名称，默认 `hostname` bindAddressHardFail: false # 当为true,kube-proxy监听ip和端口失败，报致命错误并退出 enableProfiling: false # 允许 /debug/pprof clientConnection: # 连接apiserver的配置 kubeconfig: /var/lib/kube-proxy/kubeconfig.conf acceptContentTypes: \"\" burst: 0 contentType: \"\" qps: 0 mode: \"ipvs\" ipvs: scheduler: \"rr\" clusterCIDR: \"172.168.0.0/16\" configSyncPeriod: 0s oomScoreAdj: -1000 portRange: \"20000-65535\" --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration bindAddress: 0.0.0.0 # kube-proxy 监听的地址 healthzBindAddress: \"\" # /healthz 默认 0.0.0.0:10256 metricsBindAddress: \"\" # /metrics 默认 0.0.0.0:10249 hostnameOverride: \"\" # kube-proxy 实例的名称，默认 `hostname` bindAddressHardFail: false # 当为true,kube-proxy监听ip和端口失败，报致命错误并退出 enableProfiling: false # 允许 /debug/pprof clientConnection: # 连接apiserver的配置 kubeconfig: /var/lib/kube-proxy/kubeconfig.conf acceptContentTypes: \"\" burst: 0 contentType: \"\" qps: 0 mode: \"ipvs\" ipvs: scheduler: \"rr\" clusterCIDR: \"172.168.0.0/16\" configSyncPeriod: 0s oomScoreAdj: -1000 portRange: \"20000-65535\" --- apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration authentication: anonymous: enabled: false webhook: cacheTTL: 0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca.crt authorization: mode: Webhook webhook: cacheAuthorizedTTL: 0s cacheUnauthorizedTTL: 0s cgroupDriver: cgroupfs clusterDNS: - 172.168.0.2 clusterDomain: cluster.local cpuManagerReconcilePeriod: 0s evictionPressureTransitionPeriod: 0s fileCheckFrequency: 0s healthzBindAddress: 0.0.0.0 healthzPort: 10248 httpCheckFrequency: 0s imageMinimumGCAge: 0s logging: flushFrequency: 0 options: json: infoBufferSize: \"0\" verbosity: 0 memorySwap: {} nodeStatusReportFrequency: 0s nodeStatusUpdateFrequency: 0s rotateCertificates: true runtimeRequestTimeout: 0s shutdownGracePeriod: 0s shutdownGracePeriodCriticalPods: 0s staticPodPath: /etc/kubernetes/manifests streamingConnectionIdleTimeout: 0s syncFrequency: 0s volumeStatsAggPeriod: 0s kubeadm config print join-defaults apiVersion: kubeadm.k8s.io/v1beta3 kind: JoinConfiguration caCertPath: /etc/kubernetes/pki/ca.crt discovery: bootstrapToken: apiServerEndpoint: kube-apiserver:6443 token: abcdef.0123456789abcdef unsafeSkipCAVerification: true timeout: 5m0s tlsBootstrapToken: abcdef.0123456789abcdef nodeRegistration: criSocket: /var/run/dockershim.sock imagePullPolicy: IfNotPresent name: master01 taints: null Kubeadm | Kubernetes\n使用 kubeadm 引导集群 | Kubernetes\nKubeadm | Kubernetes\nkubeadm 配置（v1beta3） | Kubernetes\n控制平面与 kubelet 之间可以存在一个次要版本的偏差，但 kubelet 的版本不可以超过 API 服务器的版本。 例如，1.7.0 版本的 kubelet 可以完全兼容 1.8.0 版本的 API 服务器，反之则不可以。\n版本偏差策略 | Kubernetes\n使用 kubeadm 创建集群 | Kubernetes\n","categories":["kubernetes"],"description":"介绍kubeadm工具的使用。搭建集群/节点扩容/版本升级\n","excerpt":"介绍kubeadm工具的使用。搭建集群/节点扩容/版本升级\n","ref":"/kubernetes/kubernetes_setup/kubeadm.html","tags":["kubeadm"],"title":"kubeadm工具"},{"body":" Mater-Slave 实现 主从是一种异步同步，主库在执行完客户端提交的事务后会立即将结果返回给客户端，并不关心从库是否已经接收并处理。 主库开启binlog 主从之间设置不同的server_id 从库设置为只读，防止多点写入 搭建主备关系: 主库 从库 CREATE USER 'replUser'@'%' IDENTIFIED BY 'RsU#58d1@'; GRANT REPLICATION SLAVE,REPLICATION CLIENT ON *.* TO 'replUser'@'%'; flush privileges; --- mysql\u003e SHOW MASTER STATUS; +----------------+----------+--------------+------------------+------------------------------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +----------------+----------+--------------+------------------+------------------------------------------+ | log_bin.000002 | 551 | | | b2a464ed-6c16-11f0-8445-000c299bad95:1-2 | +----------------+----------+--------------+------------------+------------------------------------------+ 同步指定库/表\n在主库的配置文件中执行 [mysqld] binlog-do-db=mysql binlog-do-db=radius binlog-ignore-db=syslog binlog-ignore-db=sys 5.7 之前没有开启gtid的数据库 reset slave all; --- CHANGE MASTER TO MASTER_HOST='10.128.99.157', MASTER_PORT=6301, MASTER_USER='rdsRpl', MASTER_PASSWORD='RsU#58d1@', master_log_file='log_bin.000002', MASTER_LOG_POS=551; start slave;show slave status \\G -- 设置只读 set global read_only=on; set global super_read_only=on; 5.7 及之后版本开始gtid的数据库 reset slave all; --- 重置gtid执行历史 reset slave all; reset master; SET @@GLOBAL.GTID_PURGED='b2a464ed-6c16-11f0-8445-000c299bad95:1-2'; --- CHANGE MASTER TO MASTER_HOST='10.128.99.157', MASTER_PORT=6301, MASTER_USER='rdsRpl', MASTER_PASSWORD='RsU#58d1@', MASTER_AUTO_POSITION=1; start slave;show slave status \\G /* 关键指标： Slave_IO_Running: Yes Slave_SQL_Running: Yes Seconds_Behind_Master: 0 */ -- 设置只读 set global read_only=on; set global super_read_only=on; 同步指定库/表–在配置文件中指定\n或在从库的配置文件中指定 [mysqld] replicate-do-db=mysql replicate-do-db=radius replicate-ignore-db=syslog replicate-ignore-db=sys # 通配符表级 replication-wild-do-table = db1.user_% replication-wild-ignore-table = db1.log_% 同步指定库/表–动态修改\n-- 停止 SQL 线程 STOP SLAVE SQL_THREAD; -- 修改过滤规则 CHANGE REPLICATION FILTER REPLICATE_DO_DB = (db1, db2), REPLICATE_IGNORE_TABLE = (db3.log); -- 重新启动 SQL 线程 START SLAVE SQL_THREAD; 主从报错处理 从库连接异常 Slave_IO_Running: Connecting Last_IO_Errno: 1045 Last_IO_Error: error connecting to master 'rep@168.101.1.177:3306' - retry-time: 60 retries: 1 处理办法\nupdate mysql.user set authentication_string=password('rep') where user='rep'; flush privileges; 从库sql线程错误 #跳过主从复制的sql线程错误 mysql\u003e set sql_slve_skip_counter = 1 ","categories":["mysql"],"description":"mysql主从同步的原理和实现\n","excerpt":"mysql主从同步的原理和实现\n","ref":"/mysql/master_slave.html","tags":["主从配置"],"title":"master_slave"},{"body":"组件：\nnavidrome 音乐服务端程序 music-tag-web 负责刮削音乐原数据信息 filebrowser 负责通过网页端上传音乐文件 部署 mkdir -p data/filebrowser touch data/filebrowser/database.db services: navidrome: #image: deluan/navidrome:latest image: harbor.pytc.com/music/navidrome:0.54.3 container_name: navidrome privileged: true ports: - \"8001:4533\" volumes: - ./data/music:/music # 音乐 - ./data/navidrome:/data # 数据库和cache command: - /navidrome - --datafolder=/data - --musicfolder=/music restart: unless-stopped music-tag: #image: xhongc/music_tag_web:latest image: harbor.pytc.com/music/xhongc/music_tag_web:2.3.2 container_name: music-tag-web ports: - \"8002:8002\" volumes: - ./data/music:/app/media:rw - ./data/music-tag/config:/app/data restart: unless-stopped filebrowser: image: harbor.pytc.com/music/filebrowser/filebrowser:v2.31.2 container_name: filebrowser ports: - \"8003:80\" volumes: - ./data/filebrowser/database.db:/database.db - ./data/music:/srv 使用 ","categories":["娱乐"],"description":"自建音乐播放器\n","excerpt":"自建音乐播放器\n","ref":"/2025-07-03/music.html","tags":["navidrome","music_tag_web","filebrowser "],"title":"音乐"},{"body":"\ncentos7 配置 Postfix\nyum install -y postfix mailx install cyrus-sasl-plain cyrus-sasl-lib 检查 /etc/postfix/main.cf 相关配置 cat \u003e\u003e/etc/postfix/main.cf\u003c\u003c'EOF' relayhost = [smtp.qq.com]:587 smtp_sasl_auth_enable = yes smtp_sasl_password_maps = hash:/etc/postfix/sasl_passwd smtp_sasl_security_options = noanonymous smtp_use_tls = yes smtp_tls_CAfile = /etc/ssl/certs/ca-bundle.crt inet_protocols = ipv4 EOF 检查 /etc/postfix/sasl_passwd 内容 # [smtp.qq.com]:587 你的QQ邮箱@qq.com:授权码 echo \"[smtp.qq.com]:587 810654947@qq.com:kqaexaxpbrbdbajd\" \u003e\u003e/etc/postfix/sasl_passwd 生成 hash 文件并设置权限 sudo postmap /etc/postfix/sasl_passwd sudo chmod 600 /etc/postfix/sasl_passwd /etc/postfix/sasl_passwd.db 重启 Postfix sudo systemctl restart postfix 测试 echo \"good luck\" | mail -s \"test\" -r 810654947@qq.com 1209233066@qq.com ","categories":["linux"],"description":"\ncentos7 配置 Postfix |linux\r\n","excerpt":"\ncentos7 配置 Postfix |linux\r\n","ref":"/linux/postfix.html","tags":["mail"],"title":"postfix"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\nfilebeat版本：7.17.29\nelasticsearch版本： 7.15.0\ngraph LR\rsubgraph \"【主配置文件】\"\rA\u003efilebeat.yml]\rend\rsubgraph filebeat\rB[filebeat] end\rsubgraph 定义动态发现\rC[config.inputs]\rend\rA -..-\u003e|\"所有输入、通用设置、全局处理器、模块等[\u003cfont color=red\u003e必须\u003c/font\u003e]\"|B\rB --\u003e C 加载扩展配置文件。由于我更早使用过prometheus,该功能和prometheus中基于文件的自动发现极为相似，因此我将其称之为基于文件的自动发现。\nfilebeat.yml主配置示例：\nfilebeat.config.inputs: enabled: true path: /etc/beats/inputs.d/*.yml reload.enabled: true # 扩展配置文件变动时自动重载 reload.period: 10s output.kafka: hosts: [\"192.168.0.151:9092\",\"192.168.0.152:9092\",\"192.168.0.153:9092\"] topic: '%{[fields.log_topic]}' required_acks: 1 compression: gzip max_message_bytes: 1000000 扩展文件支持inputs和modules相关配置。扩展配置文件示例:\n# cat /etc/beats/inputs.d/1.yml - type: filestream paths: - /var/log/messages scan_frequency: 10s fields: log_topic: default ","categories":["ELK"],"description":"\nfilebeat配置文件之inputs扩展配置\r\n","excerpt":"\nfilebeat配置文件之inputs扩展配置\r\n","ref":"/elk/filebeat/config/extend.html","tags":["input","扩展配置","filebeat"],"title":"扩展配置"},{"body":" os 版本： CentOS Linux release 7.9.2009 (Core)\nelasticsearch版本： 7.15.0、7.17.27\nelasticsearch 包含三个主要的配置:\n大多数配置可以在运行时通过api热修改，配置文件默认在$ES_HOME/config 目录下，可以通过变量 export ES_PATH_CONF=/path/to/my/config 自定义配置文件的路径\nelasticsearch.yml用于配置 Elasticsearch jvm.options用于配置 Elasticsearch JVM log4j2.properties用于配置 Elasticsearch 日志记录 集群管理\n节点管理\n数据和日志路径管理\n内存管理\n网络管理\n自动发现\n数据/日志目录配置\nyaml yaml扁平格式 path: data: /data/elk/instance1/data logs: /data/elk/instance1/log path.data: /data/elk/instance1/data path.logs: /data/elk/instance1/log 集群名称 不同的集群用名字来区分，配置成相同集群名字的各个节点形成一个集群\nyaml yaml扁平格式 cluster: name: cluster-alerts cluster.name: cluster-alerts 节点名称 可以不定义此参数，这时，Elasticsearch自动选择一个唯一的名称\nyaml yaml扁平格式 node: name: node-1 node.name: node-1 网络设置\nyaml yaml扁平格式 network: host: 0.0.0.0 network.host: 0.0.0.0 自动创建索引\n# 关闭自动创建索引，默认为true action.auto_create_index: false # 允许自动创建a开头但不能是an开头的索引 action.auto_create_index: -an*,+a,-* 跨域请求\n#由于前后端服务做了分离，因此需要开启elasticsearch跨域请求 http.cors.enabled: true http.cors.allow-origin: \"*\" [root@node-1 bin]# /data/elk/elasticsearch/bin/elasticsearch-plugin install https://get.infini.cloud/elasticsearch/analysis-ik/7.17.29 -\u003e Installing https://get.infini.cloud/elasticsearch/analysis-ik/7.17.29 -\u003e Downloading https://get.infini.cloud/elasticsearch/analysis-ik/7.17.29 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: plugin requires additional permissions @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ * java.net.SocketPermission * connect,resolve See https://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html for descriptions of what these permissions allow and the associated risks. Continue with installation? [y/N]y -\u003e Installed analysis-ik -\u003e Please restart Elasticsearch to activate any plugins installed curl -XPOST http://192.168.0.151:9200/_analyze \\ -H \"Content-Type: application/json\" \\ -d ' { \"analyzer\": \"ik_max_word\", \"text\": \"千锋教育\" }' su - elasticsearch -c \"export ES_PATH_CONF=/data/elk/instance1/config ;\\ export ES_JAVA_HOME=/data/elk/elasticsearch/jdk ;\\ /data/elk/elasticsearch/bin/elasticsearch-plugin list \" mapping 定义索引分片和副本数量 定义数据类型 { \"settings\": { \"number_of_shards\": 1, \"number_of_replicas\": 0 } } 支持的数据类型\n{ \"mappings\": { \"properties\": { \"name\": { \"type\": \"keyword\", \"index\": false // false 表示不创建索引不支持查询 }, \"value\": { \"type\": \"integer\" }, \"describe\": { \"type\": \"text\", // 支持分词检索 \"fields\": { \"keyword\": { // 保留原始值，用于聚合/排序/精确查询 \"type\": \"keyword\", \"ignore_above\": 256 // 长度超限就忽略，防止膨胀 } } } } } } 示例：\ncurl -X PUT 127.0.0.1:9200/alerts \\ -H \"Content-Type: application/json\" \\ -d' { \"mappings\": { \"properties\": { \"name\": { \"type\": \"keyword\", \"index\": false }, \"value\": { \"type\": \"integer\" }, \"describe\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } } } }, \"settings\": { \"number_of_shards\": 1, \"number_of_replicas\": 0 } }' 示例：动态调整分片的副本数量\n分片数量创建后不可以调整除非删除重建索引。副本数量可以动态调整\ncurl -X PUT 127.0.0.1:9200/alerts/_settings \\ -H \"Content-Type: application/json\" \\ -d' { \"number_of_replicas\" : 1 }' index template 分片数量在index创建后不能调整，一旦创建好索引，更改分片数量的唯一途径就是创建另一个索引并重新索引数据。\ncurl -XGET http://192.168.0.151:9200/_cluster/state/nodes/ { \"cluster_name\": \"cluster-alerts\", \"cluster_uuid\": \"sel1D22xQzWTiq_vZYiGow\", \"nodes\": { \"KrYRJaO-Qwq0FmnCETRnrw\": { \"name\": \"node-3\", \"ephemeral_id\": \"gOriln-aRNWmGg_6JdwkRA\", \"transport_address\": \"192.168.0.153:9300\", \"attributes\": { \"ml.machine_memory\": \"8182120448\", \"ml.max_open_jobs\": \"512\", \"xpack.installed\": \"true\", \"ml.max_jvm_size\": \"1073741824\", \"transform.node\": \"true\" }, \"roles\": [ \"data\", \"data_cold\", \"data_content\", \"data_frozen\", \"data_hot\", \"data_warm\", \"ingest\", \"master\", \"ml\", \"remote_cluster_client\", \"transform\" ] }, \"j1aQHoGtSGORdRYqSrosIQ\": { \"name\": \"node-2\", \"ephemeral_id\": \"bDT3W7RzTVyvXbRfbznVvQ\", \"transport_address\": \"192.168.0.152:9300\", \"attributes\": { \"ml.machine_memory\": \"8182112256\", \"ml.max_open_jobs\": \"512\", \"xpack.installed\": \"true\", \"ml.max_jvm_size\": \"1073741824\", \"transform.node\": \"true\" }, \"roles\": [ \"data\", \"data_cold\", \"data_content\", \"data_frozen\", \"data_hot\", \"data_warm\", \"ingest\", \"master\", \"ml\", \"remote_cluster_client\", \"transform\" ] }, \"28LwCUt0RmqDo0zSbmo8cg\": { \"name\": \"node-1\", \"ephemeral_id\": \"D8-vfrz0SY6uHru_WC84Dg\", \"transport_address\": \"192.168.0.151:9300\", \"attributes\": { \"ml.machine_memory\": \"8182120448\", \"xpack.installed\": \"true\", \"transform.node\": \"true\", \"ml.max_open_jobs\": \"512\", \"ml.max_jvm_size\": \"1073741824\" }, \"roles\": [ \"data\", \"data_cold\", \"data_content\", \"data_frozen\", \"data_hot\", \"data_warm\", \"ingest\", \"master\", \"ml\", \"remote_cluster_client\", \"transform\" ] } } } ❓ 主节点和工作节点，主节点承担了什么任务，通过哪种协议选举出的主节点\n分片数量不超过节点熟练的3倍 内存分配不超过操作系统的50%，但最大不超过32GB curl -XPUT http://192.168.0.151:9200/a/_settings \\ -H \"Content-Type: application/json\" \\ -d' { \"settings\": { \"index.unassigned.node_left.delayed_timeout\": \"5m\" } }' ","categories":["ELK"],"description":"\nelasticsearch支持restfull 方式的CRUD\r\n","excerpt":"\nelasticsearch支持restfull 方式的CRUD\r\n","ref":"/elasticsearch/configure.html","tags":["elasticsearch"],"title":"配置文件"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes/coredns/","tags":"","title":"coredns"},{"body":"\n物理机环境下作为dns服务器 运行在k8s环境中 .:53 { health localhost:8080 { lameduck 5s } ready localhost:8181 reload log errors prometheus :9153 trace zipkin forward . 223.5.5.5 114.114.114.114 { expire 10s } cache 60 hosts { # 使用本机的/etc/hosts文件 1.2.3.4 zero-dew.cn # 扩展定义其他地址解析 1.2.3.5 zero-dew.com ttl 600 reload 30s fallthrough } } tee /usr/lib/systemd/system/coredns.service \u003c\u003cEOF [Unit] Description=coredns service https://coredns.io/ After=network.target [Service] ExecStart=/usr/bin/coredns -conf /etc/corefile -pidfile /var/run/coredns.pid User=root [Install] WantedBy=multi-user.target EOF systemctl enable coredns --now .:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 forward . /etc/resolv.conf { max_concurrent 1000 } hosts { 1.2.3.4 zero-dew.cn 1.2.3.5 zero-dew.com ttl 600 reload 30s fallthrough cluster.local in-addr.arpa ip6.arpa # hosts 执行在kubernetes之前 } cache 30 loop reload loadbalance } ","categories":["coredns"],"description":"\ndemo|kubernetes\r\n","excerpt":"\ndemo|kubernetes\r\n","ref":"/kubernetes/demo.html","tags":["coredns","demo"],"title":"示例"},{"body":" 特性 开源版 企业版 部署与管理 手动部署、配置复杂 支持多种部署模式，提供管理工具（GUI/CLI/API） - 自建环境（软件安装） - Kubernetes - 托管服务（DBaaS） - 多云/混合云 扩展性 扩展需手动分片，可能停机 支持在线水平和垂直扩容 数据同步 无内置数据同步工具 RDI 自动同步关系数据库 - RDI（Redis Data Integration）自动同步关系型数据库 - 支持 Write Behind 模式 技术支持 社区支持 24x7 原厂 + 虹科技术支持 监控与运维 需自行集成监控工具 内置可观测性，支持多种监控平台 - 集成 Prometheus、Grafana、DataDog、Dynatrace 等 - 提供 Admin GUI、CLI、REST API、Terraform 安全性 基础认证，无企业级加密与审计 全链路加密、ACL、LDAP、SSO、合规支持 多租户 - 支持多租户，资源隔离 Proxy支持 - 内置高性能 Proxy，简化客户端接入 AI/ML支持 - 完整向量数据库与特征存储 - 向量数据库（Vector Database） - 特征存储（Feature Store） - 加速大语言模型（LLM）应用 双主架构 - 支持Active-Active系统架构 常见问题 redis 主从切换 1.由于主节点阻塞通讯异常导致的主从切换 ==\u003e 检查慢查询命令（批量写入、批量删除、lua脚本等）、bigkeys\n2.主节点所在机器硬件故障导致的主从切换 ==》 检查主节点是否宕机，等待机器修复；检查服务器日志是否存在硬件报错\rredis 内存使用率过高 ==》 检查是否存在内存泄漏（如：缓存未过期、缓存未命中等）；检查是否存在bigkeys（如：hash、list、set、zset等） ==》 redis节点扩容内存\nredis 连接数过高 ==》 导出客户端连接明细给开发分析，分析是否存在异常连接数增加的客户端或者调整最大连接数配置\nredis 响应时间过高 ==》 检查是否存在慢查询命令（如：批量写入、批量删除、lua脚本等）\n","categories":"","description":"","excerpt":" 特性 开源版 企业版 部署与管理 手动部署、配置复杂 支持多种部署模式，提供管理工具（GUI/CLI/API） - 自建环境（软件安装） - Kubernetes - 托管服务（DBaaS） - 多云/混合云 扩展性 扩展需手动分片，可能停机 支持在线水平和垂直扩容 数据同步 无内置数据同步工具 RDI 自动同步关系数据库 - RDI（Redis Data Integration）自动同步关系型 …","ref":"/docs/database/redis/","tags":"","title":""},{"body":"\n反向代理 nginx 是一个高性能的HTTP和反向代理服务器，本章主要介绍nginx的反向代理配置。\n四层代理 七层代理 Info nginx 在1.9 版本后开始支持 4层代理 配置文件示例：\nuser nginx; worker_processes 1; events { worker_connections 1024; } stream { log_format main '$remote_addr [$time_local] ' '$protocol $status $bytes_sent $bytes_received ' '$session_time \"$upstream_addr\" ' '\"$upstream_bytes_sent\" \"$upstream_bytes_received\" \"$upstream_connect_time\"'; upstream mysql { server 172.16.100.10:3306 weight=1 max_fails=3 fail_timeout=30s; server 172.16.100.11:3306 weight=1 max_fails=3 fail_timeout=30s; server 172.16.100.12:3306 weight=1 max_fails=3 fail_timeout=30s; } server { listen 3306; proxy_connect_timeout 2s; proxy_timeout 900s; proxy_pass mysql; access_log /dev/stdout main; } } user nginx; worker_processes 1; events { worker_connections 1024; } http { upstream backend { # backup 当其他主机失败后使用该主机，不能用在 hash ip_hash 和random 调度算法中 # down 当前主机不可用，当使用ip_hash 中标记为down不会影响hash值 server 172.16.100.10:8080 weight=1 max_fails=3 fail_timeout=30s; server 172.16.100.11:8080 weight=1 max_fails=3 fail_timeout=30s; server 172.16.100.12:8080 weight=1 max_fails=3 fail_timeout=30s; server 172.16.100.13:8080 backup; server 172.16.100.14:8080 backup; } server { # 设置X-Real-IP头，让服务端看到客户端的真实IP地址。 proxy_set_header X-Real-IP $remote_addr; # 设置X-Forwarded-For头，添加原始请求的IP地址 proxy_set_header X-Forwarded-For $remote_addr; # 设置代理请求的Host头，使用请求的原始主机名 proxy_set_header Host $host; # 设置客户端请求的最大body大小为50MB client_max_body_size 50m; # 设置客户端请求body的缓冲区大小为256KB client_body_buffer_size 256k; location / { proxy_pass http://backend; } } } 对于七层代理服务器主要有ngx_http_upstream_module和ngx_http_proxy_module 模块实现，\nngx_http_proxy_module proxy_set_header 设置 http 请求 header并传递给后端服务器。 格式 proxy_set_header field value;\nproxy_pass 把用户的请求转向到反向代理定义的 upstream 服务器池\nproxy_redirect 重写location并刷新从upstream server收到的报文的首部；\nclient_body_buffer_size 指定客户端请求主体缓冲区大小\nproxy_connect_timeout 30s; 反向代理与后端节点服务器连接的超时时间\nproxy_send_timeout 在规定时间之内后端服务器必须传完所有数据，否则 Nginx 将断开这个连接\nproxy_read_timeout Nginx 从代理的后端服务器获取信息的超时时间\nproxy_buffering on; #如果缓冲区开启，把内容保存在由指令 proxy_buffer_size proxy_buffers\nproxy_buffer_size 32k; #默认来说,该缓冲区大小等于指令proxy_buffers\nproxy_buffers 32 4k; 设置缓冲区的数量和大小，Nginx 从代理的后端服务器获取的响应信息会放置到缓冲区\nproxy_busy_buffers_size 64k; 设置系统很忙时使用的 proxy_buffers 大小，官方推荐为 proxy_buffers * 2\nproxy_temp_file_write_size 指定 proxy 缓存临时文件的大小\nproxy_next_upstream error timeout #请求发生错误分配到写一个web\nproxy_cookie_domain 将upstream server通过Set-Cookie首部设定的domain属性修改为指定的值，其值可以为一个字符串、正则表达式的模式或一个引用的变量；\nproxy_cookie_path 将upstream server通过Set-Cookie首部设定的path属性修改为指定的值，其值可以为一个字符串、正则表达式的模式或一个引用的变量；\nproxy_hide_header 设定发送给客户端的报文中需要隐藏的首部；\n获取到真实的用户ip 位置：server location 指令下 配置示例：\nlocation / { proxy_pass http://backend_server; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; # 客户端真实 IP（单值） proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 记录完整的代理路径（多层代理时拼接所有中间节点 IP） proxy_set_header X-Forwarded-Proto $scheme; # 协议（http/https） } 重写发送给后端的http请求头的Host 位置：server location 指令下 配置示例：\nlocation / { proxy_pass http://backend_server; proxy_set_header Host $host; } 设置代理连接超时时间 位置：server location 指令下 配置示例：\nlocation / { proxy_pass http://backend_server; proxy_connect_timeout 30; # 设置代理连接超时时间为30秒。 proxy_send_timeout 30; # 设置代理发送请求的超时时间为30秒。 proxy_read_timeout 60; # 设置代理读取响应的超时时间为60秒。 } 设置代理buffer 位置：server location 指令下 配置示例：\nlocation / { proxy_pass http://backend_server; proxy_buffering on; proxy_buffer_size 32k; # 设置代理缓冲区的大小为4KB proxy_buffers 4 32k; # 设置代理缓冲区的数量和大小 为4*32KB。 proxy_busy_buffers_size 64k; # 设置代理繁忙缓冲区的大小为64KB } 提高代理的容错能力 位置：server location 指令下 配置示例：\nlocation / { proxy_pass http://backend_server; # 定义当出现错误、超时、无效头部、或HTTP状态码为500、503、404时，请求将被转发到下一个上游服务器。 proxy_next_upstream error timeout invalid_header http_500 http_503 http_404; } 镜像静态内容到本地磁盘 位置：server location 指令下 配置示例：\nlocation / { proxy_pass http://backend_server; proxy_store on; # 开启代理存储静态内容到磁盘的功能 proxy_store_access user:rw group:rw all:r; # 设置代理存储文件的访问权限。 proxy_temp_path /dev/shm/nginx_proxy; # 定义了代理临时文件存储的路径 } 调度算法 Nginx使用的负载均衡调度算法主要有以下几种：\n轮询（rr Round Robin）：这是默认的负载均衡算法。每个请求按时间顺序逐一分配到不同的后端服务器，如果服务器down掉，能自动剔除。\n加权轮询（wrr Weighted Round Robin）：与轮询类似，但是不同的后端服务器可以设置不同的权重，可以根据服务器的处理能力，分配不同数量的请求。权重越高，分配的请求越多。\n最少连接（least_conn Least Connections）：优先分配给当前连接数最少的服务器，适用于请求处理时间相差较大的情况。\nIP Hash(ip_hash)：根据请求的IP的hash结果分配，每个请求会固定访问一个后端服务器，适用于需要会话保持的应用。这有助于为缓存服务器实现更高的缓存命中率,如果其中一台服务器需要临时删除，则应使用down参数标记该服务器，以保留客户端IP地址的当前哈希值\nURL Hash(url_hash)：根据请求的URL的hash结果来分配请求，使得每个URL定向到同一个后端服务器，适用于服务器缓存时提高效率。\nFair（第三方）：按后端服务器的响应时间来分配请求，响应时间短的优先分配。\nURL参数（第三方）：按照URL中带的参数进行hash，然后进行请求分发，可以实现会话保持。\n以上算法可以通过在Nginx的http或stream模块中使用upstream指令来配置。\n","categories":["nginx"],"description":"7层反向代理|nginx\n","excerpt":"7层反向代理|nginx\n","ref":"/nginx/upstream.html","tags":["方向代理"],"title":"反向代理"},{"body":"\npipeline 最佳实践 处理逻辑抽离成函数，通过共享库的方式调用。\nJenkinsfile 通过代码仓库管理\n共享库实践\n配置jenkins 共享库\n​\t路径： Dashboard \u003e 系统管理 \u003e System \u003e Global Trusted Pipeline Libraries\n编写共享库文件\n# 目录结构 # src 下放置 具体实现逻辑函数 mylib ├─resources │ └─config ├─src │ └─org │ └─jenkins | └─ hello.groovy └─vars package org.jenkins def hi(data) { printf(\"%s\",\"来自共享库函数\") return data } 在Jenkinsfile中调用共享库\n/* @Library 为固定语法声明加载共享库 jenkinslib 为jenkins 全局配置中配置的共享库的名称 _ 加载共享库 */ @Library('jenkinslib') _ // 实例化 org.jenkins 包下 hello.groovy 文件 def tools=new org.jenkins.hello() pipeline { agent any stages { stage(\"run\") { steps { script { // 调用共享库中的hi 函数 tools.hi(\"123\") // 调用共享库中的配置文件 def data=libraryResource 'config/config.json' result=tools.hi(data) println result } } } } } gitlab 代码提交触发流水线 安装插件 【 Generic Webhook Trigger 】 支持jsonpath 语法解析 【 Pipeline Utility Steps 】\npipeline { agent any stages { stage('Parse JSON') { steps { script { // JSON 字符串 def jsonString = '{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}' // 解析 JSON 字符串 def jsonObj = readJSON text: jsonString // 访问 JSON 对象中的属性 def name = jsonObj.name def age = jsonObj.age def city = jsonObj.city // 打印 JSON 数据 echo \"Name: ${name}\" echo \"Age: ${age}\" echo \"City: ${city}\" } } } } } http://175.178.65.213:18080/generic-webhook-trigger/invoke?token=qGv2J.wXhNnzg 分支名称 commit_id 运维管理 备份jenkins 停止进程 打包 JENKINS_HOME 目录 cd /root tar --exclude .jenkins/workspace -cvzf jenkins.tar.gz .jenkins 恢复方式： 解压打包文件到当前环境 JENKINS_HOME 目录 用户和权限管理 https://mp.weixin.qq.com/s/_DIWInIddfcsdggDCGOCyA\njenkins使用基于RBAC(role base access contoller)进项授权控制，要使用该功能需要安装对应插件【 Role-based Authorization Strategy 】\n添加用户\nDashboard \u003e Manage Jenkins \u003e Jenkins’ own user database 修改授权策略\n修改jenkins授权策略。修改为 Role-Based Strategy\n修改方式： Dashboard \u003e Manage Jenkins \u003e Security\n添加权限\n示例中实现目标： 开发人员dev01 只拥有自己项目 seagull 流水线的构建权限\n修改/新增jenkins角色权限\n修改方式：Dashboard \u003e Manage Jenkins \u003e Manage and Assign Roles\n新增权限\n[Warning]\n确保 Item roles 中已经存在的不可以在 Global roles 中配置，否则Global roles中权限会覆盖掉 Item roles 中配置\n授权给用户\n[Warning]\n需要双重授权\n使用dev01 用户登录检查权限 邮件、钉钉通知 邮件通知 安装邮件插件 【 Email Extension 】\nsmtp_address = smtp.qq.com smtp_port = 465 smtp_user_name = 810654947@qq.com smtp_password = kqaexaxpbrbdbajd smtp_domain = smtp.qq.com smtp_tls = true 添加凭据 设置管理员邮箱 pipeline { agent any stages { stage('Hello') { steps { emailext to: '1209233066@qq.com', mimeType: \"text/html\", subject: \"流水线'${JOB_NAME}' (${BUILD_NUMBER})构建通知\", body: \"\"\" 项目名称: ${PROJECT_NAME } \u003cbr\u003e 构建详情：\u003ca href=${BUILD_URL}console\u003e查看详情\u003c/a\u003e \"\"\" } } } } def now = new Date() pipeline { agent any stages { stage('Hello') { steps { emailext to: '1209233066@qq.com', mimeType: \"text/html\", subject: \"流水线'${JOB_NAME}' (${BUILD_NUMBER}) ${currentBuild.currentResult}构建通知\", body: \"\"\" \u003ctable border=1\u003e \u003cth\u003e项目名称\u003c/th\u003e \u003cth\u003e构建id\u003c/th\u003e \u003cth\u003e构建状态\u003c/th\u003e \u003cth\u003e构建日志\u003c/th\u003e \u003cth\u003e邮件发送时间\u003c/th\u003e \u003ctr\u003e \u003ctd\u003e ${JOB_NAME} \u003c/td\u003e \u003ctd\u003e ${BUILD_NUMBER} \u003c/td\u003e \u003ctd\u003e ${currentBuild.currentResult} \u003c/td\u003e \u003ctd\u003e\u003ca href=${BUILD_URL}console\u003e查看详情\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e${now.format('yyyy-MM-dd HH:mm:ss')}\u003c/td\u003e \u003c/tr\u003e \u003c/table\u003e \"\"\" } } } } 代码扫描 jenkins 集成sonarqube\n创建凭据 在clone后 添加扫描阶段（golang） stage('sonarqube') { environment { SONAR_SCANNER_HOME = \"/opt/sonar-scanner\" PATH = \"${SONAR_SCANNER_HOME}/bin:$PATH\" } steps { withCredentials([usernamePassword( credentialsId: 'sonarQube', passwordVariable: 'password', usernameVariable: 'username')]) { script { sh ''' sonar-scanner \\ -Dsonar.host.url=http://175.178.65.213:9000 \\ -Dsonar.projectKey=seagull-agent \\ -Dsonar.projectName=seagull-agent \\ -Dsonar.projectVersion=1.1 \\ -Dsonar.login=\"${username}\" \\ -Dsonar.password=\"${password}\" \\ -Dsonar.ws.timeout=30 \\ -Dsonar.projectDescription='seagull-agent' \\ -Dsonar.links.homepage=https://e.gitee.com/bj-pytc/repos/bj-pytc/pcloud-agent/sources \\ -Dsonar.links.ci=http://175.178.65.213:18080/job/seagull-agent/job/seagull-agent/ \\ -Dsonar.sourceEncoding=UTF-8 ''' } } } } 通过安装插件执行代码扫描【 SonarQube Scanner 】\nsonarQube 中生成token sqa_718d7307c781818bbecdd30871d28061280a7a26 将token 转换为jenkins的 credentials 添加扫描阶段 stage('sonarqube') { environment { SONAR_SCANNER_HOME = \"/opt/sonar-scanner\" PATH = \"${SONAR_SCANNER_HOME}/bin:$PATH\" } steps { // installationName 是第三步创建的名称 // credentialsId 为 第二步中创建的credentials withSonarQubeEnv(installationName: 'sonarQube',credentialsId: 'sonarQubetoken') { script { sh ''' sonar-scanner \\ -Dsonar.host.url=http://175.178.65.213:9000 \\ -Dsonar.projectKey=seagull-agent \\ -Dsonar.projectName=seagull-agent \\ -Dsonar.projectVersion=1.1 \\ -Dsonar.login=\"${SONAR_AUTH_TOKEN}\" \\ -Dsonar.ws.timeout=30 \\ -Dsonar.projectDescription='seagull-agent' \\ -Dsonar.links.homepage=https://e.gitee.com/bj-pytc/repos/bj-pytc/pcloud-agent/sources \\ -Dsonar.links.ci=http://175.178.65.213:18080/job/seagull-agent/job/seagull-agent/ \\ -Dsonar.sourceEncoding=UTF-8 ''' } } } } 执行結果，此时多个一个sonarQube的扫描结构连接 创建凭证拉取代码 jienkins 提供了凭据管理，避免将密码或token 暴露在流水线中。需要插件 【 Credentials 】\n配置方式： Dashboard \u003e Manage Jenkins \u003e Credentials\n添加全局作用域的credentials 创建一个全局的，基于用户名和密码的credentials,该credentials会在后面流水线中引用。\n引用时使用 Description 作为标识引用该credentials FQA 任务无法停止 问题描述：\n在构建一个nodejs项目时，一个构建任务运行了3天5小时。这时通过jenkins 页面取消该任务时没有响应，重启jenkins进程后jenkins会把未完成的任务直接拉起继续运行。\n解决办法\n强制取消任务\n【系统管理】\u003e 【节点和运管理】\u003e【master】\u003e【脚本命令行】\nmaster 表示当前任务运行的节点\nseagull-app 表示当前的job 名称\n2 表示需要取消的构建号\nJenkins.instance.getItemByFullName(\"seagull-app\") .getBuildByNumber(2) .finish( hudson.model.Result.ABORTED, new java.io.IOException(\"Aborting build\") ); 为流水线设置超时时间\noptions { timeout(time:3,unit: 'HOURS') } 参考\nJenkins 强制停止 job 执行_jenkins强制停止任务-CSDN博客\n【Jenkins】Pipeline - 设置超时时间_jenkins设置超时时间-CSDN博客\ngroovy\nhttps://mp.weixin.qq.com/s/TqFCyCmLHEtWoVJNq6_0cw\n","categories":["devops"],"description":"最佳实践|jenkins\n","excerpt":"最佳实践|jenkins\n","ref":"/devops/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.html","tags":["jenkins"],"title":"最佳实践"},{"body":"","categories":"","description":"\nsonar 文档中心\r\n","excerpt":"\nsonar 文档中心\r\n","ref":"/docs/devops/sonar/","tags":"","title":""},{"body":"amtool amtool is a cli tool for interacting with the Alertmanager API.\ngo get github.com/prometheus/alertmanager/cmd/amtool\n配置文件\n需要配置到/etc/amtool/config.yml\n或$HOME/.confg/amtool/config.yml\ntee /etc/amtool/config.yml \u003c\u003cEOF alertmanager.url: http://127.0.0.1:9093 author: \"1209233066@qq.com\" require-comment: true output: josn date.format: \"2006-01-02 15:04:05 MST\" EOF 命令 子命令 举例 举例解释 alert amtool alert 查看所有告警 add amtool alert add alertname=test labe-a=value-b 添加一个名称为test的带有标签为：labe-a=value-b的告警 query amtool alert query alertname=test labe-a=value-b 查看指定标签的告警 amtool alert query alertname=test labe-a=value-b -o json json 格式展示告警详细信息 amtool alert query alertname=test labe-a=value-b -o extended 扩展格式显示告警 silence amtool silence 查看所有维护期的告警 add amtool silence add alertname=test labe-a=value-b –commen “this is test” 添加一个名称为test的带有标签为：labe-a=value-b的维护期，默认1h amtool silence add alertname=test labe-a=value-b –commen “this is test” –duration=“2h” 设置2h维护期 query amtool silence query alertname=test labe-a=value-b -o extended 查看指定标签的维护期 amtool silence query alertname=test labe-a=value-b -o json json 格式展示维护期的详细信息 amtool silence query alertname=test labe-a=value-b -o extended 扩展格式显示维护期 expire amtool silence expire 223eb624-f49d-498d-ac7d-34bb7570adea 提前取消维护期(id 通过 amtool silence 查询) config template check-config amtool check-config alertmanager.yml ","categories":["prometheus","alertmanager"],"description":"\namtool|alertmanager|prometheus\r\n\r\n","excerpt":"\namtool|alertmanager|prometheus\r\n\r\n","ref":"/prometheus/alertmanager/amtool.html","tags":["prometheus","alertmanager","amtool"],"title":"amtool"},{"body":"","categories":"","description":"","excerpt":"","ref":"/Observability/grafana/","tags":"","title":"grafana"},{"body":"配置文件全景图\ngraph LR\rA\u003eprometheus.yml] ==\u003e |全局配置|B(\u003ca href=#global\u003eglobal\u003c/a\u003e)\rA ==\u003e |对接alert|C(\u003ca href='#alterings'\u003ealterings\u003c/a\u003e)\rA ==\u003e |评估/告警规则|D(\u003ca href='#rule_files'\u003erule_files\u003c/a\u003e)\rA ==\u003e |指标抓取|E(\u003ca href='#scrape_configs'\u003escrape_configs\u003c/a\u003e)\rA ==\u003e |外部存储|F(remote_write/remote_read)\rE -..-\u003e |静态配置|E1(\u003ca href=#static_configs\u003estatic_configs\u003c/a\u003e)\rE -..-\u003e |文件自动发现|E2(\u003ca href=#file_sd_configs\u003efile_sd_configs\u003c/a\u003e)\rE -..-\u003e |consul自动发现|E3(\u003ca href=#consul_sd_configs\u003econsul_sd_configs\u003c/a\u003e)\rE -..-\u003e |kubernetes自动发现|E4(\u003ca href='#kubernetes_sd_configs'\u003ekubernetes_sd_configs\u003c/a\u003e)\rE -..-\u003e |dns自动发现|E5(\u003ca href='#dns_sd_configs'\u003edns_sd_configs\u003c/a\u003e)\rE -..-\u003e |基于单个job重新打标,在指标采集之前完成|E6(\u003ca href='#relabel_configs'\u003erelabel_configs\u003c/a\u003e)\rE -..-\u003e |基于单个metrics重新打标,在指标采集之后数据存储之前完成|E7(metric_relabel_configs)\rE -..-\u003e |发送给alertmanger时重新打标|E8(alert_relabel_configs)\rE -..-\u003e |远程写入样本时重新打标|E9(write_relabel_configs) global 配置示例：\nglobal: # 1m 抓取一次target scrape_interval: 1m # 10s 抓取不到超时 scrape_timeout: 20s # 1m 评估一次rules evaluation_interval: 1m # 全局标签(在每一个时间序列上添加zoon=shanghai 的标签) external_labels: center: \"shanghai\" env: \"staging\" # 记录promql 的查询记录 query_log_file: \"/data/prometheus/log/promql\" alterings 配置示例：\nglobal: # 1m 抓取一次target scrape_interval: 1m # 10s 抓取不到超时 scrape_timeout: 20s # 1m 评估一次rules evaluation_interval: 1m # 全局标签(在每一个时间序列上添加zoon=shanghai 的标签) external_labels: center: \"shanghai\" env: \"staging\" # 记录promql 的查询记录 query_log_file: \"/data/prometheus/log/promql\" alerting: alertmanagers: - path_prefix: / static_configs: - timeout: 30s targets: # 通常情况 这三个节点是一个集群 - 'alert1:9093' - 'alert2:9093' - 'alert3:9093' rule_files global: # 1m 抓取一次target scrape_interval: 1m # 10s 抓取不到超时 scrape_timeout: 20s # 1m 评估一次rules evaluation_interval: 1m # 全局标签(在每一个时间序列上添加zoon=shanghai 的标签) external_labels: center: \"shanghai\" env: \"staging\" # 记录promql 的查询记录 query_log_file: \"/data/prometheus/log/promql\" alerting: alertmanagers: - path_prefix: / static_configs: - timeout: 30s targets: # 通常情况 这三个节点是一个集群 - 'alert1:9093' - 'alert2:9093' - 'alert3:9093' rule_files: # 在prometheus.yml主配置文件加载 record规则 和 alert规则 # 相对于主配置文件 - \"./rules/*.yaml\" 记录规则 告警规则 语法\n--- groups: - name: example interval: 30s # 规则评估周期, 省略时使用全局配置 limit: 0 # 对时间序列的限制，0表示不做限制 query_offset: 0 # 评估时的时间偏移量 labels: # 添加或修改标签 - \u003clabelname\u003e: \u003clabelvalue\u003e rules: - record: code:prometheus_http_requests_total:sum expr: sum by (code) (prometheus_http_requests_total) labels: # 添加或修改标签 - \u003clabelname\u003e: \u003clabelvalue\u003e 示例\n#./rules/record.yaml groups: - name: node_cpu rules: # 命名规范 https://prometheus.io/docs/practices/rules/#recording-rules - record: instance:node_cpu_seconds:avg_rate5m # record: level:metric:operation expr: avg by (job,instance,mode) (rate(node_cpu_seconds_total[5m])) - record: instance:node_cpu_seconds:avg_rate5m expr: avg by (job,instance,mode) (rate(node_cpu_seconds_total[5m])) labels: metric_type: \"aggration\" /data/prometheus/bin/promtool check rules ./rules/record.yaml 语法：相比record 规则，labels 支持\u003cstrong\u003e字符串模版\u003c/strong\u003e 语法\n--- groups: - name: example interval: 30s # 规则评估周期, 省略时使用全局配置 limit: 0 # 对时间序列的限制，0表示不做限制 query_offset: 0 # 评估时的时间偏移量 labels: # 添加或修改标签 - \u003clabelname\u003e: \u003clabelvalue\u003e rules: - alert: nodeCpuHight expr: 100 * (1 - avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) by(instance)) \u003e for: 2m # 达到expr 条件并持续2m,则告警从pending 转为firing keep_firing_for: 0s # 不满足expr 条件后，告警继续保持的时长 labels: # 添加或修改标签 - \u003clabelname\u003e: \u003ctmpl_string\u003e annotations: - \u003clabelname\u003e: \u003ctmpl_string\u003e 示例\n#./rules/alert.yaml groups: - name: node_cpu rules: - alert: nodeCpuHigh expr: 100 * (1 - avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) by(instance)) \u003e85 for: 5m # for 参数可省略。Alerting rules without the for clause will become active on the first evaluation labels: severity: WARNING annotations: summary: \"主机【Instance {{ $labels.instance }}】cpu 使用率高\" description: {{$labels.instance}} of job {{ $labels.job }} cpu 使用率超过85%,当前值{{ $value |printf \"%.1f\"}}% scrape_configs static_configs file_sd_configs consul_sd_configs kubernetes_sd_configs 示例\nscrape_congfigs: - job_name: prometheus scrape_interval: 5m scrape_timeout: 10s metrics_path: /metrics scheme: http static_configs: - targets: ['localhost:8080','localhost:8081'] labels: env: 'production' - job_name: 'node' static_configs: - targets: - localhost:9100 # 只收集指定的指标 params: collect[]: - cpu - meminfo - diskstats - netdev - filefd - filesystem - xfs - systemd # curl -g -X GET http://127.0.0.1:9100/metrcs?collect[]=cpu 示例\nscrape_congfigs: - job_name: prometheus scrape_interval: 5m scrape_timeout: 10s metrics_path: /metrics scheme: http file_sd_configs: - files: - file/*.yml refresh_interval: 5m #cat file/file_sd.yaml - targets: - 172.29.1.11:9090 labels: app: prometheus job: primetheus 示例\nscrape_configs: - job_name: \"node\" consul_sd_configs: - server: \"47.113.100.31:8500\" tags: - \"nodes\" refresh_interval: 2m 示例\nscrape_configs: - job_name: kube-apiserver scheme: https kubernetes_sd_configs: - api_server: https://127.0.0.1:6443 tls_config: ca_file: /etc/kubernetes/pki/ca.pem cert_file: /etc/kubernetes/pki/cert.pem key_file: /etc/kubernetes/pki/cert.key insecure_skip_verify: true role: endpoints # 如果prometheus部署在集群中，则使用endpoints,如果部署在集群外，则使用service或node relabel_configs: - source_labels: [ __meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name, ] action: keep regex: default;kubernetes;https relabel_configs relabel_configs 用于在指标抓取前对标签进行预处理。应用场景包括丢弃指定指标、替换抓取指标的端口、新增或修改label。\n语法：\nscrape_congfigs: - relabel_configs: - source_labels: [' \u003clabelname\u003e [, ...] '] # 定义预处理的标签列表 separator: \u003cstring\u003e | default = ; # 定义source_labels 中的标签以哪个字符串作为分隔符 regex: \u003cregex\u003e | default = (.*) # 对source_labels中的标签按照指定正则表达式匹配 action: \u003crelabel_action\u003e | default = replace # 正则匹配到的部分进行处理动作 target_label: \u003clabelname\u003e # 指明匹配到的标签计划被哪标签替换。对于replace来说标签是必须的 replacement: \u003cstring\u003e | default = $1 # relace中定义替换后 **值** 应该替换为什么，支持正则向后引用 source_labels 和 target_label\n可以利用指标标签以及prometheus、consul、kubernetes等特有标签作为 source_labels 和 target_labels\n例如prometheus 可以识别的标签:\n标签名 标签值 __scheme__ http 或 https __address__ ip:port instance标签会使用该值 __metrics_path__ 默认为 /metrics __param__ url 中传递过来的参数 __scrape_interval__ 抓取周期 __name__ 指标名称 __tmp__ 在打标阶段需要存储临时标签 action\n动作包括：replace、lowercase、uppercase、keep、drop、keepequal、dropequal、hashmod、labelmap、labeldrop、labelkeep\n示例 action: 新增标签 replace drop keep labeldrop labelkeep scrape_congfigs: - relabel_configs: # 添加固定标签,给当前job添加一个env=\"pro\" - replacement: pro target_label: env scrape_congfigs: - relabel_configs: - source_labels: [\"__address__\"] separator: \";\" regex: \"(^[0-9].*):[0-9]+\" action: replace replacement: \"$1\" # 新的标签值 target_label: \"IP\" # 新的标签名称 scrape_congfigs: - relabel_configs: - source_labels: [\"__name__\"] separator: \";\" regex: \"go_gc_duration_seconds\" action: drop # 删除匹配的metrics scrape_congfigs: - relabel_configs: - source_labels: [\"__name__\"] separator: \";\" regex: \"^(node|pod|kube|mysql|redis|mongo)\" action: keep # 保存匹配的metrics scrape_congfigs: - relabel_configs: - regex: \"job\" action: labeldrop # 删除了job 标签 scrape_congfigs: - relabel_configs: - regex: \".*\" action: labelkeep ","categories":["prometheus","监控"],"description":"\npromethes.yaml|prometheus\r\n\r\n","excerpt":"\npromethes.yaml|prometheus\r\n\r\n","ref":"/prometheus/config.html","tags":["prometheus","配置"],"title":"config"},{"body":"Prometheus是Go语言开发的开源监控和警报框架，遵循Apache 2.0许可协议。它起源于SoundCloud，并在2016年成为云原生计算基金会（CNCF）继Kubernetes之后的第二个项目。Prometheus不仅提供监控功能，还是一个时序数据库。\n本示例架构图\nlocation /prom { proxy_pass http://127.0.0.1:9090; } location /alert { proxy_pass http://127.0.0.1:9093; } location /grafana { proxy_pass http://127.0.0.1:3000; } flowchart LR\rA(nginx)\rB(prometheus)\rC(alertmanager)\rD(grafana)\rz\u003e浏览器] --\u003e A\rA -..-\u003e|/prom| B\rA -..-\u003e|/alert| C\rA -..-\u003e|/grafana| D\rstyle A fill:#f9f,stroke:#333,stroke-width:2px\rstyle B fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5 5\rstyle C fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5 5\rstyle D fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5 5 ","categories":"","description":"","excerpt":"Prometheus是Go语言开发的开源监控和警报框架，遵循Apache 2.0许可协议。它起源于SoundCloud，并在2016年成为云原生计算基金会（CNCF）继Kubernetes之后的第二个项目。Prometheus不仅提供监控功能，还是一个时序数据库。\n本示例架构图\nlocation /prom { proxy_pass http://127.0.0.1:9090; } …","ref":"/docs/observability/prometheus/","tags":"","title":"prometheus"},{"body":"os操作文件系统 os.Create() 创建文件或清空文件\npackage main import ( \"fmt\" \"os\" ) func main() { // 创建文件或清空文件 如果文件不存在则创建默认权限0644，如果文件存在文件中内容会被清空 if f, err := os.Create(\"1.txt\"); err != nil { fmt.Println(err) } else { defer f.Close() fmt.Fprintln(f, \"创建了一个文件,并向文件中写入了一行\") } } os.Mkdir() 创建目录\npackage main import ( \"fmt\" \"os\" ) func main() { // os.Mkdir() 创建目录,可以看作是linux中mkdir if err := os.Mkdir(\"newdir\", 0755); err != nil { fmt.Println(err) //目录如果存在时执行报错：mkdir newdir: file exists } } os.MkdirAll()递归创建目录\npackage main import ( \"fmt\" \"os\" ) func main() { // 递归创建目录 if err := os.MkdirAll(\"newdir1/newdir2\", 0755); err != nil { fmt.Println(err) //当目录存在时不会报错 } } os.Rename() 移动目录\npackage main import ( \"fmt\" \"os\" ) func main() { // mv 目录 if err := os.Rename(\"newdir1\", \"/tmp/newdir1\"); err != nil { // 当目标已经存在时报错：rename newdir1 /tmp/newdir1: file exists // 当目标是文件时报错：rename newdir1 /tmp/newdir1: not a directory fmt.Println(err) } } os.Open() 只读方式打开文件\npackage main import ( \"fmt\" \"io\" \"os\" ) func main() { // os.Open() 只读方式打开文件 if f, err := os.Open(\"/etc/hosts1\"); err != nil { fmt.Print(err) //如果文件不存在报错：open /etc/hosts1: no such file or directory } else { defer f.Close() // 分批次循环读取文件 var buffer [128]byte var result []byte for { lenth, err := f.Read(buffer[:]) if err == io.EOF { break } result = append(result, buffer[:lenth]...) } fmt.Println(string(result)) } } os.OpenFile() 整合并扩展了os.Create()，os.Open()功能\npackage main import ( \"fmt\" \"os\" ) func main() { // O_CREATE create a new file if none exists. // O_RDONLY open the file read-only. // O_WRONLY open the file write-only. // O_RDWR open the file read-write. // O_APPEND append data to the file when writing. f, err := os.OpenFile(\"1.txt\", os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0644) if err != nil { fmt.Println(err) } fmt.Fprintln(f, \"追加一条信息到文本\") } os.Remove() 和 os.RemoveAll()文件删除\npackage main import ( \"fmt\" \"os\" ) func main() { // 删除文件 或 空目录[目录下有目录或文件不能被删除] var fileList = []string{} os.Create(\"1.txt\") os.MkdirAll(\"dir1/dir2\", 0755) fileList = append(fileList, \"1.txt\") fileList = append(fileList, \"dir1\") //remove dir1: directory not empty fileList = append(fileList, \"dir1/dir2\") for _, fileName := range fileList { if err := os.Remove(fileName); err != nil { fmt.Println(err) continue } } } package main import ( \"fmt\" \"os\" ) func main() { // 删除文件 或 目录 var fileList = []string{} os.Create(\"1.txt\") os.MkdirAll(\"dir1/dir2\", 0755) fileList = append(fileList, \"1.txt\") fileList = append(fileList, \"dir1\") for _, fileName := range fileList { if err := os.RemoveAll(fileName); err != nil { fmt.Println(err) } } } os.Stat() 文件状态\npackage main import ( \"fmt\" \"os\" ) func main() { if fileInfo, err := os.Stat(\"/etc/hosts\"); err != nil { fmt.Println(err) } else { // 文件名，basename /etc/hosts filename := fileInfo.Name() fmt.Println(filename) // 文件大小 size := fileInfo.Size() fmt.Printf(\"%d bytes\\n\", size) if fileInfo.IsDir() { fmt.Println(\"是目录\") } else { fmt.Println(\"是文件\") } } } os.Args 获取命令行传递的参数\npackage main import ( \"fmt\" \"os\" ) func main() { fmt.Println(os.Args[1:]) } [root@golang ~]# go run test.go 123 455 789 [123 455 789] os/exec 执行系统命令\npackage main import ( \"fmt\" \"log\" \"os/exec\" ) func main() { cmd := exec.Command(\"ls\", \"-l\") // 更改为你需要执行的命令和参数 output, err := cmd.CombinedOutput() if err != nil { log.Fatal(err) } fmt.Println(string(output)) } 示例：\nos获取环境变量\npackage main import ( \"fmt\" \"os\" ) func main() { //os.LookupEnv 和 os.Getenv 都可以读取环境变量的值，os.Getenv 只返回值（不存在时返回空串），无法区分“未设置”与“已设置但为空”。 // 获取系统环境变量 hostname := os.Getenv(\"USER\") uid := os.Geteuid() fmt.Printf(\"当前用户名:%s\\n当前用户uid:%d\\n\", hostname, uid) } 文件读写示例\npackage main import ( \"fmt\" \"io\" \"os\" ) func main() { readFile(\"email04.png\", \"email041.png\") } func readFile(oldfile, newfile string) { // 实现文件的复制 file, err := os.Open(oldfile) if err != nil { fmt.Println(err) } var buffer [128]byte var result []byte for { if seed, err := file.Read(buffer[:]); err != nil \u0026\u0026 err == io.EOF { fmt.Println(\"文件读取完毕\") break } else { result = append(result, buffer[:seed]...) } } if f, err := os.OpenFile(newfile, os.O_CREATE|os.O_APPEND|os.O_RDWR, 0644); err != nil { fmt.Println(err) defer f.Close() } else { defer f.Close() f.Write(result) } } ","categories":["golang"],"description":"os|标准库\n","excerpt":"os|标准库\n","ref":"/golang/package/os.html","tags":["golang","os"],"title":"os"},{"body":"golang 中数据类型分为基本数据类型和复合数据类型，常量只能使用基本数据类型，变量可以使用全部数据类型。\n基本数据类型 数值型 字符串 布尔型 按照取值范围可以分为：int,int8, int16 ,int32 int64,uint,uint8 uint16 ,uint32, uint64 ,float32,float64\npackage main import \"fmt\" func main() { var weigh, price float32 = 100.1, 9.99 // %T 查看数据类型 fmt.Printf(\"数据类型：%T\\n数据类型：%T\\n\", weigh, price) } 获取字符串长度和字符串切片\npackage main import \"fmt\" func main() { // len 获取字符串长度 var message = `相对于技术革新的速度， 人性数千年的变换却是渺小的` fmt.Printf(\"%s\\n占用%dbytes\\n\", message, len(message)) // 字符串切片，一个汉字占用3个字符 // substr := message[:] // substr := message[0] substr := message[0:3] fmt.Printf(\"%s\\n\", substr) } bool 数据类型取值范围 true 和 false\npackage main import \"fmt\" func main() { var overall bool overall = true fmt.Printf(\"数据类型：%T\\n\", overall) //数据类型：bool } 复合数据类型 指针 pointer 数组 arrary 切片 slice 集合 map 结构体 struct 接口 interface 指针保存了变量的内存地址，可以是 *int *float64 *bool *string\n指针类型变量声明:\nvar \u003cname\u003e *[type] = [value] var 关键字，表示声明一个变量 \u003cname\u003e 变量名称 *[type] type为引用的数据类型 示例：\npackage main import \"fmt\" func main() { age := 32 var Age = \u0026age fmt.Printf(\"数据类型：%T\\n变量中存储的值:%d\\n变量内存引用的值：%d\\n\", Age, Age, *Age) // 数据类型：*int // 变量中存储的值:824634916864 // 变量内存引用的值：32 } 空指针\n只声明未赋值的指针成为空指针,编码过程中避免出现空指针\npackage main import \"fmt\" func main() { // 只声明未赋值的指针成为空指针 var Age *int fmt.Print(Age) //nil fmt.Println(*Age) //\u003cnil\u003epanic: runtime error: invalid memory address or nil pointer dereference } 存放相同数据类型的集合，长度相对固定，有序\n声明语法：\nvar \u003cname\u003e [size]type var 关键字，表示声明一个变量 \u003cname\u003e 变量名称 [size]string 指定数据元素size,元素类型为 type等 示例\npackage main import \"fmt\" // 数值的特征： 存放相同类型，有序，定长 func main() { // 定义数组 var \u003cname\u003e [size]string var lang [3]string lang[0] = \"python\" lang[1] = \"go\" lang[2] = \"java\" fmt.Printf( \"数据类型：%T\\n存储的值：%s\\n数组长度%d\\n\", lang, lang[0], len(lang), ) // 定义并赋值 var lang1 = [3]string{\"python\", \"go\", \"java\"} fmt.Printf( \"数据类型：%T\\n存储的值：%s\\n数组长度%d\\n\", lang1, lang1[0], len(lang1), ) // 简短定义并赋值，只能在函数里使用 lang2 := [3]string{\"python\", \"go\"} fmt.Printf( \"数据类型：%T\\n存储的值：%s\\n数组长度%d\\n\", lang2, lang2, len(lang2), ) // 定义不定长数组 lang3 := [...]string{\"python\", \"go\", \"java\"} fmt.Printf( \"数据类型：%T\\n存储的值：%s\\n数组长度%d\\n\", lang3, lang3, len(lang3), ) for i := 0; i \u003c len(lang3); i++ { fmt.Printf(\"%d -- %s\\n\", i, lang3[i]) } } 指针类型的数组\n存放一种类型数据的指针\npackage main import \"fmt\" func main() { var lang [3]*string a := \"python\" lang[0] = \u0026a fmt.Printf(\"数据类型：%T\\n存储的值：%s\\n\", lang, *lang[0]) } // 数据类型：[3]*string // 存储的值：python 使用new 创建 数组指针\n定义了一个 数组类型 的指针，这一部分应该放在指针章节\npackage main import \"fmt\" func main() { var lang = new([20]int) lang[10] = 100 fmt.Printf(\"数据类型：%T\\n存储的值：%d\\n\", lang, *lang) } // root@master01:/home/pc/Desktop/test# go run . // 数据类型：*[20]int // 存储的值：[0 0 0 0 0 0 0 0 0 0 100 0 0 0 0 0 0 0 0 0] 多维数组\npackage main import \"fmt\" func main() { // 语法 var \u003cname\u003e [外层size][内层size]type // 多为数组 [[python php perl] [c go java]] var lang [2][3]string lang[0][0] = \"python\" lang[0][1] = \"php\" lang[0][2] = \"perl\" lang[1][0] = \"c\" lang[1][1] = \"go\" lang[1][2] = \"java\" fmt.Println(lang) for i := 0; i \u003c len(lang); i++ { for _, v := range lang[i] { fmt.Println(v) } } } 切片长度可变。又称为动态数组\n声明语法：\nvar \u003cname\u003e []type var 关键字，表示声明一个变量 \u003cname\u003e 变量名称 []type 指定元素数据类型 示例：\npackage main import \"fmt\" func main() { var lang []string lang = append(lang, \"python\") lang = append(lang, \"go\") fmt.Printf(\"变量类型：%T\\n切片元素数量: %d\\n切片容量: %d\\n变量内容：%s\\n\", lang, len(lang), cap(lang), lang) // 变量类型：[]string // 切片元素数量: 2 // 切片容量: 2 // 变量内容：[python go] } 使用make创建切片\npackage main import \"fmt\" func main() { // make(t Type, size ...IntegerType) ,3 切片长度，5 切片容量 var lang = make([]string, 3, 5) // 追加两个元素 lang = append(lang, \"python\", \"go\") fmt.Printf(\"数据类型：%T\\n切片长度:%d\\n切片容量：%d\\n变量内容：%s\\n最后一个元素: %s\\n\", lang, len(lang), cap(lang), lang, lang[len(lang)-1]) // 数据类型：[]string // // 切片长度:5 // 切片容量：5 // 变量内容：[ python go] // 最后一个元素: go for k, v := range lang { fmt.Println(k, \"--\u003e\", v) } // 0 --\u003e // 1 --\u003e // 2 --\u003e // 3 --\u003e python // 4 --\u003e go } 截取数组作为切片\npackage main import \"fmt\" func main() { var lang = [5]string{\"python\", \"go\", \"c\", \"java\", \"perl\"} var langSlice = lang[:] fmt.Printf(\"数据类型：%T\\n切片长度:%d\\n切片容量：%d\\n变量内容：%s\\n\", langSlice, len(langSlice), cap(langSlice), langSlice) // 数据类型：[]string // 切片长度:5 // 切片容量：5 // 变量内容：[python go c java perl] } append()函数\npackage main import \"fmt\" // append() 函数向切片追加元素 func main() { var lang = []string{\"python\", \"java\"} lang = append(lang, \"go\", \"c\") fmt.Println(lang) //[python java go c] // 追加一个切片 lang = append(lang, []string{\"sql\", \"bash\"}...) fmt.Println(lang) //[python java go c sql bash] // 像指定位置插入。此过程利用了append的嵌套 // append([]string{\"ruby\"}, lang[1:]...) 把lang[:1]切片内容添加到切片[]string{\"ruby\"} lang = append(lang[:1], append([]string{\"ruby\"}, lang[1:]...)...) fmt.Println(lang) //[python ruby java go c sql bash] } copy()函数\npackage main import \"fmt\" func main() { var lang = []string{\"python\", \"java\", \"go\"} langCopy := make([]string, len(lang)) // copy(目标切片，原切片) copy(langCopy, lang) fmt.Println(langCopy) //[python java go] } 存放相同数据类型元素，可变长，无序\n声明语法：\nvar \u003cname\u003e map[keytype]valuetype var 关键字，表示声明一个变量\n\u003cname\u003e 变量名称\nmap[keytype]valuetype map 为关键字 [keytype]表示key的数据类型 valuetype 表示value的数据类型\n示例：\npackage main import \"fmt\" func main() { // map 声明后并不能直接使用，声明时并没有分配内存地址。因此需要make执行初始化。 var lang map[string][]string lang = make(map[string][]string) lang[\"python\"] = []string{\"吉多·范罗苏姆\"} lang[\"go\"] = []string{\"罗伯特·格瑞史莫\", \"罗布·派克\", \"肯·汤普逊\"} fmt.Println(lang) // 打印：map[python:[吉多·范罗苏姆] go:[罗伯特·格瑞史莫 罗布·派克 肯·汤普逊]] } 声明并初始化\npackage main import \"fmt\" func main() { var lang = map[string][]string{ \"python\": []string{\"吉多·范罗苏姆\"}, \"go\": []string{\"罗伯特·格瑞史莫\", \"罗布·派克\", \"肯·汤普逊\"}, } fmt.Println(lang) //map[go:[罗伯特·格瑞史莫 罗布·派克 肯·汤普逊] python:[吉多·范罗苏姆]] } make()\npackage main import \"fmt\" func main() { // 集合，存放key value 键值对，类似python中的字典 var lang = make(map[string][]string) fmt.Printf(\"数据类型：%T\\n\", lang) // 数据类型：map[string][]string lang[\"python\"] = []string{\"吉多·范罗苏姆\"} lang[\"go\"] = []string{\"罗伯特·格瑞史莫\", \"罗布·派克\", \"肯·汤普逊\"} fmt.Println(lang) // 打印：map[python:[吉多·范罗苏姆] go:[罗伯特·格瑞史莫 罗布·派克 肯·汤普逊]] for k, v := range lang { fmt.Printf(\"%s \u003e\u003e\u003e %s\\n\", k, v) } // python \u003e\u003e\u003e [吉多·范罗苏姆] // go \u003e\u003e\u003e [罗伯特·格瑞史莫 罗布·派克 肯·汤普逊] } 判断key是否存在map中\npackage main import \"fmt\" func main() { var lang = map[string][]string{ \"python\": []string{\"吉多·范罗苏姆\"}, \"go\": []string{\"罗伯特·格瑞史莫\", \"罗布·派克\", \"肯·汤普逊\"}, } if value, keyExist := lang[\"c\"];keyExist{ //[] false fmt.Println(value) }else{ fmt.Println(\"不存在\") } } 声明语法：\ntype \u003cname\u003e struct { filed1 filedtype filed1 filedtype ... } type 关键字 \u003cname\u003e 结构体名称，首字符大写表示可以被其他包引用 struct表示声明结构体类型 示例：\npackage main import \"fmt\" func main() { // 约定： 首字符大写表示为可以导出的变量和字段 type Persion struct { Name string Age int8 Sex bool Favorit []string } var persion Persion persion.Name = \"张三\" persion.Age = 33 persion.Sex = true fmt.Printf(\"%+v\\n%v\\n\", persion, persion) // {Name:张三 Age:33 Sex:true Favorit:[]} // {张三 33 true []} } 示例声明一个alertmanager 告警规则的结构体\npackage main import ( \"encoding/json\" \"fmt\" ) // AlertStatus represents the status of an alert. type AlertStatus struct { Status string Labels map[string]string `json:\"labels\"` Annotations map[string]string `json:\"annotations\"` StartsAt string `json:\"startsAt\"` EndsAt string `json:\"endsAt\"` GeneratorURL string `json:\"generatorURL\"` Fingerprint string `json:\"fingerprint\"` } // Alert represents the structure of an alert. type Alert struct { Receiver string `json:\"receiver\"` Status string `json:\"status\"` Alerts []AlertStatus `json:\"alerts\"` GroupLabels map[string]string `json:\"groupLabels\"` CommonLabels map[string]string `json:\"commonLabels\"` CommonAnnotations map[string]string `json:\"commonAnnotations\"` ExternalURL string `json:\"externalURL\"` Version string `json:\"version\"` GroupKey string `json:\"groupKey\"` TruncatedAlerts int `json:\"truncatedAlerts\"` } func main() { // JSON content as a raw string. jsonData := `{ \"receiver\": \"webhook\", \"status\": \"firing\", \"alerts\": [ { \"status\": \"firing\", \"labels\": { \"alertname\": \"cpu3\", \"instance\": \"localhost:9100\", \"job\": \"node_exporter\", \"severity\": \"warning\" }, \"annotations\": { \"description\": \"localhost:9100 of job node_exporter has been used cpu \u003e0 more than 5 minutes. (current value: 44.99%)\", \"summary\": \"Instance localhost:9100 cpu usage high\" }, \"startsAt\": \"2024-04-21T09:36:36.26148042Z\", \"endsAt\": \"0001-01-01T00:00:00Z\", \"generatorURL\": \"\", \"fingerprint\": \"c4c0776a34942165\" } ], \"groupLabels\": { \"alertname\": \"cpu3\" }, \"commonLabels\": { \"alertname\": \"cpu3\", \"instance\": \"localhost:9100\", \"job\": \"node_exporter\", \"severity\": \"warning\" }, \"commonAnnotations\": { \"description\": \"localhost:9100 of job node_exporter has been used cpu \u003e0 more than 5 minutes. (current value: 44.99%)\", \"summary\": \"Instance localhost:9100 cpu usage high\" }, \"externalURL\": \"http://e7b40e5c9054:9093\", \"version\": \"4\", \"groupKey\": \"{}:{alertname=\\\"cpu3\\\"}\", \"truncatedAlerts\": 0 }` // Declare a variable of type Alert to hold the unmarshalled data. var alert Alert // Unmarshal the JSON data into the struct. err := json.Unmarshal([]byte(jsonData), \u0026alert) if err != nil { fmt.Println(\"Error unmarshalling JSON:\", err) return } // Print the unmarshalled data. fmt.Printf(\"%+v\\n\", alert) } ","categories":["golang"],"description":"go|数据类型\n","excerpt":"go|数据类型\n","ref":"/golang/type.html","tags":["golang","数据类型"],"title":"数据类型"},{"body":" 天气时钟看板 https://github.com/teojs/clock-dashboard\nnode v18.12.0\rpnpm 10.26.2 # corepack enable pnpm\rgit clone https://gitee.com/mingtian66/clock-dashboard.git\rcd clock-dashboard/\rpnpm install\rpnpm run build 全屏\r","categories":[""],"description":"\n\r\n\r\n\r\n\r\n\r\n","excerpt":"\n\r\n\r\n\r\n\r\n\r\n","ref":"/hugo/clock-dashboard.html","tags":["时钟和天气"],"title":"时钟和天气"},{"body":"运算符 算术运算符中 % 仅仅使用于整数类型，其他符号可用于整数、浮点数、复数的运算\n算术运算 + - * / % ++ -- 赋值运算 = += -= *= /= %= 比较运算 == != \u003c \u003c= \u003e \u003e= 逻辑运算 \u0026\u0026 || ! 位运算 \u0026 | ^ \u003c\u003c \u003e\u003e 指针运算符 \u0026 * 幂运算需要使用math包下的 pow()函数\n算术运算符\npackage main // 假设一个收银系统 import \"fmt\" func main() { // 重量和单价 weigh, price := 2.18, 1.99 // 称重去皮 weigh -= 0.1 // 总价 total := weigh * price fmt.Printf(\"应付金额：%2.1f\\n\", total) } ","categories":["golang"],"description":"go|运算符\n","excerpt":"go|运算符\n","ref":"/golang/operator.html","tags":["golang","运算符"],"title":"运算符"},{"body":" 图床工具 https://github.com/chaos-zhu/easyimg\ndocker run -d --name easyimg -p 3000:3000 -v ./db:/app/db -v ./uploads:/app/uploads ghcr.io/chaos-zhu/easyimg:latest ","categories":["easyimg"],"description":"\n\r\n\r\n\r\n\r\n\r\n","excerpt":"\n\r\n\r\n\r\n\r\n\r\n","ref":"/hugo/easyimg.html","tags":["图床工具"],"title":"图床工具"},{"body":" kubernetes版本: 1.23.17\npython版本：3.6+\n安装：pip install kubernetes==23.6.0 文档：https://github.com/kubernetes-client/python https://kopf.readthedocs.io/en/stable/index.html\n​\n","categories":["python"],"description":"kopf是一个kubernetes的operator框架，用于开发自定义的controller。\n","excerpt":"kopf是一个kubernetes的operator框架，用于开发自定义的controller。\n","ref":"/python/package/kopf.html","tags":["kubernetes","operator"],"title":"kopf"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\njdk版本：openjdk 11\nzookeeper版本: 3.4.14\nkafka版本：2.13-3.3.2 龙腾出行生产环境使用2.11-2.2.0\n2011年初，美国领英公司（LinkedIn）开源了一款基础架构软件，以奥地利作家弗兰兹·卡夫卡（Franz Kafka）的名字命名，之后 LinkedIn将其贡献给 Apache基金会，随后该软件于2012年10月完成孵化并顺利晋升为Apache顶级项目——这便是大名鼎鼎的Apache Kafka\n用户 创建用户\n# 创建用户test 密码123456 加密方式SCRAM-SHA-256 [root@seagullcore01-uat-s2 kafka_2.13-3.3.2]# bin/kafka-configs.sh \\ --bootstrap-server 192.168.0.161:9092 \\ --alter \\ --add-config 'SCRAM-SHA-256=[password=123456]' \\ --entity-type users \\ --entity-name test Completed updating config for user test. # 查看用户 [root@seagullcore01-uat-s2 kafka_2.13-3.3.2]# bin/kafka-configs.sh \\ --bootstrap-server 192.168.0.161:9092 \\ --describe \\ -entity-type users \\ --entity-name test SCRAM credential configs for user-principal 'test' are SCRAM-SHA-256=iterations=4096 修改kafka配置文件并重启服务\ncat \u003e\u003econfig/server.properties \u003c\u003c'EOF' # 添加以下配置 sasl.enabled.mechanisms=SCRAM-SHA-256 # 如果已有 SSL，可写 SASL_SSL security.inter.broker.protocol=SASL_PLAINTEXT sasl.mechanism.inter.broker.protocol=SCRAM-SHA-256 listeners=SASL_PLAINTEXT://192.168.0.161:9092 advertised.listeners=SASL_PLAINTEXT://192.168.0.161:9092 # 用于集群成员间通讯 listener.name.sasl_plaintext.scram-sha-256.sasl.jaas.config=\\ org.apache.kafka.common.security.scram.ScramLoginModule required \\ username=\"test\" \\ password=\"123456\"; EOF cat \u003econfig/kafka_server_jaas.conf \u003c\u003c'EOF' KafkaServer { org.apache.kafka.common.security.scram.ScramLoginModule required username=\"test\" password=\"123456\"; }; EOF export KAFKA_OPTS=\"-Djava.security.auth.login.config=config/kafka_server_jaas.conf\" ./bin/kafka-server-start.sh config/server.properties 修改filebeat配置启动密码认证\nfilebeat.inputs: - type: filestream enabled: true paths: - /var/log/messages output.kafka: enabled: true hosts: [\"192.168.0.161:9092\"] username: test password: '123456' sasl.mechanism: 'SCRAM-SHA-256' topic: 'foobar' required_acks: 1 compression: gzip # 1MB max_message_bytes: 1000000 acl鉴权 ","categories":["kafka"],"description":"\n*Kafka认证和授权*\r\n\r\n","excerpt":"\n*Kafka认证和授权*\r\n\r\n","ref":"/database/kafka/authtication.html","tags":["认证授权"],"title":"Kafka认证和授权"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes/serviceless/","tags":"","title":"serviceless"},{"body":" Tested on Redis 5.0, Redis 6.0 and Redis 7.0 参数\n变量名 可选值（举例） 备注 redisSourceVersion such as 2.8, 4.0, 5.0, 6.0, 6.2, 7.0, … 浮点类型，赋值不需要加引号 sourceAddress 10.1.1.1:20331 字符串类型，赋值需要加引号。集群中任意节点 sourcePassword 字符串类型，赋值需要加引号 targetType “standalone” or “cluster” 字符串类型，赋值需要加引号 redisTargetVersion such as 2.8, 4.0, 5.0, 6.0, 6.2, 7.0, … 浮点类型，赋值不需要加引号 targetAddress 10.1.1.1:20331 如果是集群模式，只写其中一个node,其他node信息redis-shake 通过 cluster nodes获取 targetPassword 字符串类型，赋值需要加引号 keyExists rewrite panic ignore 字符串类型，赋值需要加引号 prefixfilter true false 确定前缀后后缀匹配 filterKey {“ABC”,“abc”,“def”,“def_”} blackfilter true false 确定是黑名单还是白名单过滤 构建镜像 Details 启动脚本entrypoint.sh\n#!/bin/sh set -ex config(){ cat \u003e/etc/sync.toml \u003c\u003cEOF type = \"sync\" [source] version = ${redisSourceVersion} address = ${sourceAddress} username = \"\" # keep empty if not using ACL password = ${sourcePassword} tls = false [target] type = ${targetType} version = ${redisTargetVersion} address = ${targetAddress} username = \"\" # keep empty if not using ACL password = ${targetPassword} tls = false [advanced] dir = \"data\" # runtime.GOMAXPROCS, 0 means use runtime.NumCPU() cpu cores ncpu = 4 pprof_port = 9310 metrics_port = 9320 rdb_restore_command_behavior = ${keyExists:-\"ignore\"} pipeline_count_limit = 1024 target_redis_client_max_querybuf_len = 1024_000_000 target_redis_proto_max_bulk_len = 512_000_000 EOF #array = {\"ABC\",\"abc\",\"def\",\"def_\"} array=${filterKey:-\"{}\"} if ${prefixfilter};then prefix='\"^\"' suffix='\"\"' else prefix='\"\"' suffix=\"$\" fi if ${blackfilter};then allow=1 disallow=0 else allow=0 disallow=1 fi cat \u003e/etc/filter.lua\u003c\u003cEOF function filter(id, is_base, group, cmd_name, keys, slots, db_id, timestamp_ms) -- 对mset多key的判断 if #keys ~= 1 then for _k,key in ipairs(keys) do for k,v in ipairs(${array}) do -- if string.match(key,string.format(\"^%s\",v)) then if string.match(key,string.format(\"%s%s%s\",${prefix:-\"\"},v,${suffix:-\"\"})) then -- print(key,v) return ${allow:-0}, db_id -- 0 is allow end end end return ${disallow:-0}, db_id --1 is disallow end for k,v in ipairs(${array}) do -- if string.sub(keys[1], -3, -1) == v then if string.match(keys[1],string.format(\"%s%s%s\",${prefix},v,${suffix})) then return ${allow:-0}, db_id -- allow end end return ${disallow:-0}, db_id -- disallow end EOF } config exec \"$@\" Details Dockerfile.yml\nFROM alpine:3.13 ARG version=3.1.11 RUN sed -i s#dl-cdn.alpinelinux.org#mirrors.tuna.tsinghua.edu.cn#g /etc/apk/repositories \u0026\u0026 \\ apk update \u0026\u0026 \\ apk add vim curl python3 \u0026\u0026 \\ alias python=\"python3\" \u0026\u0026 \\ rm -rf /var/cache/apk/* #wget https://github.com/tair-opensource/RedisShake/releases/download/v3.1.11/redis-shake-linux-amd64.tar.gz COPY entrypoint.sh / COPY redis-shake /usr/bin/redis-shake EXPOSE 9320 ENTRYPOINT [\"/entrypoint.sh\"] 构建镜像\nchmod +x entrypoint.sh docker build . -t redis-shake:3.1.11-alpine 迁移示例 docker环境测试变量解析\ntee env\u003c\u003cEOF redisSourceVersion=5.0 sourceAddress=\"127.0.0.1:6379\" sourcePassword=\"123\" redisTargetVersion=5.0 targetType=\"cluster\" targetAddress=\"127.0.0.1:6479\" targetPassword=\"456\" keyExists=\"rewrite\" prefixfilter=true filterKey={\"ABC\",\"abc\",\"def\",\"def_\"} blackfilter=false EOF docker run --rm -it --env-file env redis-shake:3.1.11-alpine sh docker run --rm -it --env-file env redis-shake:3.1.11-alpine redis-shake /etc/sync.toml docker run --rm -it --env-file env redis-shake:3.1.11-alpine redis-shake /etc/sync.toml /etc/filter.lua docker exec -it 36e54 curl 127.0.0.1:9320/metrics|python -m json.tool 指标监控：\nsyncing aof. allowOps=[0.60], disallowOps=[0.00], entryId=[2], InQueueEntriesCount=[0], unansweredBytesCount=[0]bytes, diff=[0], aofReceivedOffset=[9808], aofAppliedOffset=[9808] allowOps：代表每秒向目的端发送多少条命令 disallowOps：代表每秒有多少条命令被过滤掉 entryId：从 1 开始计数，代表 redis-shake 从启动开始总共处理多少条命令 InQueueEntriesCount：代表多少条命令待发送 unansweredBytesCount：代表多少 bytes 命令已经发送了，但是对端还没有答复 diff：aofReceivedOffset-aofAppliedOffset aofReceivedOffset：已经从源端接收到的点位 aofAppliedOffset：已经在目的端恢复的点位 一般当 allowOps 为 0 时，代表数据迁移完成，可以停止 redis-shake。⚠️注意，因为源端会定时发送 PING 命令，所以哪怕源端没有写入，allowOps 偶尔也会不是 0。 资源压力测试\n源端（版本5.0） 目标端（版本5.0） 数据量 8分片 3分片 12GB 测试结果\n单pod 消耗 220% cpu 单pod 消耗 12MB memory 全量同步耗时 6分钟 ","categories":["redis"],"description":"\nredis-shake3.1.11 docker镜像构建\r\n","excerpt":"\nredis-shake3.1.11 docker镜像构建\r\n","ref":"/redis/redis-shake3.1.11.html","tags":["迁移案例"],"title":"redis-shake3.1.11"},{"body":" 主机名 IP server_id 角色 版本 设置 mysqldbphw01 172.32.4.10/24 1 master/MHA Node 5.6.40 1.设置server-id\n2.开启binlog\n3.配置vip 172.32.4.50/24 mysqldbphw02 172.32.4.20/24 2 slave/MHA Node 5.6.40 1.设置server-id\n2.设置为只读 mysqldbphw03 172.32.4.30/24 3 slave/MHA Node 5.6.40 1.设置server-id2.设置为只读 mysqldbphw04 172.32.4.40/24 MHA Manager 添加一个vip 用于应用接入 设置ssh免密要认证，用于failover时执行网络、binlog拷贝等动作 ifconfig eth0:2 172.32.4.50/24 ssh-keygen -t rsa -f /root/.ssh/id_rsa -N \"\" 2.5测试 主库手工绑定vip：ifconfig eth0:1 168.204.37.17/24 up masterha_check_ssh --conf=/etc/mha/app1.cnf masterha_check_repl --conf=/etc/mha/app1.cnf 2.6启动 [root@mysqldbphw04 /]# nohup masterha_manager --conf=/etc/mha/app1.cnf \u003e /mha/manager.log \u003c /dev/null \u0026 [1] 933 查看运行状态 [root@mysqldbphw04 /]# masterha_check_status --conf=/etc/mha/app1.cnf app1 (pid:933) is running(0:PING_OK), master:mysqldbphw01 2.7关闭 masterha_sotp --conf=/etc/mha/app1.cnf # [root@mysqldbphw04 /]# masterha_secondary_check -s mysqldbphw01 -s mysqldbp hw02 -s mysqldbphw03 --user=root --master_host=mysqldbphw01 --master_ip=172 .32.4.10 --master_port=3306 Master is reachable from mysqldbphw01! MYSQL高可用架构MGR、MHA对比 - 小雨淅淅o0 - 博客园 (cnblogs.com)\n","categories":["mysql"],"description":"mha是使用ruby语言实现的高可用方案\n","excerpt":"mha是使用ruby语言实现的高可用方案\n","ref":"/mysql/mha.html","tags":["mha","ha"],"title":"mha"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\nfilebeat版本：7.17.29\nelasticsearch版本： 7.15.0\ngraph LR\rsubgraph \"【主配置文件】\"\rA\u003efilebeat.yml]\rend\rsubgraph output\rB[output] end\rA -..-\u003e|\"日志事件发送到的位置[\u003cfont color=red\u003e必须\u003c/font\u003e]\"|B\rB --\u003e Elasticsearch\rB --\u003e Kafka\rB --\u003e Logstash elasticsearch output.elasticsearch: enabled: true hosts: [\"192.168.0.161:9200\"] indices: - index: \"os-linux-warning-%{[agent.version]}-%{+yyyy.MM.dd}\" when: contains: message: \"WARN\" - index: \"os-linux-error-%{[agent.version]}-%{+yyyy.MM.dd}\" when: contains: message: \"ERR\" - index: \"os-linux-info-%{[agent.version]}-%{+yyyy.MM.dd}\" when: contains: message: \"INFO\" # 默认索引 - index: \"os-linux-%{[agent.version]}-%{+yyyy.MM.dd}\" # 启用ssl 和密码认证后的语法 #protocol: \"https\" #username: \"kibanauser\" #password: \"kibana\" #ssl.verification_mode: none # 禁用索引生命周期管理，默认为true。在启用 setup.ilm.enabled 状态下setup.template 配置不会生效 # https://www.elastic.co/guide/en/beats/filebeat/7.17/configuration-template.html setup.ilm.enabled: false # 加载索引模版，默认为true setup.template.enabled: true # 设置索引模板的名称，默认值为 filebeat-%{[agent.version]} setup.template.name: \"os-linux\" # 设置索引模板的匹配模式,默认值为 filebeat-%{[agent.version]}-* setup.template.pattern: \"os-linux-*\" # 覆盖已有的索引模板，默认值为false setup.template.overwrite: true # 配置索引模板属性 setup.template.settings: # 设置索引分片数量 index.number_of_shards: 3 # 设置索引副本数量，要求小于集群的数量 index.number_of_replicas: 0 Kafka 提醒 For Kafka version 0.10.0.0+ the message creation timestamp is set by beats and equals to the initial timestamp of the event. This affects the retention policy in Kafka: for example, if a beat event was created 2 weeks ago, the retention policy is set to 7 days and the message from beats arrives to Kafka today, it’s going to be immediately discarded since the timestamp value is before the last 7 days. It’s possible to change this behavior by setting timestamps on message arrival instead, so the message is not discarded but kept for 7 more days. To do that, please set log.message.timestamp.type to LogAppendTime (default CreateTime) in the Kafka configuration.\nBeats 等客户端带上了事件原始时间（CreateTime），而 Kafka 按这个时间做 retention，导致“老”事件一进来就被丢弃。\n# kafka 修改配置 log.message.timestamp.type=LogAppendTime 输出到到kafka: 格式样例一： 格式样例二： output.kafka: enabled: true hosts: [\"kafka1:9092\",\"kafka2:9092\",\"kafka3:9092\"] #username: #password: topic: \"logs-%{[agent.version]}\" topics: - topic: \"critical-%{[agent.version]}\" when.contains: message: \"CRITICAL\" - topic: \"error-%{[agent.version]}\" when.contains: message: \"ERR\" output.kafka: enabled: true hosts: [\"kafka1:9092\",\"kafka2:9092\",\"kafka3:9092\"] #username: #password: topic: '%{[fields.log_topic]}' required_acks: 1 compression: gzip # 1MB max_message_bytes: 1000000 ","categories":["ELK"],"description":"\nfilebeat配置文件之output\r\n","excerpt":"\nfilebeat配置文件之output\r\n","ref":"/elk/filebeat/config/output.html","tags":["output","filebeat"],"title":"filebeat_output"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\nfilebeat版本：7.17.29\nelasticsearch版本： 7.15.0\nprocessors 用于执行output前过滤和丰富日志流，多个processor 按照定义顺序执行。\ngraph LR\rsubgraph \"【主配置文件】\"\rA\u003efilebeat.yml]\rend\rsubgraph input\rB[input] end\rA -..-\u003eB --\u003eB1[processors]\rA -..-\u003eprocessors 语法格式: 格式1 格式2 processors: - \u003cprocessor_name\u003e: # 指定由哪个processor执行动作 when: \u003ccondition\u003e # 定义条件 \u003cparameters\u003e # 是要传递给processor的参数列表。 - \u003cprocessor_name\u003e: when: \u003ccondition\u003e \u003cparameters\u003e processors: - if: \u003ccondition\u003e then: - \u003cprocessor_name\u003e: \u003cparameters\u003e - \u003cprocessor_name\u003e: \u003cparameters\u003e ... else: - \u003cprocessor_name\u003e: \u003cparameters\u003e - \u003cprocessor_name\u003e: \u003cparameters\u003e 参照语法格式，我将该配置的学习分为两个部分。即条件表达式 和 可执行动作\n条件表达式（condition） equals\n示例：\nfilebeat.inputs: - type: filestream enabled: true paths: - /var/log/*.log processors: - drop_event: when: equals: status: \"firing\" output.console: pretty: true contains\n示例：\nfilebeat.inputs: - type: filestream enabled: true paths: - /var/log/*.log processors: - drop_event: when: contains: status: \"firing\" output.console: pretty: true regexp\n示例：\nfilebeat.inputs: - type: filestream enabled: true paths: - /var/log/*.log processors: - drop_event: when: regexp: status: \"^firing\" output.console: pretty: true range\n支持通过lt(小于)、 gt(大于)、 lte(小于等于)、 gte(大于等于)符号判断\n示例：\nprocessors: - drop_event: when: range: http.response.code: lte: 400 # 200\u003c= http.response.code \u003c=400 processors: - drop_event: when: range: http.response.code.lte: 400 http.response.code.gte: 200 network\n用于判断ip是否在指定的网络范围,有效取值包括：\nloopback 代表回环地址127.0.0.0/8 、::1/128 private ipv4定义的私有地址 使用 CIDR 表示法 示例：\nprocessors: - drop_event: when: network: destination.ip: ['192.168.1.0/24', '10.0.0.0/8', loopback] has_fields\n示例：\nprocessors: - drop_event: when: has_fields: [\"kubernetes.labels.filebeat\"] or 和 and\n示例：\nprocessors: - drop_event: when: or: - has_fields: [\"kubernetes.labels.filebeat\"] - network: destination.ip: ['192.168.1.0/24', '10.0.0.0/8', loopback] not\n示例：\nprocessors: - drop_event: when: not: has_fields: [\"kubernetes.labels.filebeat\"] 可执行动作（processor_name） drop_event丢弃整个事件（通常与条件判断结合）\nprocessors: # 丢弃开头为^DEBUG的日志 - drop_event: when: regexp: message: \"^DEBUG\" drop_fields\nprocessors: # 丢弃开头为^DEBUG的日志 - drop_event: when: regexp: message: \"^DEBUG\" - drop_fields: when: has_fields: [\"kubernetes.labels.filebeat\"] ignore_missing: true # 当缺失签时忽略错误 rename 重命名字段\nprocessors: # 丢弃开头为^DEBUG的日志 - drop_event: when: regexp: message: \"^DEBUG\" - drop_fields: when: has_fields: [\"kubernetes.labels.filebeat\"] ignore_missing: true # 当缺失签时忽略错误 - rename: ignore_missing: false fail_on_error: true fields: - from: \"msg\" to: \"message\" replace\n替换值，例如把字段message的值改为INFO\nprocessors: # 丢弃开头为^DEBUG的日志 - drop_event: when: regexp: message: \"^DEBUG\" - drop_fields: when: has_fields: [\"kubernetes.labels.filebeat\"] ignore_missing: true # 当缺失签时忽略错误 - rename: ignore_missing: false fail_on_error: true fields: - from: \"msg\" to: \"message\" - replace: fields: - field: \"message\" pattern: \"^(I|i|info|INFO)\" replacement: \"INFO\" ignore_missing: false fail_on_error: true add_labels: 配置 日志效果 processors:\r- add_labels:\rlabels:\rnumber: 1\rnested:\rwith.dots: nested\rarray:\r- do\r- re\r- with.fields: mi \"labels\": {\r\"array.0\": \"do\",\r\"array.1\": \"re\",\r\"array.2.with.fields\": \"mi\",\r\"number\": \"1\",\r\"nested.with.dots\": \"nested\"\r}, add_tags: 配置 日志效果 processors:\r- add_tags:\rtarget: env # 可选，如果以己经存在则覆盖\rtags: [web,pro] \"env\": [\r\"web\",\r\"pro\"\r], add_fields: 配置 日志效果 processors:\r- add_fields:\rtarget: host_info # 可选，如果以己经存在则覆盖\rfields:\rzone: BeiJing\rcenter: BJ-1 \"host_info\": {\r\"center\": \"BJ-1\",\r\"zone\": \"BeiJing\"\r}, add_host_metadata 配置 日志效果 processors:\r- add_host_metadata: netinfo.enabled: false # 默认为true false不收集ip和mac\r\"host\": {\r\"hostname\": \"seagullcore01-uat-s2\",\r\"os\": {\r\"version\": \"7 (Core)\",\r\"family\": \"redhat\",\r\"codename\": \"Core\",\r\"type\": \"linux\",\r\"platform\": \"centos\",\r\"kernel\": \"3.10.0-957.el7.x86_64\",\r\"name\": \"CentOS Linux\"\r},\r\"architecture\": \"x86_64\",\r\"containerized\": false,\r\"name\": \"seagullcore01-uat-s2\",\r\"id\": \"e81d9b925a3e41e7a01e9fe553b68d6c\"\r}, add_kubernetes_metadata\ndecode_json_fields\nprocessors: - decode_json_fields: fields: [\"field1\", \"field2\", ...] # 对那些字段执行解析 process_array: false #是否对数组执行解析 max_depth: 1 target: \"\" #解析后的结果在哪个字段下 overwrite_keys: false add_error_key: true #解析失败时将错误信息添加到解析结果中 dissect / grok: 非常强大的处理器，用于解析非结构化的日志文本，并将其结构化为多个字段。\nDissect：使用简单的分隔符匹配，性能极高。\n日志格式\n# 日志示例：2023-10-27 12:01:45 [ERROR] Service payment failed. processors: - dissect: tokenizer: \"%{+YYYY-MM-dd} %{+HH:mm:ss} [%{log.level}] Service %{service.name} %{service.status}\" field: \"message\" target_prefix: \"\" Grok：使用复杂的正则表达式模式，功能更强大但更耗 CPU。\nscript 支持执行javascript 脚本，从而允许用户自定义处理动作\n","categories":["ELK"],"description":"\n\r\n","excerpt":"\n\r\n","ref":"/elk/filebeat/config/processors.html","tags":["processors","filebeat"],"title":"filebeat_processors"},{"body":" os 版本： CentOS Linux release 7.9.2009 (Core)\nelasticsearch版本： 7.15.0、7.17.27\n概念 Elasticsearch 是一个分布式文档存储，使用一种全文搜索的倒排索引，倒排索引列出任何文档中出现的每个唯一单词，并标识所有 每个单词都出现在文档中。\n索引可以被认为是文档的集合，每个文档 document 是字段的集合\n默认情况下，Elasticsearch 会索引每个字段和每个索引中的所有数据字段具有专用的、优化的数据结构。例如，文本字段是存储在倒排索引中，数字和地理字段存储在 BKD 树中。\nLucene 是 Apache 软件基金会下的一个「纯 Java 编写的开源全文检索引擎库」。 它本身不是一套完整的搜索服务器，而是一组「jar 包」：你的应用把 Lucene 的代码直接引入，就能在 JVM 进程里完成倒排索引的构建、分词、打分、查询、高亮、拼写检查等全部搜索相关功能。换句话说，Lucene 是「搜索引擎的内核」。\nElasticsearch 是对Lucene 的封装,在Lucene功能的基础上实现了数据分片、副本、REST API、聚合、SQL、安全、监控等。\nIndex（索引）物理上就是一组 Segment 文件，内部采用「倒排索引」结构，保证关键词 → 文档列表的快速映射。\nDocument（文档）一条可被搜索的业务数据，由多个 Field 组成，等价于 ES 里的一条 JSON 记录。\nindex操作 创建索引alerts\n# curl -X PUT 127.0.0.1:9200/alerts curl -X PUT 127.0.0.1:9200/alerts \\ -H \"Content-Type: application/json\" \\ -d' { \"mappings\": { \"properties\": { \"name\": { \"type\": \"keyword\" }, \"value\": { \"type\": \"integer\" }, \"describe\": { \"type\": \"text\", // 支持分词检索 \"fields\": { \"keyword\": { // 保留原始值，用于聚合/排序/精确查询 \"type\": \"keyword\", \"ignore_above\": 256 // 长度超限就忽略，防止膨胀 } } } } } }' 列出所有索引\ncurl -X GET 127.0.0.1:9200/_cat/indices 列出索引alerts\ncurl -X GET 127.0.0.1:9200/alerts?pretty 删除索引alerts\ncurl -X DELETE 127.0.0.1:9200/alerts document操作 写入一条数据\n格式： 127.0.0.1:9200/{索引名称}/{文档类型}/{文档id}\ncurl -X POST 127.0.0.1:9200/alerts/_doc/1001 \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"MemHigh\", \"value\": \"80\", \"describe\": \"instance [node-1] memory high,please recheck\" }' 写入多条数据\ncurl -X POST 127.0.0.1:9200/alerts/_bulk \\ -H \"Content-Type: application/x-ndjson\" \\ -d' { \"index\" : { \"_index\" : \"alerts\", \"_id\" : \"1002\" } } { \"name\" : \"MemHigh\", \"value\" : 85 ,\"describe\" : \"instance [node-2] memory high,please recheck[cpu/diskio/memory]\"} { \"index\" : { \"_index\" : \"alerts\", \"_id\" : \"1003\" } } { \"name\" : \"CpuHigh\", \"value\" : 95 ,\"describe\" : \"instance [node-3] cpu high,please recheck[cpu/diskio/memory]\"} ' # ← 这里隐含 \\n，确保最后有空行 查看数据\ncurl 127.0.0.1:9200/alerts/_search?pretty curl 127.0.0.1:9200/alerts/_doc/1001?pretty 查找所有文档 分页显示和排序 仅展示指定字段 范围查找 关键字查找和高亮显示 全文匹配 聚合查询 curl 127.0.0.1:9200/alerts/_search?pretty \\ -H \"Content-Type: application/json\" \\ -d '{ \"query\": { \"match_all\": {} } }' curl 127.0.0.1:9200/alerts/_search?pretty \\ -H \"Content-Type: application/json\" \\ -d '{ \"query\": { \"match_all\": {} }, \"from\": 0, \"size\": 2 }' curl 127.0.0.1:9200/alerts/_search?pretty \\ -H \"Content-Type: application/json\" \\ -d '{ \"query\": { \"match_all\": {} }, \"from\": 0, \"size\": 2, \"sort\": { \"value\": {\"order\":\"desc\"} } }' curl 127.0.0.1:9200/alerts/_search?pretty \\ -H \"Content-Type: application/json\" \\ -d '{ \"query\": { \"match_all\": {} }, \"from\": 0, \"size\": 2, \"sort\": { \"value\": {\"order\":\"desc\"} }, \"_source\": [\"name\"] }' curl 127.0.0.1:9200/alerts/_search?pretty \\ -H \"Content-Type: application/json\" \\ -d '{ \"query\": { \"range\": { \"value\": { \"gte\": 85, \"lt\": 100 } } } }' # 查找 name=\"MemHigh\" curl 127.0.0.1:9200/alerts/_search?pretty \\ -H \"Content-Type: application/json\" \\ -d '{ \"query\": { \"match\": { \"name\": \"MemHigh\" } } }' # 多条件查找，name=\"MemHigh\" and value=85 curl 127.0.0.1:9200/alerts/_search?pretty \\ -H \"Content-Type: application/json\" \\ -d ' { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"name\": \"MemHigh\" } }, { \"match\": { \"value\": \"85\" } } ] } } }' # 高亮显示name字段值。适用于keyword类型字段 curl 127.0.0.1:9200/alerts/_search?pretty \\ -H \"Content-Type: application/json\" \\ -d ' { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"name\": \"MemHigh\" } }, { \"match\": { \"value\": \"85\" } } ] } }, \"highlight\": { \"fields\": { \"name\": {} } } }' # match 分词后不考虑`cpu` `High` 分词的顺序只要出现过的都会匹配到 curl 127.0.0.1:9200/alerts/_search?pretty \\ -H \"Content-Type: application/json\" \\ -d '{ \"query\": { \"match\": { \"describe\":\"cpu High\" } } }' # match_phrase 分词后考虑`cpu` `High` 分词的顺序，只有完全符合的会匹配到 curl 127.0.0.1:9200/alerts/_search?pretty \\ -H \"Content-Type: application/json\" \\ -d '{ \"query\": { \"match_phrase\": { \"describe\":\"cpu High\" } } }' 文档参考\n# 按照name 聚合计数 curl 127.0.0.1:9200/alerts/_search?pretty \\ -H \"Content-Type: application/json\" \\ -d '{ \"aggs\": { \"count_high\": { \"terms\": { \"field\": \"name\" } } } }' # 求内存告警中最大值,类推 max / min / avg / sum curl 127.0.0.1:9200/alerts/_search?pretty \\ -H \"Content-Type: application/json\" \\ -d '{ \"query\": { \"match\": { \"name\":\"MemHigh\" } }, \"aggs\": { \"mem_max\": { \"max\": { \"field\": \"value\" } } }, \"_source\": false // 不显示原始数据 }' 修改数据\n全量覆盖1001 更新其中某些字段 curl -X PUT 127.0.0.1:9200/alerts/_doc/1001 \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"MemHigh\", \"value\": \"80\" }' curl -X POST 127.0.0.1:9200/alerts/_update/1001 \\ -H \"Content-Type: application/json\" \\ -d' { \"doc\": { \"value\": \"88\" } }' ","categories":["ELK"],"description":"\nelasticsearch支持restfull 方式的CRUD\r\n","excerpt":"\nelasticsearch支持restfull 方式的CRUD\r\n","ref":"/elasticsearch/RestfullAPI.html","tags":["elasticsearch"],"title":"RestfullAPI"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/database/postgres/","tags":"","title":""},{"body":"\n按照uri 执行路由\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: flask namespace: default spec: rules: - host: test.com http: paths: - path: \"/\" backend: serviceName: flask servicePort: 80 - path: \"/index\" backend: serviceName: index servicePort: 80 按照servicename执行路由\napiVersion: extensions/v1beata1 kind: Ingress metadata: name: flask namespace: default spec: rules: - host: test.com http: paths: - path: \"/\" backend: serviceName: flask servicePort: 80 - host: pro.com http: paths: - path: \"/\" backend: serviceName: index servicePort: 80 ","categories":["kubernetes"],"description":"ingress|k8s\n","excerpt":"ingress|k8s\n","ref":"/kubernetes/ingress.html","tags":["ingress"],"title":"Ingress"},{"body":"\n# 查看集群列表 tiup cluster list # 查看集群每个组件的运行状态 tiup cluster display \u003ccluster-name\u003e # 启动集群 tiup cluster start \u003ccluster-name\u003e # -R 启动指定组件 tiup cluster start \u003ccluster-name\u003e -R pd # -N 启动指定进程 tiup cluster start ${cluster-name} -N 1.2.3.4:2379,1.2.3.5:2379 # show-config 查看当前配置 tiup cluster show-config cluster01 # edit-config 在线编辑配置 tiup cluster edit-config cluster01 # 查看tiup仓库 tiup mirror show # 默认读取变量 ${TIUP_MIRRORS} 的值 tiup mirror set # tiup mirror merge # 查看仓库中可用的包 , 查看已安装包 tiup list --installed tiup list https://docs.pingcap.com/zh/tidb/stable/maintain-tidb-using-tiup/\n","categories":["tidb"],"description":"tiup|tidb\n","excerpt":"tiup|tidb\n","ref":"/tidb/03tiup.html","tags":["tiup工具"],"title":"tiup工具"},{"body":"","categories":"","description":"\ndevops 文档中心\r\n","excerpt":"\ndevops 文档中心\r\n","ref":"/docs/devops/","tags":"","title":""},{"body":"","categories":"","description":"\ngitlab 文档中心\r\n","excerpt":"\ngitlab 文档中心\r\n","ref":"/docs/devops/gitlab/","tags":"","title":""},{"body":"","categories":"","description":"\nredmine 文档中心\r\n","excerpt":"\nredmine 文档中心\r\n","ref":"/docs/devops/redmine/","tags":"","title":""},{"body":"","categories":"","description":"\nterraform 文档中心\r\n","excerpt":"\nterraform 文档中心\r\n","ref":"/docs/devops/terraform/","tags":"","title":""},{"body":"术语：\n1️⃣物理卷PV(Physical Volume): 通过 pvcreate 创建的物理设备（如磁盘或分区），是LVM的基础存储单元。\n2️⃣物理块PE(Physical Extent) LVM管理的最小存储单元，默认大小为4MB\n3️⃣卷组VG(Volume Group) 由多个PV组合而成，构成逻辑存储池。\n4️⃣逻辑卷LV(Logical Volume) 从VG中划分的逻辑分区，可挂载使用。\n环境准备 创建pv\nroot@lvm:~# pvcreate /dev/sd{c..g} Physical volume \"/dev/sdc\" successfully created. Physical volume \"/dev/sdd\" successfully created. Physical volume \"/dev/sde\" successfully created. Physical volume \"/dev/sdf\" successfully created. Physical volume \"/dev/sdg\" successfully created. 创建vg\nroot@lvm:~# vgcreate data /dev/sd{c..f} Volume group \"data\" successfully created 逻辑卷类型 逻辑卷类型: 线性卷 条带化卷 Raid卷 精简卷 lvcreate --name test-nomarl --size 1g data root@pc:~# lvs --select lv_name=\"test-nomarl\" -o lv_name,vg_name,lv_size -o +seg_type LV VG LSize Type test-nomarl data 1.00g linear –stripes 条带数量，不超过pv数量\n–stripesize 条带大小，超过该值的大小保存到下一个pv上\nlvcreate --name test-stripes --stripes 4 --stripesize 64kb --size 1g data root@lvm:~# lvs --select lv_name=\"test-stripes\" -o lv_name,vg_name,lv_size -o +seg_type LV VG LSize Type test-stripes data 1.00g striped 包括 RAID0、RAID1、RAID4、RAID5、RAID6 和 RAID10\nRAID 4 的校验集中在一个磁盘，RAID 5 和 6 的校验分散在所有磁盘。 RAID 6 有两份独立的校验（如 P+Q 或 Reed-Solomon 编码），可承受双磁盘故障。 RAID级别 校验方式 写入性能特点 容错能力 最小磁盘数 RAID 4 专用校验盘（固定某个磁盘） 写入瓶颈：所有校验数据写入专用盘，高负载时易成为性能瓶颈。 允许 1块磁盘故障 3块 RAID 5 分布式校验（校验块轮流分布） 较高并发：校验分散，支持并行读写，但每次写操作需计算并更新校验块（读-改-写）。 允许 1块磁盘故障 3块 RAID 6 双重分布式校验（两套独立校验） 较低写入速度：相比 RAID 5，每次写入需计算两套校验，延迟更高（更高的 写惩罚）。 允许 2块磁盘故障 4块 Raid0\n最少2块pv, 2个条带\nlvcreate --name test-raid0 --type raid0 --stripes 4 --stripesize 64k --size 1g data root@pc:~# lvs --select lv_name=\"test-raid0\" -o lv_name,vg_name -o +seg_type LV VG Type test-raid0 data raid0 raid4 ，raid5\n最少3块pv, 2个条带\nlvcreate --name test-raid4 --type raid4 --stripes 2 --stripesize 64k --size 1g data root@pc:~# lvs --select lv_name=\"test-raid4\" -o lv_name,vg_name -o +seg_type LV VG Type test-raid4 data raid4 raid5\nlvcreate --name test-raid5 --type raid5 --stripes 2 --stripesize 64k --size 100g data root@pc:~# lvs --select lv_name=\"test-raid5\" -o lv_name,vg_name -o +seg_type LV VG Type test-raid5 data raid5 raid6\n最少5块pv，3个条带\nlvcreate --name test-raid6 --type raid6 --stripes 3 --stripesize 64k --size 10g data root@pc:~# lvs --select lv_name=\"test-raid6\" -o lv_name,vg_name -o +seg_type LV VG Type test-raid6 data raid6 raid1\n# lvcreate --type raid1 --mirrors MirrorsNumber --size Size --name LogicalVolumeName VolumeGroupName lvcreate --name raid1 --type raid1 --mirrors 1 --size 1g data raid10\n# lvcreate --type raid10 --mirrors MirrorsNumber --stripes NumberOfStripes --stripesize StripeSize --size Size --name LogicalVolumeName VolumeGroupName lvcreate --name raid10 --type raid10 --mirrors 1 --stripes 2 --stripesize 64k --size 1g data # lvcreate --type thin-pool --size PoolSize --name ThinPoolName VolumeGroupName lvcreate --name thinpool --type thin-pool --size 1g data # lvcreate --type thin --virtualsize MaxVolumeSize --name ThinVolumeName --thinpool ThinPoolName VolumeGroupName lvcreate --name test-thin --type thin --virtualsize 100g --thinpool thinpool data 场景举例 场景：磁盘空间马上要满\n第一步：创建分区或增加一块硬盘\nvmware 扩展磁盘分区\n第二步：给刚才扩展的磁盘分配一个分区\n查看磁盘有多少未分区空间\n[root@lvm ~]# parted /dev/sda GNU Parted 3.1 使用 /dev/sda Welcome to GNU Parted! Type 'help' to view a list of commands. (parted) print free Model: VMware, VMware Virtual S (scsi) Disk /dev/sda: 69.8GB Sector size (logical/physical): 512B/512B Partition Table: msdos Disk Flags: Number Start End Size Type File system 标志 32.3kB 1049kB 1016kB Free Space 1 1049kB 1075MB 1074MB primary xfs 启动 2 1075MB 21.5GB 20.4GB primary lvm 21.5GB 64.4GB 42.9GB Free Space 创建分区\n[root@lvm ~]# fdisk /dev/sda Welcome to fdisk (util-linux 2.23.2). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): n Partition type: p primary (2 primary, 0 extended, 2 free) e extended Select (default p): p Partition number (3,4, default 3): First sector (41943040-125829119, default 41943040): Using default value 41943040 Last sector, +sectors or +size{K,M,G} (41943040-125829119, default 125829119): Using default value 125829119 Partition 3 of type Linux and of size 40 GiB is set Command (m for help): w 通知内核分区变动,否则需要重启操作系统\n[root@lvm ~]# partprobe 第三步：把分区转换为pv\n[root@lvm ~]# pvcreate /dev/sda3 Physical volume \"/dev/sda3\" successfully created. [root@lvm ~]# pvs PV VG Fmt Attr PSize PFree /dev/sda2 centos lvm2 a-- \u003c19.00g 0 /dev/sda3 lvm2 --- 40.00g 40.00g 第四步：扩展所在vg\n[root@lvm ~]# vgextend centos /dev/sda3 Volume group \"centos\" successfully extended [root@lvm ~]# vgs centos VG #PV #LV #SN Attr VSize VFree centos 2 2 0 wz--n- 58.99g \u003c40.00g 第四步：调整lv的空间大小\n[root@lvm ~]# lvresize -L +39g /dev/centos/root Size of logical volume centos/root changed from \u003c17.00 GiB (4351 extents) to \u003c56.00 GiB (14335 extents). Logical volume centos/root successfully resized 第五步：通知文件系统调整文件系统大小\n[root@lvm ~]# xfs_growfs /dev/mapper/centos-root meta-data=/dev/mapper/centos-root isize=512 agcount=4, agsize=1113856 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0 spinodes=0 data = bsize=4096 blocks=4455424, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 data blocks changed from 4455424 to 14679040 完成\n[root@lvm ~]# df -HT 文件系统 类型 容量 已用 可用 已用% 挂载点 devtmpfs devtmpfs 1.5G 0 1.5G 0% /dev tmpfs tmpfs 1.5G 0 1.5G 0% /dev/shm tmpfs tmpfs 1.5G 11M 1.5G 1% /run tmpfs tmpfs 1.5G 0 1.5G 0% /sys/fs/cgroup /dev/mapper/centos-root xfs 61G 15G 46G 25% / /dev/sda1 xfs 1.1G 224M 840M 22% /boot tmpfs tmpfs 1.5G 25k 1.5G 1% /var/lib/ceph/osd/ceph-1 tmpfs tmpfs 1.5G 25k 1.5G 1% /var/lib/ceph/osd/ceph-0 tmpfs tmpfs 1.5G 25k 1.5G 1% /var/lib/ceph/osd/ceph-2 tmpfs tmpfs 297M 13k 297M 1% /run/user/42 tmpfs tmpfs 297M 0 297M 0% /run/user/0 [root@lvm ~]# lsblk -f /dev/sda NAME FSTYPE LABEL UUID MOUNTPOINT sda ├─sda1 xfs 2698408f-1135-4c91-b11c-02892e5208f8 /boot ├─sda2 LVM2_member eZESEK-nJJ1-UvCV-x3st-lJKM-m3Zx-JZNvWO │ ├─centos-root xfs e011e279-e5c9-4dfd-87e9-ecfc7e46bccd / │ └─centos-swap swap e14f66e7-6647-486f-a9d7-acc044d455a1 └─sda3 LVM2_member ugnjZw-FN0H-V8vi-acLb-D6sC-YB1f-CtZIF7 └─centos-root xfs e011e279-e5c9-4dfd-87e9-ecfc7e46bccd / 参考 红帽\n","categories":["lvm"],"description":"linux lvm\n","excerpt":"linux lvm\n","ref":"/linux/lvm.html","tags":["linux","lvm"],"title":"LVM (logical volume manager)"},{"body":"# A scrape configuration for running Prometheus on a Kubernetes cluster. # This uses separate scrape configs for cluster components (i.e. API server, node) # and services to allow each to use different authentication configs. # # Kubernetes labels will be added as Prometheus labels on metrics via the # `labelmap` relabeling action. # # If you are using Kubernetes 1.7.2 or earlier, please take note of the comments # for the kubernetes-cadvisor job; you will need to edit or remove this job. # Keep at most 100 sets of details of targets dropped by relabeling. # This information is used to display in the UI for troubleshooting. global: keep_dropped_targets: 100 # Scrape config for API servers. # # Kubernetes exposes API servers as endpoints to the default/kubernetes # service so this uses `endpoints` role and uses relabelling to only keep # the endpoints associated with the default/kubernetes service using the # default named port `https`. This works for single API server deployments as # well as HA API server deployments. scrape_configs: - job_name: \"kubernetes-apiservers\" kubernetes_sd_configs: - role: endpoints # Default to scraping over https. If required, just disable this or change to # `http`. scheme: https # This TLS \u0026 authorization config is used to connect to the actual scrape # endpoints for cluster components. This is separate to discovery auth # configuration because discovery \u0026 scraping are two separate concerns in # Prometheus. The discovery auth config is automatic if Prometheus runs inside # the cluster. Otherwise, more config options have to be provided within the # \u003ckubernetes_sd_config\u003e. tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt # If your node certificates are self-signed or use a different CA to the # master CA, then disable certificate verification below. Note that # certificate verification is an integral part of a secure infrastructure # so this should only be disabled in a controlled environment. You can # disable certificate verification by uncommenting the line below. # # insecure_skip_verify: true authorization: credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token # Keep only the default/kubernetes service endpoints for the https port. This # will add targets for each API server which Kubernetes adds an endpoint to # the default/kubernetes service. relabel_configs: - source_labels: [ __meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name, ] action: keep regex: default;kubernetes;https # Scrape config for nodes (kubelet). - job_name: \"kubernetes-nodes\" # Default to scraping over https. If required, just disable this or change to # `http`. scheme: https # This TLS \u0026 authorization config is used to connect to the actual scrape # endpoints for cluster components. This is separate to discovery auth # configuration because discovery \u0026 scraping are two separate concerns in # Prometheus. The discovery auth config is automatic if Prometheus runs inside # the cluster. Otherwise, more config options have to be provided within the # \u003ckubernetes_sd_config\u003e. tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt # If your node certificates are self-signed or use a different CA to the # master CA, then disable certificate verification below. Note that # certificate verification is an integral part of a secure infrastructure # so this should only be disabled in a controlled environment. You can # disable certificate verification by uncommenting the line below. # # insecure_skip_verify: true authorization: credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) # Scrape config for Kubelet cAdvisor. # # This is required for Kubernetes 1.7.3 and later, where cAdvisor metrics # (those whose names begin with 'container_') have been removed from the # Kubelet metrics endpoint. This job scrapes the cAdvisor endpoint to # retrieve those metrics. # # In Kubernetes 1.7.0-1.7.2, these metrics are only exposed on the cAdvisor # HTTP endpoint; use the \"/metrics\" endpoint on the 4194 port of nodes. In # that case (and ensure cAdvisor's HTTP server hasn't been disabled with the # --cadvisor-port=0 Kubelet flag). # # This job is not necessary and should be removed in Kubernetes 1.6 and # earlier versions, or it will cause the metrics to be scraped twice. - job_name: \"kubernetes-cadvisor\" # Default to scraping over https. If required, just disable this or change to # `http`. scheme: https # Starting Kubernetes 1.7.3 the cAdvisor metrics are under /metrics/cadvisor. # Kubernetes CIS Benchmark recommends against enabling the insecure HTTP # servers of Kubernetes, therefore the cAdvisor metrics on the secure handler # are used. metrics_path: /metrics/cadvisor # This TLS \u0026 authorization config is used to connect to the actual scrape # endpoints for cluster components. This is separate to discovery auth # configuration because discovery \u0026 scraping are two separate concerns in # Prometheus. The discovery auth config is automatic if Prometheus runs inside # the cluster. Otherwise, more config options have to be provided within the # \u003ckubernetes_sd_config\u003e. tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt # If your node certificates are self-signed or use a different CA to the # master CA, then disable certificate verification below. Note that # certificate verification is an integral part of a secure infrastructure # so this should only be disabled in a controlled environment. You can # disable certificate verification by uncommenting the line below. # # insecure_skip_verify: true authorization: credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) # Example scrape config for service endpoints. # # The relabeling allows the actual service scrape endpoint to be configured # for all or only some endpoints. - job_name: \"kubernetes-service-endpoints\" kubernetes_sd_configs: - role: endpoints relabel_configs: # Example relabel to scrape only endpoints that have # \"example.io/should_be_scraped = true\" annotation. # - source_labels: [__meta_kubernetes_service_annotation_example_io_should_be_scraped] # action: keep # regex: true # # Example relabel to customize metric path based on endpoints # \"example.io/metric_path = \u003cmetric path\u003e\" annotation. # - source_labels: [__meta_kubernetes_service_annotation_example_io_metric_path] # action: replace # target_label: __metrics_path__ # regex: (.+) # # Example relabel to scrape only single, desired port for the service based # on endpoints \"example.io/scrape_port = \u003cport\u003e\" annotation. # - source_labels: [__address__, __meta_kubernetes_service_annotation_example_io_scrape_port] # action: replace # regex: ([^:]+)(?::\\d+)?;(\\d+) # replacement: $1:$2 # target_label: __address__ # # Example relabel to configure scrape scheme for all service scrape targets # based on endpoints \"example.io/scrape_scheme = \u003cscheme\u003e\" annotation. # - source_labels: [__meta_kubernetes_service_annotation_example_io_scrape_scheme] # action: replace # target_label: __scheme__ # regex: (https?) - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: service # Example scrape config for probing services via the Blackbox Exporter. # # The relabeling allows the actual service scrape endpoint to be configured # for all or only some services. - job_name: \"kubernetes-services\" metrics_path: /probe params: module: [http_2xx] kubernetes_sd_configs: - role: service relabel_configs: # Example relabel to probe only some services that have \"example.io/should_be_probed = true\" annotation # - source_labels: [__meta_kubernetes_service_annotation_example_io_should_be_probed] # action: keep # regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: namespace - source_labels: [__meta_kubernetes_service_name] target_label: service # Example scrape config for probing ingresses via the Blackbox Exporter. # # The relabeling allows the actual ingress scrape endpoint to be configured # for all or only some services. - job_name: \"kubernetes-ingresses\" metrics_path: /probe params: module: [http_2xx] kubernetes_sd_configs: - role: ingress relabel_configs: # Example relabel to probe only some ingresses that have \"example.io/should_be_probed = true\" annotation # - source_labels: [__meta_kubernetes_ingress_annotation_example_io_should_be_probed] # action: keep # regex: true - source_labels: [ __meta_kubernetes_ingress_scheme, __address__, __meta_kubernetes_ingress_path, ] regex: (.+);(.+);(.+) replacement: ${1}://${2}${3} target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_ingress_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: namespace - source_labels: [__meta_kubernetes_ingress_name] target_label: ingress # Example scrape config for pods # # The relabeling allows the actual pod scrape to be configured # for all the declared ports (or port-free target if none is declared) # or only some ports. - job_name: \"kubernetes-pods\" kubernetes_sd_configs: - role: pod relabel_configs: # Example relabel to scrape only pods that have # \"example.io/should_be_scraped = true\" annotation. # - source_labels: [__meta_kubernetes_pod_annotation_example_io_should_be_scraped] # action: keep # regex: true # # Example relabel to customize metric path based on pod # \"example.io/metric_path = \u003cmetric path\u003e\" annotation. # - source_labels: [__meta_kubernetes_pod_annotation_example_io_metric_path] # action: replace # target_label: __metrics_path__ # regex: (.+) # # Example relabel to scrape only single, desired port for the pod # based on pod \"example.io/scrape_port = \u003cport\u003e\" annotation. # - source_labels: [__address__, __meta_kubernetes_pod_annotation_example_io_scrape_port] # action: replace # regex: ([^:]+)(?::\\d+)?;(\\d+) # replacement: $1:$2 # target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: pod kubernetes_sd_configs\ncurl --cacert /etc/kubernetes/pki/ca.crt \\ --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt \\ --key /etc/kubernetes/pki/apiserver-kubelet-client.key \\ https://192.168.0.244:6443/api/v1/services?limit=100\u0026resourceVersion=0 node、service、pod、endpoints、endpointslice、ingress\nrole: node __meta_kubernetes_node_name __meta_kubernetes_node_label_\u003clabelname\u003e __meta_kubernete_node_labelpressent_\u003clabelname\u003e __meta_kubernetes_node_annotation_\u003cannotationname\u003e __meta_kubernetes_node_annotationpresent_\u003cannotationname\u003e __meta_kubernetes_node_address_\u003caddress_type\u003e ## role: service #这通常对于服务的黑盒监视很有用 __meta_kubernetes_namespace __meta_kubernetes_service_name __meta_kubernetes_service_port_name __meta_kubernetes_service_port_protocol __meta_kubernetes_service_type #服务的类型 __meta_kubernetes_service_label_\u003clabelname\u003e __meta_kubernetes_service_labelpresent_\u003clabelname\u003e __meta_kubernetes_service_annotation_\u003cannotationname\u003e __meta_kubernetes_service_annotationpresent_\u003cannotationname\u003e __meta_kubernetes_service_cluster_ip # 服务的群集 IP 地址 __meta_kubernetes_service_external_name #服务的群集 IP 地址 ### role: pod __meta_kubernetes_namespace：容器对象的命名空间。 __meta_kubernetes_pod_name：容器对象的名称。 __meta_kubernetes_pod_ip：容器对象的容器 IP。 __meta_kubernetes_pod_label_\u003clabelname\u003e：容器对象中的每个标签。 __meta_kubernetes_pod_labelpresent_\u003clabelname\u003e：对于容器对象中的每个标签。true __meta_kubernetes_pod_annotation_\u003cannotationname\u003e：来自容器对象的每个注释。 __meta_kubernetes_pod_annotationpresent_\u003cannotationname\u003e：对于容器对象中的每个注释。true __meta_kubernetes_pod_container_init：如果容器是InitContainertrue __meta_kubernetes_pod_container_name：目标地址指向的容器的名称。 __meta_kubernetes_pod_container_port_name：容器端口的名称。 __meta_kubernetes_pod_container_port_number：容器端口的编号。 __meta_kubernetes_pod_container_port_protocol：容器端口的协议。 __meta_kubernetes_pod_ready：设置为或表示容器的就绪状态。truefalse __meta_kubernetes_pod_phase：在生命周期中设置为 、、 或 。PendingRunningSucceededFailedUnknown __meta_kubernetes_pod_node_name：调度 Pod 所到的节点的名称。 __meta_kubernetes_pod_host_ip：容器对象的当前主机 IP。 __meta_kubernetes_pod_uid：容器对象的 UID。 __meta_kubernetes_pod_controller_kind：容器控制器的对象类型。 __meta_kubernetes_pod_controller_name：容器控制器的名称 ### role: endpoints __meta_kubernetes_namespace：终结点对象的命名空间。 __meta_kubernetes_endpoints_name：终结点对象的名称。 对于直接从终端节点列表中发现的所有目标（未从底层 Pod 额外推断出的目标），将附加以下标签： __meta_kubernetes_endpoint_hostname：终结点的主机名。 __meta_kubernetes_endpoint_node_name：承载终结点的节点的名称。 __meta_kubernetes_endpoint_ready：设置为终结点的就绪状态或为其设置。truefalse __meta_kubernetes_endpoint_port_name：终结点端口的名称。 __meta_kubernetes_endpoint_port_protocol：端点端口的协议。 __meta_kubernetes_endpoint_address_target_kind：终结点地址目标的类型。 __meta_kubernetes_endpoint_address_target_name：终结点地址目标的名称。 如果终结点属于某个服务，则会附加发现的所有标签。role: service 对于容器支持的所有目标，将附加发现的所有标签。role: pod ### role: ingress 该角色为每个入口的每个路径发现一个目标。这通常对于入口的黑盒监视很有用。该地址将设置为入口规范中指定的主机。ingress 可用的元标签： __meta_kubernetes_namespace：入口对象的命名空间。 __meta_kubernetes_ingress_name：入口对象的名称。 __meta_kubernetes_ingress_label_\u003clabelname\u003e：入口对象中的每个标签。 __meta_kubernetes_ingress_labelpresent_\u003clabelname\u003e：对于入口对象中的每个标签。true __meta_kubernetes_ingress_annotation_\u003cannotationname\u003e：来自入口对象的每个批注。 __meta_kubernetes_ingress_annotationpresent_\u003cannotationname\u003e：对于入口对象中的每个批注。true __meta_kubernetes_ingress_class_name：入口规范中的类名（如果存在）。 __meta_kubernetes_ingress_scheme：入口的协议方案（如果设置了 TLS 配置）。缺省值为 。httpshttp __meta_kubernetes_ingress_path：入口规范的路径。缺省值为 。 ","categories":["prometheus","监控"],"description":"\nkubernetes_sd_configs|prometheus\r\n\r\n","excerpt":"\nkubernetes_sd_configs|prometheus\r\n\r\n","ref":"/prometheus/kubernetes-sd-configs.html","tags":["prometheus","配置","kubernetes_sd_configs"],"title":"kubernetes_sd_configs"},{"body":"filepath获取文件路径 package main import ( \"fmt\" \"path/filepath\" ) func main() { // 判断是否为绝对路径 if filepath.IsAbs(\"/\") { fmt.Println(\"是绝对路径\") } else { fmt.Println(\"是相对路径\") } // 获取一个文件的绝对路径，如果当前文件是绝对路径则直接输出，否则拼接当前目录并输出 dirname, err := filepath.Abs(\"hosts\") if err != nil { fmt.Println(err) } fmt.Println(dirname) // 获取上一级目录 fmt.Println(filepath.Join(\"/etc/hosts\", \"..\")) } ","categories":["golang"],"description":"path|标准库\n","excerpt":"path|标准库\n","ref":"/golang/package/path.html","tags":["golang","path"],"title":"path"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\nfilebeat版本：7.17.29\nelasticsearch版本： 7.15.0\nfilebeat支持基于docker、kubernetes等自动发现，kubernetes的自动发现可以完成对node、pod、service日志的自动采集。\n阅读该文档你将学习到如何动态收集kubernetes中pod日志：\n示例假定每一个 namespace对应一个业务单元，为了实现业务日志的隔离将日志按照 namespace 分类保存在elasticsearch不同的index中\n采集指定 namespace 中所有pod日志、采集指定 pod 日志 动态定制 pod 多行日志采集 json日志的格式化 配置示例 filebeat.yml 运行在k8s外 运行在k8s内 filebeat.autodiscover: providers: - type: kubernetes node: seagullcore01-uat-s2 # 指定filebeat作用范围，防止无法自动检测的节点【可选】 #namespace: default # 只收集default namespace下pod日志 【可选】 resource: pod # 资源发现的对象，可选值 pod/node/service, 默认为pod 【可选】 scope: node # 资源发现作用域，可选值 node/cluster。service资源选择cluster 【可选】 add_resource_metadata: # 对namespace 和 node 元数据进行处理，默认收集所有labels, 忽略所有annotations 【可选】 namespace: include_labels: [\"mylabel\"] include_annotations: [\"co.elastic.logs/enabled\"] node: include_labels: [\"block\",\"zone\"] include_annotations: [\"myannotation\"] deployment: false # 如果pod是deploy管理默认展示deploy名称，通过设置为false 关闭该元数据 kube_config: \"/root/.kube/config\" hints.enabled: true # 启用hints 功能 # false 表示禁用默认配置，filebeat仅自动发现注解包含\"co.elastic.logs/enabled\": \"true\"的pod或namespace # true 表示启用默认配置，filebeat会自动发现k8s所有pod hints.default_config.enabled: false templates: - config: - type: container # 使用filestream 可能无法读取符号连接，建议使用container id: ${data.kubernetes.container.id} paths: - /var/log/containers/*-${data.kubernetes.container.id}.log # 添加 multiline 配置，从 hints 中读取 multiline: pattern: ${data.hints.logs.multiline.pattern} negate: ${data.hints.logs.multiline.negate} match: ${data.hints.logs.multiline.match} output.kafka: hosts: [\"192.168.0.151:9092\",\"192.168.0.152:9092\",\"192.168.0.153:9092\"] #topic: '%{[fields.log_topic]}' topic: '%{[kubernetes.namespace]}' required_acks: 1 compression: gzip max_message_bytes: 1000000 apiVersion: v1 kind: ConfigMap metadata: name: filebeat-config namespace: kube-system data: filebeat.yml: | filebeat.autodiscover: providers: - type: kubernetes node: ${NODE_NAME} hints.enabled: true # 启用hints 功能 # false 表示禁用默认配置，filebeat仅自动发现注解包含\"co.elastic.logs/enabled\": \"true\"的pod # true 表示启用默认配置，filebeat会自动发现k8s所有pod hints.default_config.enabled: false templates: - config: - type: container # 使用filestream 可能无法读取符号连接，建议使用container id: ${data.kubernetes.container.id} paths: - /var/log/containers/*-${data.kubernetes.container.id}.log # 添加 multiline 配置，从 hints 中读取 multiline: pattern: ${data.hints.logs.multiline.pattern} negate: ${data.hints.logs.multiline.negate} match: ${data.hints.logs.multiline.match} output.kafka: hosts: [\"192.168.0.151:9092\",\"192.168.0.152:9092\",\"192.168.0.153:9092\"] # topic: '%{[fields.log_topic]}' topic: '%{[kubernetes.namespace]}' required_acks: 1 compression: gzip max_message_bytes: 1000000 需求一: 采集指定 namespace 中pod日志、采集指定 pod 日志\n功能实现：\n第一步：filebeat.yaml 配置中启用condition：\nhints.enabled: true # 启用hints 功能 # false 表示禁用默认配置，filebeat仅自动发现注解包含\"co.elastic.logs/enabled\": \"true\"的pod或namespace # true 表示启用默认配置，filebeat会自动发现k8s所有pod hints.default_config.enabled: false 第二步：对需要采集的pod 或 namepsace添加注解(该注解名称为自定义)\n收集指定名称空间下所有pod 日志,示例收集default名称空间下所有pod日志\nkubectl annotate ns default co.elastic.logs/enabled=true 收集指定pod日志\nkubectl -n kube-system annotate pod filebeat-99gsc co.elastic.logs/enabled=true 需求二: 多行日志的采集，java报错堆栈处理\n功能实现：\n第一步：开启日志采集，参考 需求一\n第二步：通过注解添加多行日志规则，多行规则参考\nkubectl -n kube-system annotate pod filebeat-bklsz \\ co.elastic.logs/multiline.pattern=\"^[0-9]{4}-[0-9]{2}-[0-9]{2}\" \\ co.elastic.logs/multiline.negate=true \\ co.elastic.logs/multiline.match=after 需求三: 实现对指定pod 日志进行json格式化\n功能实现：\n第一步：开启日志采集，参考 需求一\n第二步：通过注解添加格式解析\nkubectl -n kube-system annotate pod filebeat-99gsc \\ co.elastic.logs/json.keys_under_root=true \\ co.elastic.logs/json.message_key=message 需求四: 实现对pod日志的processor处理,丢弃info日志\n功能实现：\n第一步：开启日志采集，参考 需求一\n第二步：通过注解添加格式解析\n第三步：通过注解添加processors\nprocessors原始格式 转成hints格式 processors: - drop_events: when: regexp: log.level: 'info|I|INFO' kubectl -n kube-system annotate pod filebeat-lxmnp \\ co.elastic.logs/processors.drop_event.when.regexp.log.level='info|I|INFO' \\ co.elastic.logs/json.keys_under_root=true \\ co.elastic.logs/json.message_key=message \\ co.elastic.logs/enabled=true --overwrite ","categories":["ELK"],"description":"\n\r\n","excerpt":"\n\r\n","ref":"/elk/filebeat/config/autodiscover.html","tags":["autodiscover","filebeat"],"title":"filebeat_autodiscover"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\njdk版本：openjdk 11\nzookeeper版本: 3.4.14\nkafka版本：2.13-3.3.2 龙腾出行生产环境使用2.11-2.2.0\n2011年初，美国领英公司（LinkedIn）开源了一款基础架构软件，以奥地利作家弗兰兹·卡夫卡（Franz Kafka）的名字命名，之后 LinkedIn将其贡献给 Apache基金会，随后该软件于2012年10月完成孵化并顺利晋升为Apache顶级项目——这便是大名鼎鼎的Apache Kafka\nKafka 组件订阅 Zookeeper 的 /brokers/ids 路径（broker 在 Zookeeper 上的注册路径），当有 broker 加入集群或退出集群时，这些组件就可以获得通知。\n控制器其实就是一个 broker，只不过它除了具有一般 broker 的功能之外，还负责分区首领的选举\n集群里第一个启动的 broker 通过在Zookeeper 里创建一个临时节点 /controller 让自己成为控制器其他 broker 在启动时也会尝试创建这个节点，不过它们会收到一个“节点已存在”的异常，然后“意识”到控制器节点已存在，也就是说集群里已经有一个控制器了其他 broker 在控制器节点上创建Zookeeper watch 对象，这样它们就可以收到这个节点的变更通知。这种方式可以确保集群里一次只有一个控制器存在。\n每个新选出的控制器通过 Zookeeper 的条件递增操作获得一个全新的、数值更大的 controller epoch。其他 broker 在知道当前 controller epoch 后，如果收到由控制器发出的包含较旧epoch 的消息，就会忽略它们。\nKafka 使用主题来组织数据，每个主题被分为若干个分区，每个分区有多个副本,副本有以下两种类型。\n首领副本每个分区都有一个首领副本。为了保证一致性，所有生产者请求和消费者请求都会经过这个副本。\n跟随者副本首领以外的副本都是跟随者副本。跟随者副本不处理来自客户端的请求，它们唯一的任务就是从首领那里复制消息，保持与首领一致的状态。如果首领发生崩溃，其中的一个跟随者会被提升为新首领。\n为了与首领保持同步，跟随者向首领发送获取数据的请求，这种请求与消费者为了读取消息而发送的请求是一样的。请求消息里包含了跟随者想要获取消息的偏移量，而且这些偏移量总是有序的。\n如果跟随者在 10s 内没有请求任何消息，或者虽然在请求消息，但在 10s 内没有请求最新的数据，那么它就会被认为是不同步的,通过replica.lag.time.max.ms参数来配置超时时长\n首选首领的主要目的是保证集群的负载均衡。除了当前首领之外，每个分区都有一个首选首领——创建主题时选定的首领就是分区的首\n选首领，如果不是，并且该副本是同步的，那么就会触发首领选举，让首选首领成为当前首领。\nauto.leader.rebalance.enable=true 默认已开启\n元数据请求服务器端的响应消息里指明了这些主题所包含的分区、每个分区都有哪些副本，以及哪个副本是首领。元数据请求可以发送给任意一个 broker，因为所有 broker 都缓存了这些信息。\n客户端会把这些信息缓存起来，并直接往目标 broker 上发送生产请求和获取请求。它们需要时不时地通过发送元数据请求来刷新这些信息（python客户端中通过 metadata_max_age_ms参数来配置）\n生产请求 通过acks参数broker确认消息的成功写入，在 Linux 系统上，消息会被写到文件系统缓存里，并不保证它们何时会被刷新到磁盘上\n获取请求 broker按照批次向客户端响应数据，客户端同步配置 fetch_min_bytes和fetch_max_wait_ms参数来控制每个批次大小或批次间隔。同时在多副本中消费者只能消费所有ISR副本已经完成复制的消息（也就是kafka数据的高水位），既然有了高水位也就引出了主从分片间数据同步超时时间replica.lag.time.max.ms(超过该时长的副本会被leader partition从ISR列表中提出，从而减少对消费者进度造成影响)\nflowchart LR\rA(元数据请求)\rB(生产请求)\rC(获取请求)\rZ\u003e请求类型]\rZ -..-\u003e A\rZ -..-\u003e B\rZ -..-\u003e C\rstyle A fill:#bbf,stroke:#333,stroke-width:2px,color:#fff,stroke-dasharray: 5 5\rstyle B fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5 5\rstyle C fill:#bbf,stroke:#333,stroke-width:2px,color:#fff,stroke-dasharray: 5 5 kafka 数据保留策略：\nlog.dir 确认数据保留位置\nlog.retention.hours = 168 和 log.retention.bytes = 1073741824 确认数据保留的时长或大小。\n数据文件按照片段保留数据默认1g（ log.segment.bytes=1073741824 ）如果达到片段上限，就关闭当前文件，并打开一个新文件。当前正在写入数据的片段叫作活跃片段。活动片段永远不会被删除，所以如果你要保留数据 1 天(log.retention.hours = 24)，但片段里包含了 5 天的数据，那么这些数据会被保留 5 天\nkafka 提供了工具查看片段中的内容:\n/opt/kafka/bin/kafka-dump-log.sh --files 00000000000000000504.log |less JMX 全称为 Java Management Extensions，用来管理和监测 Java 程序。最常用到的就是对于 JVM 的监测和管理，比如 JVM 内存、CPU 使用率、线程数、垃圾收集情况等等。另外，还可以用作日志级别的动态修改，比如 log4j 就支持 JMX 方式动态修改线上服务的日志级别.\nJMX是一个标准接口，不但可以用于管理JVM，还可以管理应用程序自身。下图是JMX的架构\n┌─────────┐ ┌─────────┐ │jconsole │ │ Web │ └─────────┘ └─────────┘ │ │ ┌ ─ ─ ─ ─│─ ─ ─ ─ ─ ─ ┼ ─ ─ ─ ─ JVM ▼ ▼ │ │ ┌─────────┐ ┌─────────┐ ┌─┤Connector├──┤ Adaptor ├─┐ │ │ │ └─────────┘ └─────────┘ │ │ MBeanServer │ │ │ │ ┌──────┐┌──────┐┌──────┐ │ └─┤MBean1├┤MBean2├┤MBean3├─┘ │ │ └──────┘└──────┘└──────┘ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┘ ","categories":["kafka"],"description":"\n*剖析kafka副本之间数据如何复制*\r\n\r\n","excerpt":"\n*剖析kafka副本之间数据如何复制*\r\n\r\n","ref":"/database/kafka/replication.html","tags":["zookeeper","kafka"],"title":"Kafka 如何进行复制"},{"body":"MySQL Group Replication（MGR）是 MySQL 官方提供的高可用、高一致性的分布式数据库解决方案，基于原生 MySQL 实现多主/单主架构的同步复制。其核心原理是通过 Paxos 协议变种（Group Communication System, GCS） 实现节点间的数据强一致性，MGR和传统主从复制的本质区别——不是简单的异步复制改进，而是基于paxos的同步复制。\n使用 Paxos 分布式共识算法（具体实现为 XCom）确保集群内节点状态一致。 所有事务需在组内多数节点（N/2+1） 确认后才提交（避免脑裂）。 通信基于 MySQL 插件 group_replication 实现。 graph LR A[客户端发起事务] --\u003e B[本地节点执行] B --\u003e C[生成Binlog Event] C --\u003e D[广播到组内所有节点] D --\u003e E[多数节点验证冲突] E --\u003e F{多数节点通过?} F --\u003e|Yes| G[提交事务并通知组] F --\u003e|No| H[回滚事务] ","categories":["mysql"],"description":"MGR是 MySQL 官方提供的高可用、高一致性的分布式数据库解决方案\n","excerpt":"MGR是 MySQL 官方提供的高可用、高一致性的分布式数据库解决方案\n","ref":"/mysql/mgr.html","tags":["mysql安装"],"title":"mgr"},{"body":" 交易原则 保住本金再图收益，买在分歧时，卖在人声鼎沸处\n买入：换手率大于5%，最近5个交易日有涨停或大于7%冲高，中高位时防止利空庄家拉高套现 买入：市场普遍大跌，第二天缩量下跌，可在第二天或第三天逢低买入绩优股 卖出：市场普遍高开大涨，无量 短期高位票 \u0026 叠加人气票 当天跌幅超过6% 不要急于抄底，当天大概率放量跌停第二天开始缩量下跌。第三天可以考虑买入 主升浪行情，买流通流通股市值大于100亿小于500亿板块龙头，小市值股票更容易被量化操纵无法走出大行情 二十届四中全会中提到：\n打造新兴支柱产业，加快新能源、新材料（半导体材料、PVA 膜、COC/COP、碳纤维复核材料）、航空航天、低空经济\n布局未来产业，推动量子科技、生物制造、氢能和核聚变能、脑机接口、具身智能、第六代移动通信等成为新的经济增长点\n锚定工业化：\n电动汽车 + 智能驾驶 + 飞行汽车 + 换电 + 特快充电\n清洁能源（光、风、核、氢）+ 电网\n机器人 （关注IPO）\n工业材料： 铜 + 银 + 铝 + 碳纤维 + 半导体材料\n人工智能(算力 + 芯片 + 数据中心 + 服务器 + 存储) 集成电路\n通讯 (光纤光缆 + 卫星通信 + 6g ) 6g推动小组 主要成员：中兴通讯 中移动 中联通 中电信 海能达 h3c(紫光)\n国际地缘：\n国产替代 （芯片、操作系统、软件） 军工 稀土 海运 + 港口 基本概念 PB：计算公式 市净率 = 股价 / 每股账面值，也就是市值与总资产的比率\nPE：计算公式 市盈率=现时股价 / 公司盈利 也就是市值与利润的比率。PE计算公司盈利时，往往基于上一年业绩，未必能够反映企业最新情况，例如当公司业务有变动或转型，这个计法就未必适合。\n换手率\n筹码峰\n换手率\n货币中的M1、M2：\n中国的货币统计口径（M0, M1, M2）主要覆盖的是银行体系的存款。一旦钱进入了证券公司的账户，它就暂时退出了央行的货币统计范围。当钱在证券账户时：它被称为“保证金”，是你在证券公司的资产，但这笔钱实际上存放在一个专门的第三方存管账户中。对于央行来说，这部分钱在统计时被视作非存款类金融机构的存款，不纳入M1或M2的统计范围。因此，当大量资金涌入股市时，在统计上会表现为银行体系内的存款减少，可能引起M1甚至M2增速的下降。 计算公式：M1=流通中现金 + 活期存款，M2 = M1 + 准货币，准货币=企业和居民的定期存款+货币市场基金份额（例如余额宝）、住房公积金存款等\n正剪刀差（M1增速 \u003e M2增速）：通常表示流动的热钱多，经济活跃市场信息强劲\n企业层面： 企业更愿意将定期存款（属于M2）转换为活期存款（属于M1），以准备用于支付货款、工资、进行新的投资等。这表明企业看好未来，经营和扩张意愿强烈。 居民层面： 居民可能更倾向于消费或投资股市等，而不是进行长期储蓄。 总体： 货币的流通速度加快，更多的钱从“沉睡”的储蓄状态进入“活跃”的交易状态，预示着经济趋向过热，可能伴随通货膨胀压力。通常出现在经济上行或繁荣期。 负剪刀差（M1增速 \u003c M2增速）：经济下行压力大，市场信心不足，存在“流动性陷阱”风险\n企业层面： 由于找不到好的投资机会，或者对前景悲观，企业将大量资金以定期存款等形式“窖藏”起来，导致活期存款（M1）增长放缓，而定期存款（M2）部分增长较快。这被称为“企业存款定期化”。 居民层面： 由于收入预期不佳或避险情绪上升，居民更倾向于减少消费和购房，将钱存入长期储蓄账户。 总体： 货币的流通速度减慢，资金“沉淀”在银行体系内，无法有效转化为实际需求和投资。即使央行降准降息（释放流动性），钱也流不到实体经济中去，这是典型的通缩风险信号。通常出现在经济下行或衰退期。 货币政策-MLF（Medium-term Lending Facility）：是中央银行提供中期（三个月以上通常为一年）基础货币的货币政策工具，MLF的发放对象是符合监管要求的商业银行和政策性银行，发放方式为质押（质押物可以是：国债、央行票据、政策性金融债、高等级信用债等优质债券）方式\n货币政策-LPR：贷款市场报价利率（LPR）是指由各报价行按照公开市场操作利率加点形成的利率，由全国银行间同业拆借中心计算得出，为银行贷款提供定价参考。具体来说，LPR主要由中期借贷便利利率（MLF）加点形成，反映了市场资金成本和银行的加点成本。LPR包括1年期和5年期以上两个品种，分别用于不同期限的贷款定价。LPR的形成机制确保了其能够及时反映市场利率变化，为实体经济融资成本的降低提供政策引导\n货币政策-回购/逆回购：\n操作 资金流向 抵押品流向 目的 逆回购 央行 → 市场 市场 → 央行 释放流动性（放水） 正回购 市场 → 央行 央行 → 市场 回收流动性（抽水） 持仓\n2025年11月23日15:07:55 摩尔线程正式上市交易，可能带动AI硬件股票上攻\n特朗普批准英伟达H200恢复对华出口\n首条固态电池产线建成\n中日关系\n代码 名称 操作 001333 光华股份 000547 航天发展 605366 宏柏新材 000792 盐湖股份 002607 中公教育 301091 深城交 600089 特变电工 ⭐ 东方电缆、国电南瑞 002340 格林美 920985 海泰新能 000821 京山轻机 600062 华润双鹤 603982 泉峰汽车 000009 中国宝安 002122 汇洲智能 001208 华菱线缆 ⭐ 603979 金诚信 ⭐ 002642 荣联科技 002600 领益智造 ⭐ 002196 方正电机 300397 天和防务 000938 紫光股份 688353 华盛锂电 000158 常山北明 001298 好上好 002085 万丰奥威 300498 温氏股份 603191 望变电气 920826 盖世食品 603650 彤程新材 688726 拉普拉斯 300024 机器人 688458 美芯晟 未见底不要加仓 000001 平安银行 科华数据 厦门 + 数据中心 + 沐熙合作 2 振华科技 蚂蚁上树态势，⭐⭐ 沃尔核材 27.5考虑建仓 安妮股份 15号=》12快建仓 3 广大特材\u003e、中际联合 观察 再升科技、天通股份、⭐金风科技 等待回调 ","categories":["股票"],"description":"学习金融知识，实战股票投资[上证](https://www.szse.cn/)|[深证](https://www.szse.cn/)\n","excerpt":"学习金融知识，实战股票投资[上证](https://www.szse.cn/)|[深证](https://www.szse.cn/)\n","ref":"/2025-07-02/stock.html","tags":["akshare"],"title":"Stock"},{"body":"航天一院（中国运载火箭技术研究院，CASC 一院）\r├─ 003009 中天火箭（一院控股 51.26%，固体小火箭+商业发射） ☆\r航天八院（上海航天技术研究院，CASC 八院）\r├─ 600151 航天机电（八院控股 34.05%，光伏组件+汽车热管理+卫星应用）\r—— 以下公司不在一院/八院体系，顺带列清，避免混淆 ——\r航天五院（中国空间技术研究院，CASC 五院）\r├─ 600118 中国卫星（五院控股 51.32%，小卫星+遥感应用） ☆☆\r└─ 601698 中国卫通（五院为实际控制人，卫星运营+通信广播）\r航天九院（中国航天电子技术研究院，CASC 九院）\r├─ 600879 航天电子（九院控股 35.78%，航天电子设备+测控通信）\t☆☆\r└─ 688562 航天软件（九院间接控股，工业软件+信息化）\r航天六院（中国航天推进技术研究院，CASC 六院）\r├─ 600343 航天动力（六院控股 39.07%，液体火箭发动机技术衍生——泵阀、消防、化工装备）☆☆\r航天七院（四川航天技术研究院，CASC 七院）\r├─ 600855 航天长峰（七院下属，安保+医疗工程，前面已出现）\r中国航天建设集团（CASC 工程服务板块）\r├─ 603698 航天工程（煤气化工程 EPC，航天粉煤加压气化技术） ☆\r航天十一院（中国航天空气动力技术研究院，CASC 十一院）\r├─ 002389 航天彩虹（十一院控股 38.62%，彩虹无人机+膜材料）\r航天科工系（CASIC，与 CASC 并列的另一央企）\r├─ 600271 航天信息（税控+企业数字化）\r├─ 600501 航天晨光（核工装备+环保装备）\r├─ 000547 航天发展（电子对抗+电磁安防） ☆☆\r├─ 002025 航天电器（高端连接器）\t☆\r├─ 300747 锐科激光（光纤激光器）\t☆\r└─ 科创板在审 航天南湖（预警雷达）\r港股红筹\r├─ 00331 航天控股（中国航天国际控股有限公司，CASC 香港投融资平台）\r└─ 01045 亚太卫星（亚太通信卫星，CASC 卫星运营海外窗口）\r乐凯胶片（600135）\r实际控制人是“中国航天科技集团”通过乐凯集团，但无明确对应研究院，归口集团层面管理。 无线电创新院（全称：无线电频谱开发利用和技术创新研究院）于2025年12月30日在雄安新区注册成立，是我国无线电管理技术领域首家以“技术创新+成果转化”为目标的新型研发机构，由国家无线电监测中心、雄安新区管委会、河北省工信厅、中国卫星网络集团、南京航空航天大学、北京交通大学、中国电子科技集团（CETC）7家单位联合共建。 一、主要业务方向 服务卫星互联网产业 牵头向国际电信联盟（ITU）提交了CTC-1、CTC-2两个巨型星座（各96 714颗，合计超19万颗卫星）的频轨资源申请，被视为我国卫星互联网“国家队”。 电磁空间技术研究 聚焦频谱认知、空天地一体电磁安全、低空空域监管等关键技术，为雄安新区数字城市建设和低空经济提供技术支撑。 频谱资源价值挖掘 搭建“政产学研用”一体化平台，推动无线电频谱高效利用、标准制定与产业孵化，形成从芯片、载荷、地面设备到应用服务的全链条创新生态。 二、关联上市公司（通过股东CETC体系） 中国电子科技集团（CETC）是无线电创新院七家共建方之一，其控股或参股的A股上市公司中，已明确参与卫星互联网/商业航天核心环节的主要有： 表格 复制 证券简称\t股票代码\t在卫星互联网链条中的角色 普天科技\t002122\t星地融合通信基础设施、地面站建设 电科数字\t600850\t星载计算载荷、数据中心、AI星载平台 电科芯片\t688081\t星载/终端射频芯片、北斗短报文芯片 电科网安\t300479\t星间/星地抗量子密码与数据安全 四创电子\t600990\t星载电源系统、低空监视雷达 国睿科技\t600562\t地面相控阵监测雷达、微波器件 天奥电子\t002935\t星载原子钟、发射场T0测控设备 东信和平\t002017\t卫星物联网eSIM及终端安全模块 莱斯信息\t688631\t空管自动化系统、低空经济平台 国博电子\t688361\t星载有源相控阵T/R组件及射频芯片 2026新发布鼓励外商投资产业目录\n我来查看你提供的国家发展改革委发布的鼓励外商投资产业目录文件，了解新增产业和重点鼓励产业的具体内容。 根据《鼓励外商投资产业目录（2025年版）》，我为您梳理了新增产业和重点鼓励产业的主要内容：\n新增产业亮点 1. 新能源与储能产业\n新型储能技术：钠离子电池、液流电池、铅炭电池、压缩空气储能、重力储能、飞轮储能、氢储能等 氢能全产业链：制备、储运、燃料电池、加氢站建设 虚拟电厂运营及相关技术研发 2. 智能制造与高端装备\n人形机器人及关键零部件（减速机、电机、控制器等） 半导体设备：光刻机、单晶炉、研磨抛光设备 3D打印设备、量子计算机系统研发制造 3. 生物医药创新\n细胞治疗药物、核酸类药物、核药开发 罕见病用药、儿童专科用药 脑机接口核心器件研发制造 4. 绿色低碳技术\n碳捕集与封存(CCS/CCUS)设备制造 零碳燃料发动机(氢、氨、醇醚燃料) 生物基材料、可降解塑料全链条开发 重点鼓励产业 1. 先进制造业\n新能源汽车全产业链：电池、电机、电控、充电设施 航空航天：民用飞机、航空发动机、卫星制造 集成电路：28纳米及以下芯片制造、先进封装测试 2. 现代服务业\n跨境电商及供应链服务 在线医疗、教育、办公系统开发 工业设计、研发服务外包(KPO) 3. 现代农业\n智慧农业装备、农产品冷链物流 畜禽智能化养殖技术 农业废弃物资源化利用 4. 区域优势产业\n中西部地区：能源化工、新材料、特色农业 东北地区：冰雪装备、生物医药、现代农业 长江经济带：高端制造、绿色产业 目录体现了中国推动高质量发展、促进绿色低碳转型、加快科技创新的政策导向，为外商投资提供了明确的方向指引。\n能源全产业链\nflowchart LR\rsubgraph P[能源]\rsubgraph A[化石能源]\rpowerA(火力发电)\rpowerB(石油)\rpowerC(燃气)\rend\rsubgraph B[新型能源]\rNewPowerA(风电)\rNewPowerB(核能)\rNewPowerC(氢能)\rNewPowerD(光伏)\rNewPowerD(抽水发电)\rend\rend\rsubgraph C[输送渠道]\rtrasportA(管道运输)\rtrasportB(高压运输)\rend\rsubgraph D[储能]\rBatteryA(三元锂电池)\rBatteryB(固态电池)\rend\rsubgraph E[消费终端]\rConsumerA(电动汽车)\rConsumerB(无人机)\rConsumerC(机器人)\rend\rP -..-\u003e C -..-\u003eD -..-\u003e E 电子消费终端\nflowchart LR\rsubgraph E[消费终端]\rConsumerA(电动汽车)\rConsumerB(无人机)\rConsumerC(机器人)\rend\rsubgraph common[公共供应商]\rcarC[电池供应商]\rcarD[电机供应商]\rend\rsubgraph EA[电动汽车]\rcarA[汽车集成商]\rcarB[智能驾驶]\rcarE[一体压铸]\rcarF[轮胎/底盘]\rend\rsubgraph EB[无人机集中关注深圳]\rairA[消费机无人机大疆/影石头]\rairB[低空载人/载物]\rend\rsubgraph EC[机器人]\rrobotA[集成商]\rrobotB[皮肤材料]\rrobotC[关节]\rend\rConsumerA -..-\u003eEA\rConsumerA -..-\u003ecommon\rConsumerB -..-\u003eEB\rConsumerB -..-\u003e|固态高密度|common\rConsumerC -..-\u003eEC\rConsumerC -..-\u003e|固态高密度|common 碳酸锂电池\n长期持有首选：盐湖股份\n成本全球最低，即使碳酸锂跌至 5 万元/吨仍有 40%+ 毛利率，抗周期能力最强。\n中国五矿入主后，2026 起连续资产注入，权益产能 6→11 万吨，复合增速 13%，确定性高。\n政策定位“世界级盐湖产业基地”，资源税、环保审批等风险最低\n。\n优质弹性标的：天齐锂业\n掌控 Greenbushes+SQM，资源品位与规模无替代品；固态电池用超薄锂箔、硫化锂等前沿技术已商业化，享受技术溢价。\n债务高峰已过，2025 起自由现金流大幅回正，锂价每上涨 1 万元，业绩弹性约 25 亿元\n。\n波段交易标的：志存锂业、永兴材料、盛新锂能\n三者共同特征：2025-2027 年产能翻倍，但现金成本 5.5-7 万元/吨，仅适合在锂价 8 万元以上参与反弹。 永兴材料分红率 50%，现金流稳健，防御性稍好；志存锂业未上市，需通过一级或并购基金参与，风险最高。 磷酸铁锂\n钙钛矿电池\n京山轻机 电池关键材料环节\n正极：振华新材、容百科技、当升科技（层状氧化物路线） 负极：贝特瑞、华阳股份、璞泰来（硬碳） 电解液六氟磷酸锂：天赐材料、新宙邦、多氟多（NaPF6盐） 铝箔（集流体）：鼎胜新材、南山铝业 FROM python:3.11.12-alpine3.21 LABEL maintainer=1209233066@qq.com RUN apk add gcc musl-dev linux-headers RUN pip3 install akshare jupyter notebook pandas -i https://mirrors.aliyun.com/pypi/simple/ EXPOSE 80 WORKDIR /opt CMD [\"jupyter\",\"notebook\",\"--ip\",\"0.0.0.0\",\"--port\",\"80\",\"--allow-root\"] docker build . -t ak docker run -d --name ak -v `pwd`:/opt -p 80:80 ak ''' 1. 获取交易时间，排除非交易时间 2. 告警: 每隔五分钟向dingtalk 发送一次 涨跌超5%告警 涨跌速超2%告警 ''' import akshare as ak import pandas as pd # 获取A股实时数据 data = ak.stock_zh_a_spot_em() # 将数据转换为DataFrame df = pd.DataFrame(data) # 假设我们想要获取多个股票的实时数据，例如股票代码列表 stock_codes={\"000001\", \"000009\", \"000547\", \"000792\", \"000821\", \"000938\", \"001333\", \"002085\", \"002340\", \"002607\", \"002642\", \"002803\", \"300498\", \"301091\", \"600089\", \"600062\", \"603191\", \"603650\", \"603982\", \"605366\", \"835985\", \"603315\", \"300085\", \"600536\", \"300397\", \"002122\", \"600876\", \"836826\" } # 筛选指定股票的实时数据 stock_data = df[df[f\"代码\"].isin(stock_codes)] # # 选择我们想要展示的列 columns_to_display = [\"代码\", \"名称\", \"最低\",\"最新价\",\"最高\", \"涨跌幅\",\"涨速\",\"5分钟涨跌\",\"换手率\",\"成交量\",\"量比\",\"年初至今涨跌幅\"] stock_data_display = stock_data[columns_to_display] #print(stock_data_display) # # 按照涨跌幅倒序排列 stock_data_sorted = stock_data_display.sort_values(by=\"5分钟涨跌\", ascending=False) # # # 打印结果 print(\"近期成交信条超过5%涨幅的考虑卖出，不追高！！！\") print(stock_data_sorted) prometheus指标 FROM python:3.11.12-alpine3.21 LABEL maintainer=1209233066@qq.com RUN apk add gcc musl-dev linux-headers RUN pip3 install akshare pandas prometheus_client apscheduler -i https://mirrors.aliyun.com/pypi/simple/ ADD ./stock_metrics.py /opt/stock_metrics.py EXPOSE 6500 CMD [\" python3\",\"/opt/stock_metrics.py\"] #!/usr/bin/python3 \"\"\" 成交量占比{code=\"000001\",name=\"平安银行\"} 换手率{code=\"000001\",name=\"平安银行\"} 涨跌幅{code=\"000001\",name=\"平安银行\"} 年初至今涨跌幅{code=\"000001\",name=\"平安银行\"} \"\"\" from prometheus_client import start_http_server, Gauge from apscheduler.schedulers.blocking import BlockingScheduler import akshare as ak import pandas as pd # 暴露prometheus指标 http://xxxx:6500 start_http_server(6500) g1=Gauge(\"stock_amount_rate\",\"量比%\",[\"code\",\"name\"]) g2=Gauge(\"stock_turnover_rate\",\"换手率\",[\"code\",\"name\"]) g3=Gauge(\"stock_price_change\",\"涨跌幅\",[\"code\",\"name\"]) g4=Gauge(\"stock_year_to_date_changed_rate\",\"年初至今涨跌幅\",[\"code\",\"name\"]) def get_data(): # 获取A股实时数据 data = ak.stock_zh_a_spot_em() # 将数据转换为DataFrame df = pd.DataFrame(data) # 假设我们想要获取多个股票的实时数据，例如股票代码列表 stock_codes={\"000001\", \"000009\", \"000547\", \"000792\", \"000821\", \"000938\", \"001333\", \"002085\", \"002340\", \"002607\", \"002642\", \"002803\", \"300498\", \"301091\", \"600089\", \"600062\", \"603191\", \"603650\", \"603982\", \"605366\", \"835985\", \"688726\", \"300085\", \"600536\", \"300397\", \"002122\", \"600876\", \"836826\", \"688366\", \"600779\",\"300024\" } # 筛选指定股票的实时数据 stock_data = df[df[f\"代码\"].isin(stock_codes)] # # 选择我们想要展示的列 columns_to_display = [\"代码\", \"名称\", \"最低\",\"最新价\",\"最高\", \"涨跌幅\",\"涨速\",\"5分钟涨跌\",\"换手率\",\"成交额\",\"流通市值\",\"量比\",\"年初至今涨跌幅\",\"量比\"] stock_data_display = stock_data[columns_to_display] #print(stock_data_display) # # 按照涨跌幅倒序排列 stock_data_sorted = stock_data_display.sort_values(by=\"5分钟涨跌\", ascending=False) stock_data_list=stock_data_sorted.values.tolist() for item in stock_data_list: g1.labels(item[0],item[1]).set(item[13]) g2.labels(item[0],item[1]).set(item[8]) g3.labels(item[0],item[1]).set(item[5]) g4.labels(item[0],item[1]).set(item[12]) print(\"exec successfull!\") ###############################################main # 初次启动执行一次 get_data() # 实例化定时任务 scheduler = BlockingScheduler(timezone='Asia/Shanghai') # 添加每日 # 1) 09:00-12:00 scheduler.add_job( get_data, 'cron', day_of_week='mon-fri', hour='9-12', minute='8', second=0 ) # 2) 13:00-15:00 scheduler.add_job( get_data, 'cron', day_of_week='mon-fri', hour='13-15', minute='8', second=0 ) print(\"定时任务已启动，将在每日下午3:05执行...\") print(\"按 Ctrl+C 退出\") try: # 启动调度器 scheduler.start() except (KeyboardInterrupt, SystemExit): print(\"\\n定时任务已停止\") docker run -d -p6500:6500 stock_metrics ","categories":["股票"],"description":"\n学习金融知识，实战股票投资[上证](https://www.szse.cn/)|[深证](https://www.szse.cn/)\r\n","excerpt":"\n学习金融知识，实战股票投资[上证](https://www.szse.cn/)|[深证](https://www.szse.cn/)\r\n","ref":"/2025-11-23/stock.html","tags":["akshare"],"title":"Stock-2026年"},{"body":"需求： 1.按照故障等级，故障名称进行告警收敛 顶级路由group_wait 和 repeat_interval 验证步骤：\n发送告警示例1，预期30s后收到告警 等待步骤1完成后，再次发送告警示例1，预期10m + 30s 后收到告警 顶级路由group_interval 验证步骤： 发送告警示例1，在过20s后发送告警示例2，预期在告警2发送后20s收到告警 子路由 group_wait 和 repeat_interval 验证步骤：\n发送告警示例3，预期10m后收到告警 等待步骤1完成后，再次发送告警示例3，预期1h后收到告警 子路由 group_interval 验证步骤：\n发送告警示例3，在过5m后发送告警示例4，预期在告警4发送后10m收到告警 global: resolve_timeout: 30m route: receiver: 'webhook' group_by: ['alertname'] group_wait: 30s group_interval: 5m repeat_interval: 10m routes: - receiver: 'webhook' group_by: ['alertname'] group_wait: 10m group_interval: 20m repeat_interval: 1h matchers: - alertname=~\"database|web\" - severity=\"E4\" receivers: - name: 'webhook' webhook_configs: - url: 'http://127.0.0.1:5001' send_resolved: true 告警示例1 告警示例2 告警示例3 告警示例4 curl -X POST -H \"Content-Type: application/json\" -d '[ { \"labels\": { \"alertname\": \"cpuHigh\", \"instance\": \"node01:9100\", \"job\": \"node_exporter\", \"severity\": \"E3\" }, \"annotations\": { \"description\": \"node01:9100 of job node_exporter has been used cpu \u003e85 more than 5 minutes. (current value: 89.99%)\", \"summary\": \"Instance node01:9100 cpu usage more than 85%\" }, \"generatorURL\": \"http://prometheus.pytc.com/prom\" } ]' \"http://127.0.0.1:9093/alert/api/v2/alerts\" curl -X POST -H \"Content-Type: application/json\" -d '[ { \"labels\": { \"alertname\": \"cpuHigh\", \"instance\": \"node02:9100\", \"job\": \"node_exporter\", \"severity\": \"E3\" }, \"annotations\": { \"description\": \"node02:9100 of job node_exporter has been used cpu \u003e85 more than 5 minutes. (current value: 99.99%)\", \"summary\": \"Instance node02:9100 cpu usage more than 85%\" }, \"generatorURL\": \"http://prometheus.pytc.com/prom\" } ]' \"http://127.0.0.1:9093/alert/api/v2/alerts\" curl -X POST -H \"Content-Type: application/json\" -d '[ { \"labels\": { \"alertname\": \"database\", \"instance\": \"node01:9100\", \"job\": \"node_exporter\", \"severity\": \"E3\" }, \"annotations\": { \"description\": \"node01:9100 of job node_exporter has been used cpu \u003e85 more than 5 minutes. (current value: 89.99%)\", \"summary\": \"Instance node01:9100 cpu usage more than 85%\" }, \"generatorURL\": \"http://prometheus.pytc.com/prom\" } ]' \"http://127.0.0.1:9093/alert/api/v2/alerts\" curl -X POST -H \"Content-Type: application/json\" -d '[ { \"labels\": { \"alertname\": \"database\", \"instance\": \"node01:9100\", \"job\": \"node_exporter\", \"severity\": \"E4\" }, \"annotations\": { \"description\": \"node01:9100 of job node_exporter has been used cpu \u003e85 more than 5 minutes. (current value: 89.99%)\", \"summary\": \"Instance node01:9100 cpu usage more than 85%\" }, \"generatorURL\": \"http://prometheus.pytc.com/prom\" } ]' \"http://127.0.0.1:9093/alert/api/v2/alerts\" ","categories":["告警"],"description":"\n告警收敛,减少告警噪音\r\n","excerpt":"\n告警收敛,减少告警噪音\r\n","ref":"/prometheus/alertmanager/group.html","tags":["alertmanager"],"title":"告警收敛"},{"body":"\n#!/bin/bash # description: 在中控机tidb用户下，检查集群节点ssh秘钥认证 # PasswordAuthentication=no 禁用密码认证 # StrictHostKeyChecking=no 禁用know_hosts 检查 declare -A info clusterName=$(tiup cluster list |awk 'NR\u003e2{print $1}') for cls in $clusterName;do iplist=$(tiup cluster display $cls|awk -F \":\" '/^[0-9].*/{print $1}'|sort |uniq) info[\"$cls\"]=$iplist done for cls in ${!info[@]} do echo \"\" for host in ${info[$cls]};do rsa=\"/tidbdata/tiupshare/tiuptool/storage/cluster/clusters/${cls}/ssh/id_rsa\" ssh -o PasswordAuthentication=no -o StrictHostKeyChecking=no -T -i ${rsa} ${host} exit 2\u003e/dev/null if [ $? -eq 0 ];then printf \"集群：%s,主机:%s [OK]\\n\" $cls $host else printf \"集群：%s,主机:%s [Failed]\\n\" $cls $host fi done done 输出结果：\n集群：cluster01,主机:192.168.0.105 [OK] 集群：cluster01,主机:192.168.0.106 [OK] 集群：cluster01,主机:192.168.0.107 [OK] 集群：tidb_dev_001,主机:192.168.0.223 [Failed] 集群：tidb_dev_001,主机:192.168.0.224 [OK] 集群：tidb_dev_001,主机:192.168.0.225 [OK] ","categories":["tidb"],"description":"checkssh|tidb\n","excerpt":"checkssh|tidb\n","ref":"/tidb/checkssh.html","tags":["tiup工具"],"title":"checkssh"},{"body":"\nlocation 所属模块：ngx_http_core_module\n位置：server、location\n语法：location [=|~|~*|^~|@] /uri/ { ... }\n匹配动作 解释 优先级 = 精确匹配 1 ^~ 匹配以xx开头,忽略正则 2 /html/ 匹配目录 3 ~ 匹配正则 区分大小写 4 ~* 匹配正则 不区分大小写 4 / 默认规则 5 @ 内部的重定向 location ~ .*\\.(js|jpg|jpeg|JPEG|css|BMP|gif|GIF)$ { access_log off; expires 3650d; } location ~ ^/images/.*\\.(php|php5)$ { deny all; } location ~ ^/static..*\\.(php|php5)$ { deny all; } location ~* ^/data/(attachment|avatar)/.*\\.(php|php5)$ { deny all; } location ~ (roboots.txt) { log_not_found off; expires 7d; break; } server { location / { set $memcached_key $uri; memcached_pass name:11211; default_type text/html; error_page 404 @fallback; } location @fallback { proxy_pass http://backend; } } 在location 最经常使用的参数为proxy_pass，接下来参照下图解释proxy_pass中路径拼接规则 location /foo { proxy_pass http://host/bar; } 如果 location 后面没有 /，Nginx 会把请求的 URI 不变地拼接到 proxy_pass 后的地址。\nlocation /foo/ { proxy_pass http://host/bar/; } 如果 location 后面有 /，Nginx 会把匹配到的前缀 /foo/ 去掉，然后把剩下的 URI 拼接到 proxy_pass 后的地址\n示例：\nflowchart LR A(nginx) B(prometheus) C(alertmanager) D(grafana) z\u003e浏览器] --\u003e A A -..-\u003e|/prom| B A -..-\u003e|/alert| C A -..-\u003e|/grafana| D style A fill:#f9f,stroke:#333,stroke-width:2px style B fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5 5 style C fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5 5 style D fill:#bbf,stroke:#f66,stroke-width:2px,color:#fff,stroke-dasharray: 5 5 location匹配不以/结尾 location匹配以/结尾(推荐) 修改访问url 正确配置 访问 /prom/graph 时，Nginx 会把 /prom/graph 直接拼接到 http://192.168.0.161:9090/prom 后面，变成http://192.168.0.161:9090/prom/graph location /prom { proxy_pass http://192.168.0.161:9090/prom; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } 错误配置，显示404 访问 /prom/graph 时，Nginx 会把 /prom/graph 直接拼接到 http://192.168.0.161:9090/prom/ 后面，变成http://192.168.0.161:9090/prom//graph location /prom { proxy_pass http://192.168.0.161:9090/prom/; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } 正确配置 访问 /prom/graph 时，Nginx 会把 /prom/ 去掉，剩下 graph，拼接到 http://192.168.0.161:9090/prom/ 后面，变成http://192.168.0.161:9090/prom/graph location /prom/ { proxy_pass http://192.168.0.161:9090/prom/; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } 错误配置，显示404 访问 /prom/graph 时，Nginx 会把 /prom/ 去掉，剩下 graph，拼接到 http://192.168.0.161:9090/prom 后面，变成http://192.168.0.161:9090/promgraph location /prom/ { proxy_pass http://192.168.0.161:9090/prom; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } 默认情况下，nginx会将请求转发到后端的服务器上，而不会将请求的路径进行修改。如果需要将请求的路径进行修改，可以使用 proxy_redirect 指令。\nlocation = /abc { return 301 /abc/; } location /abc/ { proxy_pass http://192.168.0.161:9090/prom/; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_redirect /prom/ /abc/; } rewrite rewrite https://www.cnblogs.com/likwo/p/6513117.html\nNginx 的 rewrite 指令的主要作用是通过重写请求 URI，从而满足安全、兼容、优化或特定业务逻辑需求\n所属模块：ngx_http_core_module\n位置：server、location\n基本语法：\n关键字 正则表达式 代替的内容 重写类型 rewrite \u003cregex\u003e \u003creplacement\u003e \u003cflag\u003e # \u003cregex\u003e: 可以是字符串或者正则来表示想要匹配的目标URL # \u003creplacement\u003e: 将正则匹配的内容替换成replacement # \u003cflag\u003e: 重写类型： # last: 本条规则匹配完成后，继续向下匹配新的location URI规则；相当于Apache里(L)标记，表示完成rewrite，浏览器地址栏URL地址不变；一般写在server和if中; # break: 本条规则匹配完成后，终止匹配，不再匹配后面的规则，浏览器地址栏URL地址不变；一般使用在location中； # redirect: 返回302临时重定向，浏览器地址会显示跳转后的URL地址； # permanent: 返回301永久重定向，浏览器地址栏会显示跳转后的URL地址； # 访问 /last.html 的时候，页面内容重写到 /index.html 中，并继续后面的匹配，浏览器地址栏URL地址不变 rewrite /last.html /index.html last; # 访问 /break.html 的时候，页面内容重写到 /index.html 中，并停止后续的匹配，浏览器地址栏URL地址不变； rewrite /break.html /index.html break; # 访问 /redirect.html 的时候，页面直接302定向到 /index.html中，浏览器地址URL跳为index.html rewrite /redirect.html /index.html redirect; # 访问 /permanent.html 的时候，页面直接301定向到 /index.html中，浏览器地址URL跳为index.html rewrite /permanent.html /index.html permanent; # 把 /html/*.html =\u003e /post/*.html ，301定向 rewrite ^/html/(.+?).html$ /post/$1.html permanent; # 把 /search/key =\u003e/search.html?keyword=key rewrite ^/search\\/([^\\/]+?)(\\/|$) /search.html?keyword=$1 permanent; # 把当前域名的请求，跳转到新域名上，域名变化但路径不变 rewrite ^/(.*) http://www.jd.com/$1 permanent; # 隐藏真实目录 server { root /var/www/html; # 用 /html_test 来掩饰 html location / { rewrite /html_test /html break; # 使用break拿一旦匹配成功则忽略后续location } location /html { return 403; # 访问真实地址直接报没权限 } } # 对/images/bla_500x400.jpg文件请求，重写到/resizer/bla.jpg?width=500\u0026height=400地址，并会继续尝试匹配location。 rewrite ^/images/(.*)_(\\d+)x(\\d+)\\.(png|jpg|gif)$ /resizer/$1.$4?width=$2\u0026height=$3? last; 20.防盗链 location ~* \\.(jpg|gif|png|swf|flv|wmv|asf|mp3|mmf|zip|rar)${ vaild_referers none blocked *.etiantian.org etiantian.org; if ($incalid_referer) { rewirte ^/http://www.etiantian.org/img/nolink.jpg; } } 21.伪静态（discuz） rewrite ^([^\\.]*)/topic-(.+)\\.html$ $1/portal.php?mod=topic\u0026topic=$2 last; rewrite ^([^\\.]*)/article-([0-9]+)-([0-9]+)\\.html$ $1/portal.php?mod=view\u0026aid=$2\u0026page=$3 last; rewrite ^([^\\.]*)/forum-(\\w+)-([0-9]+)\\.html$ $1/forum.php?mod=forumdisplay\u0026fid=$2\u0026page=$3 last; rewrite ^([^\\.]*)/thread-([0-9]+)-([0-9]+)-([0-9]+)\\.html$ $1/forum.php?mod=viewthread\u0026tid=$2\u0026extra=page%3D$4\u0026page=$3 last; rewrite ^([^\\.]*)/group-([0-9]+)-([0-9]+)\\.html$ $1/forum.php?mod=group\u0026fid=$2\u0026page=$3 last; rewrite ^([^\\.]*)/space-(username|uid)-(.+)\\.html$ $1/home.php?mod=space\u0026$2=$3 last; rewrite ^([^\\.]*)/blog-([0-9]+)-([0-9]+)\\.html$ $1/home.php?mod=space\u0026uid=$2\u0026do=blog\u0026id=$3 last; rewrite ^([^\\.]*)/(fid|tid)-([0-9]+)\\.html$ $1/index.php?action=$2\u0026value=$3 last; rewrite ^([^\\.]*)/([a-z]+[a-z0-9_]*)-([a-z0-9_\\-]+)\\.html$ $1/plugin.php?id=$2:$3 last; if (!-e $request_filename) { return 404; } rewrite ^/images/(.*\\.jpg)$ /images2/$1 break; rewrite ^/abc/.*$ /$1/abc/ last location / { rewrite ^/images/(.*\\.jpg)$ /images2/$1 break; rewrite ^/abc/.*$ /$1/abc/ last } location /images/ { rewrite ^/images/(.*\\.jpg)$ /images2/$1 break; } location /abc/ { rewrite ^/abc/(.*)$ /$1/abc/ last; } location / { rewrite ^/images/.*\\.jpg$ /images/b.jpg break; } http://172.16.100.1/images/b.jpg rewrite \"^/test/(.*\\.jpg)$\" \"/test/repire.jpg\" break; rewrite \" 一、设置一个简单的URL重写： 比如，某网站原有的论坛访问路径为/forum/，但后来根据要求需要更改为/bbs，于是，就可以通过下面的方法实现： rewrite ^/forum/?$ /bbs/ permanent; http://172.16.100.1/forum/ 1、if指令： 语法: if (condition) { ... } 应用环境: server, location 条件: 1、变量名; false values are: empty string (\"\", or any string starting with \"0\";) 2、对于变量进行的比较表达式，可使用=或!=进行测试; 3、正则表达式的模式匹配: ~ 区分大小的模式匹配 ~* 不区分字母大小写的模式匹配 !~ 和 !~* 分别对上面的两种测试取反 4、测试文件是否存在-f或!-f 5、测试目录是否存在-d或!-d 6、测试目录、文件或链接文件的存在性-e或!-e 7、检查一个文件的执行权限-x或!-x 在正则表达式中，可以使用圆括号标记匹配到的字符串，并可以分别使用$1,$2,...,$9进行引用； 例如： 判断用户的浏览器类型： if ($http_user_agent ~* MSIE) { rewrite ^(.*)$ /msie/$1 break; } if ($http_user_agent ~* opera) { rewrite ^(.*)$ /opera/$1 break; } 如果用户请求的页面不存在，实现自定义跳转： if (!-f $request_filename) { rewrite ^(/.*)$ /rewrite.html permanent; } 实现域名跳转 server { listen 80; server_name jump.magedu.com; index index.html index.php; root /www/htdocs; rewrite ^/ http://www.magedu.com/; } 实现域名镜像 server { listen 80; server_name mirror.magedu.com; index index.html index.php; root /www/htdocs; rewrite ^/(.*)$ http://www.magedu.com/$1 last; } 简单的防盗链配置： location ~* \\.(gif|jpg|png|swf|flv)$ { valid_referers none blocked www.magedu.com; if ($invalid_referer) { rewrite ^/ http://www.magedu.com/403.html; # return 404 } } 第一行：gif|jpg|png|swf|flv 表示对gif、jpg、png、swf、flv后缀的文件实行防盗链 第二行：www.magedu.com 表示对www.magedu.com这个来路进行判断if{}里面内容的意思是，如果来路不是指定来路就跳转到错误页面，当然直接返回404也是可以的。 if (!-e $request_filename) { rewrite ^/user/([0-9]+)/?$ /view.php?go=user_$1 last; rewrite ^/component/id/([0-9]+)/?$ /page.php?pageid=$1 last; rewrite ^/component/([^/]+)/?$ /page.php?pagealias=$1 last; rewrite ^/category\\_([0-9]+)\\.htm$ http://$host/category/$1/ permanent; rewrite ^/showday\\_([0-9]+)\\_([0-9]+)\\_([0-9]+)\\.htm$ http://$host/date/$1/$2/$3/ permanent; showday_1_2_3.htm $host/date/1/2/3/ } server { listen 80 default; server_name *.mysite.com; rewrite ^ http://mysite.com$request_uri permanent; } if if判断自定义和内置全局环境变量\n位置: 全局 location 指令下\n语法: if (表达式) { };\n例如访问： http://localhost:81/download/stat.php?id=1585378\u0026web_id=1585378 root：/var/www/html\n变量 解释 示例 $args 这个变量等于请求行中的参数，同$query_string $content_length 请求头中的Content-length字段 $content_type 请求头中的Content-Type字段 $document_root 当前请求在root指令中指定的值 /var/www/html $host 请求主机头字段，否则为服务器名称 localhost $http_user_agent 客户端agent信息 $http_cookie 客户端cookie信息 $limit_rate 这个变量可以限制连接速率 $request_method 客户端请求的动作，通常为GET或POST $request_body_file $request_filename 当前请求的文件路径，由root或alias指令与URI请求生成 /var/www/html/download/stat.php $request_uri 包含请求参数的原始URI，不包含主机名，如：”/foo/bar.php?arg=baz” /download/stat.php?id=1585378\u0026web_id=1585378 $query_string $remote_addr 客户端的IP地址 $remote_port 客户端的端口 $remote_user 已经经过Auth Basic Module验证的用户名 $scheme HTTP方法（如http，https） $server_protocol 请求使用的协议，通常是HTTP/1.0或HTTP/1.1 $server_addr 服务器地址，在完成一次系统调用后可以确定这个值 $server_name 服务器名称 $server_port 请求到达服务器的端口号 81 $uri 不带请求参数的当前URI，$uri不包含主机名，如”/foo/bar.html $document_uri 与$uri相同 /download/stat.php 条件表达式\n正则表达式匹配： ~：与指定正则表达式模式匹配时返回“真”，判断匹配与否时区分字符大小写； ~*：与指定正则表达式模式匹配时返回“真”，判断匹配与否时不区分字符大小写； !~：与指定正则表达式模式不匹配时返回“真”，判断匹配与否时区分字符大小写； !~*：与指定正则表达式模式不匹配时返回“真”，判断匹配与否时不区分字符大小写； 文件及目录匹配判断： -f, !-f：判断指定的路径是否为存在且为文件； -d, !-d：判断指定的路径是否为存在且为目录； -e, !-e：判断指定的路径是否存在，文件或目录均可； -x, !-x：判断指定路径的文件是否存在且可执行； # 如果文件不存在则返回400 if (!-f $request_filename) { return 400; } # 如果host是www.360buy.com，则301到www.jd.com中 if ( $host != \"www.jd.com\" ){ rewrite ^/(.*)$ https://www.jd.com/$1 permanent; } # 如果请求类型是POST则返回405，return不能返回301,302 if ($request_method = POST) { return 405; } # 如果参数中有 a=1 则301到指定域名 if ($args ~ a=1) { rewrite ^ http://example.com/ permanent; } # 文件名及参数重写 location = /index.html { set $name test; # 修改默认值为 if($args ~* name=(\\w+?)(\u0026|$)) { set $name $1; # 如果参数中有 name=xx 则使用该值 } #permanent 301重定向 rewrite ^ /$name.html permanent; } # 禁止指定IP访问 location / { if ($remote_addr = 192.168.1.253) { return 403; } } # 如果请求的文件不存在，则反向代理到localhost 。这里的break也是停止继续rewrite if (!-f $request_filename){ break; proxy_pass http://127.0.0.1; } location / { if ($http_user_agent ~* \"andriod\") { proxy_pass http://lb1; } if ($http_user_agent ~* \"iphome\") { proxy_pass http://lb2; } if ($http_user_agent ~* \"qihoobot|Baiduspider|Googlebot|Googlebot-Mobile|Googlebot-Image|Mediapartners-Google|Adsbot-Google|Feedfetcher-Google|Yahoo! Slurp|Yahoo! Slurp China|YoudaoBot|Sosospider|Sogou spider|Sogou web spider|MSNBot|ia_archiver|Tomato Bot\") { return 403; } } 在location中使用if语句可以实现条件判断，其通常有一个return语句，且一般与有着last或break标记的rewrite规则一同使用。但其也可以按需要使用在多种场景下，需要注意的是，不当的使用可能会导致不可预料的后果。 location / { if ($request_method == “PUT”) { proxy_pass http://upload.magedu.com:8080; } if ($request_uri ~ \"\\.(jpg|gif|jpeg|png)$\") { proxy_pass http://imageservers; break; } } agent 参数 根基客户端设备不同选择主机 _______________________________________________________ upstream LB1 { server 10.0.0.2:81 weight=10; server 10.0.0.3:82 weight=6 down; server 10.0.0.1:83 weight=5 backup max_fails=2 fail_timeout=20s; ip_hash; upstream LB2 { server 10.0.0.2:84 weight=10; server 10.0.0.3:85 weight=6 down; server 10.0.0.1:86 weight=5 backup max_fails=2 fail_timeout=20s; ip_hash; } server { listen 80; server_name blog.etiantian.org; location / { if ($http_user_agent ~* \"andriod\") { proxy_pass http://lb1; } if ($http_user_agent ~* \"iphone\") { proxy_pass http://lb2; } proxy_pass http://lb1; proxy_set_header X-Real-IP $remote_addr; 后端服务器接收到真实的客户IP proxy_set_header X-Forwarded-For $remote_addr; 后端服务器接收到真实的客户IP proxy_set_header Host $host; 按照负载均衡的server_name 名称查找后端的基于域名的虚拟主机 } } ","categories":["nginx"],"description":"location指令|nginx\n","excerpt":"location指令|nginx\n","ref":"/nginx/location.html","tags":["location指令"],"title":"location指令"},{"body":"\n创建hpa配置文件\nkubectl autoscale deployment python –min=2 –max=5 –cpu-percent=80\napiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: python namespace: default spec: maxReplicas: 5 minReplicas: 2 targetCPUUtilizationPercentage: 80 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: python deployment 示例\napiVersion: apps/v1 kind: Deployment metadata: name: alpine namespace: default spec: replicas: 1 selector: matchLabels: app: alpine template: metadata: labels: app: alpine spec: nodeName: hdss7-21.host.com containers: - name: alpine image: alpine imagePullPolicy: IfNotPresent command: [\"sleep\",\"10000\"] resources: limits: cpu: 1000m memory: 1000Mi requests: cpu: 1000m memory: 1000Mi ","categories":["kubernetes"],"description":"hpa|k8s\n","excerpt":"hpa|k8s\n","ref":"/kubernetes/hpa.html","tags":["hpa"],"title":"Hpa"},{"body":"graph LR\rA[\"/\"] -.-\u003e|User Binaries| B[\"/bin\"]\rA -.-\u003e|System Binaries| C[\"/sbin\"]\rA -.-\u003e|Configuration Files| D[\"/etc\"]\rA -.-\u003e|Device Files| E[\"/dev\"]\rA -.-\u003e|Process Information| F[\"/proc\"]\rA -.-\u003e|Variable Files| G[\"/var\"]\rA -.-\u003e|Temporary Files| H[\"/tmp\"]\rA -.-\u003e|User Programs| I[\"/usr\"]\rA -.-\u003e|Home Directories| J[\"/home\"]\rA -.-\u003e|Boot Loader Files| K[\"/boot\"]\rA -.-\u003e|System Libraries| L[\"/lib\"]\rA -.-\u003e|Optional add-on Apps| M[\"/opt\"]\rA -.-\u003e|Mount Directory| N[\"/mnt\"]\rA -.-\u003e|Removable Devices| O[\"/media\"]\rA -.-\u003e|Service Data| P[\"/srv\"] ","categories":"","description":"\nlinux 文档中心\r\n","excerpt":"\nlinux 文档中心\r\n","ref":"/docs/linux/","tags":"","title":""},{"body":"\n集群安装 数据同步 数据迁移 数据备份 数据恢复 命令行工具 监控 安全 性能优化 故障处理 ","categories":"","description":"","excerpt":"\n集群安装 数据同步 数据迁移 数据备份 数据恢复 命令行工具 监控 安全 性能优化 故障处理 ","ref":"/docs/database/tidb/","tags":"","title":""},{"body":"consul_sd_configs\n# 启动一个单实例的[consul](https://www.consul.io/downloads) consul agent -bootstrap \\ --client=0.0.0.0 \\ -config-dir=/etc/conf.d \\ -data-dir=/consul/data/ \\ -dev \\ -enable-local-script-checks \\ -ui # 在consul 中注册服务 vi /etc/conf.d/node.json { \"services\": [ { \"id\": \"node_exporter-node01\", \"name\": \"node01\", \"address\": \"10.4.7.11\", \"port\": 9100, \"tags\": [\"nodes\"], \"checks\": [{ \"http\": \"http://10.4.7.11:9100/metrics\", \"interval\": \"5s\" }] }, { \"id\": \"node_exporter-node02\", \"name\": \"node02\", \"address\": \"10.4.7.12\", \"port\": 9100, \"tags\": [\"nodes\"], \"checks\": [{ \"http\": \"http://10.4.7.12:9100/metrics\", \"interval\": \"5s\" }] } ] } # 加载配置文件/etc/conf.d/node.json consul reload scrape_configs: - job_name: \"node\" consul_sd_configs: - server: \"47.113.100.31:8500\" tags: - \"nodes\" refresh_interval: 2m # 通过测试命令测试服务的注册发现 consul services register -id=\"node_exporter-node02\" consul services deregister -id=\"node_exporter-node02\" ","categories":["prometheus","监控"],"description":"\nkconsul_sd_configs|prometheus\r\n\r\n","excerpt":"\nkconsul_sd_configs|prometheus\r\n\r\n","ref":"/prometheus/consul-sd-configs.html","tags":["prometheus","配置","consul_sd_configs"],"title":"consul_sd_configs"},{"body":"","categories":"","description":"\n编程语言\r\n","excerpt":"\n编程语言\r\n","ref":"/docs/code/","tags":"","title":""},{"body":"i/o 操作文件内容 读取文件\npackage main import ( \"fmt\" \"io\" \"os\" ) func main() { // 打开文件 file, err := os.Open(\"/etc/hosts\") if err != nil { fmt.Println(err) return } // 关闭文件 defer file.Close() // 读取文件 var buffer [2048]byte var result []byte for { seed, err := file.Read(buffer[:]) if err != nil \u0026\u0026 err == io.EOF { fmt.Println(err) break } result = append(result, buffer[:seed]...) } fmt.Println(string(result)) } 文件写入\npackage main import ( \"fmt\" \"io\" \"os\" ) func main() { // 打开文件 file, err := os.Open(\"/etc/hosts\") if err != nil { fmt.Println(err) return } // 关闭文件 defer file.Close() // 读取文件 var buffer [2048]byte var result []byte for { seed, err := file.Read(buffer[:]) if err != nil \u0026\u0026 err == io.EOF { fmt.Println(\"文件读取完毕\") break } result = append(result, buffer[:seed]...) } // 文件写入操作 f, err := os.OpenFile(\"/tmp/hosts\", os.O_CREATE|os.O_APPEND|os.O_WRONLY, 0644) if err != nil { fmt.Println(err) return } defer f.Close() f.Write(result) } 文件拷贝 io.cpoy()\npackage main import ( \"fmt\" \"io\" \"os\" ) func main() { source, err := os.Open(\"/etc/hosts\") if err != nil { fmt.Println(err) } dest, err := os.OpenFile(\"/tmp/hosts\", os.O_CREATE|os.O_APPEND|os.O_WRONLY, 0644) if err != nil { fmt.Println(err) } defer source.Close() defer dest.Close() countwrite, err := io.Copy(dest, source) fmt.Println(countwrite, err) } ","categories":["golang"],"description":"io|标准库\n","excerpt":"io|标准库\n","ref":"/golang/package/io.html","tags":["golang","io"],"title":"io"},{"body":"流程控制语句 if 判断 switch 判断 for 循环 for range goto 跳转 if判断 基本if语法\npackage main import \"fmt\" func main() { var overall = false if overall { fmt.Println(\"当前值为true\") } if !overall { fmt.Println(\"当前值不是true\") } } if else\npackage main import \"fmt\" func main() { if overall := false; overall { fmt.Println(\"你做出了正确的选择\") } else { fmt.Println(\"再接再厉\") } } if else if\npackage main import \"fmt\" func main() { if score := 60; score \u003c 60 { fmt.Println(\"成绩不及格需要补考\") } else if score == 60 { fmt.Println(\"恭喜你，你是最幸运的人\") } else { fmt.Println(\"恭喜你，你的能力很强\") } } switch 基本switch 语法\npackage main import \"fmt\" func main() { role := \"other\" switch role { case \"master\": fmt.Printf(\"我是%s\", role) case \"slave\": fmt.Printf(\"我是%s\", role) default: fmt.Printf(\"unknow\") } } package main import \"fmt\" func main() { switch role := \"other\"; role { case \"master\": fmt.Printf(\"我是%s\", role) case \"slave\": fmt.Printf(\"我是%s\", role) default: fmt.Printf(\"unknow\") } } package main import \"fmt\" func main() { switch role := \"MASTER\"; role { case \"master\", \"Master\", \"MASTER\": fmt.Printf(\"我是%s\", role) case \"slave\", \"Slave\", \"SLAVE\": fmt.Printf(\"我是%s\", role) default: fmt.Printf(\"unknow\") } } 判断条件中包含逻辑运算\npackage main import ( \"fmt\" ) func main() { var os string = \"Centos\" var release int = 7 switch { case os == \"Centos\" \u0026\u0026 release == 7: fmt.Print(\"centos7\") case os == \"Centos\" \u0026\u0026 release == 6: fmt.Print(\"centos6\") case os == \"Centos\" \u0026\u0026 release == 5: fmt.Print(\"centos5\") case os == \"Centos\" \u0026\u0026 release == 4: fmt.Print(\"centos4\") } } break 跳出switch\npackage main import ( \"fmt\" ) func main() { var os string = \"Centos\" var release int = 7 switch { case os == \"Centos\" \u0026\u0026 release == 7: fmt.Print(\"centos7\") fmt.Println(\"我准备用systemd管理服务\") break fmt.Println(\"我准备用rsyslog记录日志\") case os == \"Centos\" \u0026\u0026 release == 6: fmt.Print(\"centos6\") case os == \"Centos\" \u0026\u0026 release == 5: fmt.Print(\"centos5\") case os == \"Centos\" \u0026\u0026 release == 4: fmt.Print(\"centos4\") } } fallthrough执行完毕当前case 会执行紧接着的case\npackage main import ( \"fmt\" ) func main() { var os string = \"Centos\" var release int = 7 switch { case os == \"Centos\" \u0026\u0026 release == 7: fmt.Print(\"centos7\") fmt.Println(\"我准备用systemd管理服务\") fmt.Println(\"我准备用rsyslog记录日志\") fallthrough case os == \"Centos\" \u0026\u0026 release == 6: fmt.Print(\"centos6\") fmt.Println(\"我准备用init管理服务\") fmt.Println(\"我准备用syslog记录日志\") fallthrough case os == \"Centos\" \u0026\u0026 release == 5: fmt.Print(\"centos5\") case os == \"Centos\" \u0026\u0026 release == 4: fmt.Print(\"centos4\") } } for 循环 格式1 标准格式\npackage main import \"fmt\" func main() { for i := 1; i \u003c= 9; i++ { for j := 1; j \u003c= i; j++ { fmt.Printf(\"%d * %d = %d \", j, i, j*i) } fmt.Println() } } 格式2\npackage main import ( \"fmt\" ) func main() { for i := 1; i \u003c 10; { for j := 1; j \u003c= i; { fmt.Printf(\"%d * %d = %d \", j, i, j*i) j++ } fmt.Println() i++ } } package main import \"fmt\" func main() { i := 1 for i \u003c 10 { j := 1 for j \u003c= i { fmt.Printf(\"%d * %d = %d \", j, i, j*i) j++ } i++ fmt.Println() } } 格式三，相当于while true\npackage main import \"fmt\" func main() { i := 1 for { fmt.Println(i) i++ if i \u003e 100 { break } } } 示例： 水仙花数\npackage main import \"fmt\" // 水仙花数 // 百位^3 + 十位^3 + 个位^3 = 这个数 func main() { for x := 1; x \u003c= 999; x++ { a := x / 100 b := x % 100 / 10 c := x % 10 if a*a*a+b*b*b+c*c*c == x { fmt.Println(x) } } } for range 对数组，切片，集合，字符串友好\npackage main func main() { var a =[4]string{\"a\",\"b\",\"c\",\"d\"} for k, v := range a { println(k, \"----\u003e\", v) } } break break 默认结束当前循环，也可以定义标签跳出指定层循环\npackage main import \"fmt\" func main() { out: for i := 1; i \u003c 10; i++ { for j := 1; j \u003c= i; j++ { fmt.Printf(\"%d*%d=%d \", j, i, i*j) if i \u003e 5 { break out } } fmt.Println() } } continue 跳过当前循环，执行下一个循环。也可以指定下一个循环的标签\npackage main import \"fmt\" func main() { out: for i := 1; i \u003c 10; i++ { for j := 1; j \u003c= i; j++ { fmt.Printf(\"%d*%d=%d \", j, i, i*j) if i \u003e 5 { continue out } } fmt.Println() } } goto 跳转到指定标签处\npackage main import \"fmt\" func main() { out: for i := 1; i \u003c 10; i++ { for j := 1; j \u003c= i; j++ { fmt.Printf(\"%d*%d=%d \", j, i, i*j) if i \u003e 5 { goto out } } fmt.Println() } } ","categories":["golang"],"description":"go|流程控制语句\n","excerpt":"go|流程控制语句\n","ref":"/golang/control.html","tags":["golang","流程控制语句"],"title":"流程控制语句"},{"body":"yum install rsyslog [root@zabbix ~]# grep -Ev '^$|^#' /etc/rsyslog.conf #开启udp/tcp 514 接收日志 $ModLoad imudp $UDPServerRun 514 $ModLoad imtcp $InputTCPServerRun 514 #默认的日志格式模板，无需定义 $ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat #屏蔽本机的日志 :fromhost-ip,!isequal,\"127.0.0.1\" ?RSYSLOG_TraditionalFileFormat #所有类型.所以级别\t: :ommysql:主机,库名,用户名,密码 *.* @filebeat:端口 #检查语法rsyslog.conf [root@Nagios ~]# rsyslogd -f /etc/rsyslog.conf -N1 [root@Nagios ~]# /etc/init.d/rsyslog restart 客户端配置\nH3C 配置 sys info-center loghost 168.204.37.50 linux配置 *.* @168.204.37.50 # udp *.* @@168.204.37.50 ## tcp rsyslog.conf\nrsyslog中的数据库支持是通过可加载的插件模块集成的。要使用数据库功能，必须在使用第一个数据库表操作之前在配置文件中启用数据库插件。这是通过放置\n$ModLoad ommysql\n接下来，我们需要告诉rsyslogd将数据写入数据库。由于使用默认架构，因此无需为此定义模板。我们可以使用硬编码的代码（rsyslogd处理正确的模板链接）。因此，对于MySQL，我们需要做的就是在/etc/rsyslog.conf中添加一个简单的选择器行：\n. :ommysql:database-server,database-name,database-userid,database-password\n例如，如果您仅对来自邮件子系统的消息感兴趣，则可以使用以下选择器行：\nmail.* :ommysql:127.0.0.1,syslog,syslogwriter,topsecret\n如果您要转发多个服务器，则可以很快完成。Rsyslog对操作的数量或类型没有限制，因此您可以定义任意多个目标。然而，重要的是要知道，全套指令构成了一个动作。因此，您不能简单地添加（仅）第二条转发规则，而还需要复制规则配置。请注意，对于第二个操作，请使用不同的队列文件名，否则将使系统混乱。\n转发到两个主机的示例如下所示：\n$ ModLoad imuxsock ＃本地消息接收\n$ WorkDirectory / rsyslog / work ＃工作（假脱机）文件的默认位置\n*记录系统日志消息的优先级**¶* ＃开始转发规则1\n$ ActionQueueType LinkedList ＃使用异步处理\n$ ActionQueueFileName srvrfwd1 ＃设置文件名，还启用磁盘模式\n$ ActionResumeRetryCount -1 ＃插入失败时无限重试\n$ ActionQueueSaveOnShutdown on ＃如果rsyslog关闭，则保存内存数据\n. @@ server1：端口\n＃结束转发规则1\n＃开始转发规则2\n$ ActionQueueType LinkedList ＃使用异步处理\n$ ActionQueueFileName srvrfwd2 ＃设置文件名，还启用磁盘模式\n$ ActionResumeRetryCount -1 ＃插入失败时无限重试\n$ ActionQueueSaveOnShutdown on ＃如果rsyslog关闭，则保存内存数据\n. @@ server2\n＃结束转发规则2\n配置日志模板：\n$template 模板名称,\"%timegenerated% %HOSTNAME% %syslogtag%%msg%0\r$template RemoteLogs,\"/tmp/rsyslog/%$YEAR%-%$MONTH%/%fromhost-ip%/%fromhost-ip%_%$YEAR%-%$MONTH%-%$DAY%.log\"\r","categories":["linux"],"description":"\ncentos7 配置 rsyslog|linux\r\n","excerpt":"\ncentos7 配置 rsyslog|linux\r\n","ref":"/linux/rsyslog.html","tags":["log"],"title":"rsyslog"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\njdk版本：openjdk 11\nzookeeper版本: 3.4.14\nkafka版本：3.3.2\n2011年初，美国领英公司（LinkedIn）开源了一款基础架构软件，以奥地利作家弗兰兹·卡夫卡（Franz Kafka）的名字命名，之后 LinkedIn将其贡献给 Apache基金会，随后该软件于2012年10月完成孵化并顺利晋升为Apache顶级项目——这便是大名鼎鼎的Apache Kafka\nkafka_exporter wget https://github.com/danielqsj/kafka_exporter/releases/download/v1.9.0/kafka_exporter-1.9.0.linux-amd64.tar.gz tar xf kafka_exporter-1.9.0.linux-amd64.tar.gz mv kafka_exporter-1.9.0.linux-amd64/kafka_exporter /usr/bin/kafka_exporter tee /usr/lib/systemd/system/kafka_exporter.service \u003c\u003c'EOF' [Unit] Description=kafka_exporter service Documentation=https://github.com/danielqsj/kafka_exporter After=network.target [Service] ExecStart=/usr/bin/kafka_exporter \\ --web.listen-address=\":9309\" \\ --kafka.server=192.168.0.151:9092 \\ --kafka.server=192.168.0.152:9092 \\ --kafka.server=192.168.0.153:9092 # 监控有密码认证的kafka #--sasl.enabled \\ #--sasl.username=filebeat \\ #--sasl.password=ZmlsZWJlYXRAMjAyNgo= \\ #--sasl.mechanism=scram-sha256 User=root [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable kafka_exporter --now systemctl status kafka_exporter grafna 报表id 21078\n指标名称 指标含义 kafka_consumergroup_lag 消费者组的滞后消息数，即消息积压量 kafka_consumergroup_members 消费者组中成员数量 kafka_consumergroup_current_offset 消费者组在当前分区的最新提交偏移量 kafka_topic_partition_current_offset 主题分区在Broker上的最新消息偏移量（生产者偏移） kafka_brokers Kafka集群中的代理（Broker）数量 kafka_broker_info kafka_topic_partitions 指定主题的分区数量 kafka_topic_partition_replicas kafka_topic_partition_in_sync_replica process_cpu_seconds_total process_max_fds process_open_fds process_start_time_seconds - name: kafka rules: - alert: KAFKA_brokers异常 expr: kafka_broker_info != 1 for: 2m labels: severity: critical annotations: description: \"{{ $labels.name }}当前brokers异常：{{ $labels.address }}\" - alert: 电商生产KAFKA消息整体积压 expr: sum(kafka_consumergroup_lag_sum{job=\"kafka-exporter\"}) by (name,consumergroup, topic)\u003e5000 for: 2m labels: severity: critical annotations: description: \"【环境】{{ $labels.name }}\\n【消费组】{{ $labels.consumergroup }}\\n【topic】{{ $labels.topic }}【积压】：{{ $value | printf \\\"%.2f\\\" }}\" - alert: 电商生产KAFKA消息分区积压 expr: (sum(kafka_consumergroup_lag{job=\"kafka-exporter\"}) by (name,consumergroup, topic, partition)\u003e1500) AND ON() (hour()+8)%24 \u003e= 7 \u003c= 21 for: 3m labels: severity: critical annotations: description: \"【环境】{{ $labels.name }}\\n【消费组】{{ $labels.consumergroup }}\\n【topic】{{$labels.topic}}【分区】{{ $labels.partition }}【积压】：{{ $value | printf \\\"%.2f\\\" }}\" - alert: 电商生产KAFKA分区数过多 expr: sum by(name)(kafka_topic_partitions{job=\"kafka-exporter\",topic !~\"__.*\"})\u003e1500 for: 2m labels: severity: critical annotations: description: \"{{ $labels.name }}当前分区数：{{ $value | printf \\\"%.2f\\\" }}\" - alert: 电商生产KAFKA_brokers丢失 expr: kafka_brokers{job=\"kafka-exporter\"} \u003c 3 for: 2m labels: severity: critical annotations: description: \"{{ $labels.name }}当前brokers数：{{ $value | printf \\\"%.2f\\\" }}\" - alert: 电商生产KAFKA_TopicsReplicas expr: sum(kafka_topic_partition_in_sync_replica{job=\"kafka-exporter\"}) by (name,topic) \u003c1 for: 2m labels: severity: critical annotations: description: \"{{ $labels.name }} Kafka topic in-sync partition：{{ $value | printf \\\"%.2f\\\" }}\" ","categories":["kafka"],"description":"\n*通过kafka-exporter监控broker、producer、consumer*\r\n\r\n","excerpt":"\n*通过kafka-exporter监控broker、producer、consumer*\r\n\r\n","ref":"/database/kafka/monitor.html","tags":["monitor","kafka","prometheus","exporter"],"title":"kafka_exporter"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\nfilebeat版本：7.17.29\nelasticsearch版本： 7.15.0\ngraph LR\rA\u003e\"日志\"] A -.-\u003e|服务访问日志| B[\"/var/log/nginx/access.log\"]\rA -.-\u003e|错误日志| c[\"/var/log/nginx/error.log\"] 配置nginx访问日志格式为json\n位置 http\nlog_format custom_json escape=json '{' '\"level\":\"info\",' '\"ts\": \"$time_iso8601\",' '\"message\": \"handled request $request_method $request_uri\",' '\"request\": {' '\"id\": \"$http_x_request_id\",' '\"contry_code\": \"$geoip_country_code\",' '\"remote_ip\": \"$remote_addr\",' '\"remote_port\": \"$remote_port\",' '\"protocol\": \"$server_protocol\",' '\"method\": \"$request_method\",' '\"host\": \"$host\",' '\"uri\": \"$request_uri\",' '\"headers\": {' '\"user-agent\": \"$http_user_agent\",' '\"accept\": \"$http_accept\",' '\"accept-encoding\": \"$http_accept_encoding\",' '\"traceparent\": \"$http_traceparent\",' '\"tracestate\": \"$http_tracestate\"' '}' '},' '\"bytes_read\": $request_length,' '\"duration_msecs\": $request_time,' '\"size\": $bytes_sent,' '\"status\": $status,' '\"resp_headers\": {' '\"content_length\": \"$sent_http_content_length\",' '\"content_type\": \"$sent_http_content_type\"' '}' '}'; 引用该格式日志: 引用日志格式 按照条件记录日志 禁用日志 access_log /var/log/nginx/access.log custom_json; 语法 : access_log /path/to/access.log custom_json if=\u003ccondition\u003e;\nloggable 为true 是记录日志，2xx， 3xx 不记录日志\nhttp { map $status $loggable { ~^[23] 0; # Match 2xx and 3xx status codes default 1; # Log everything else } access_log /var/log/nginx/access.log combined if=loggable; } access_log off; 或者\naccess_log /dev/null; 使用filbeat收集日志，关键参数 json.keys_under_root: true\ncat \u003e/data/elk/filebeat/filebeat.yml\u003c\u003c'EOF' filebeat.inputs: - type: log id: accesslog paths: - /var/log/nginx/access* tags: - \"nginx-access\" # 将message中json格式日志解析出来，如果没有改配置日志文件将以字符串格式保存在message中 # https://www.elastic.co/docs/reference/beats/filebeat/filebeat-input-log#filebeat-input-log-config-json json.keys_under_root: true - type: log id: errlog paths: - /var/log/nginx/error* include_lines: - \"emerg\" - \"error\" tags: - \"nginx-error\" output.elasticsearch: hosts: [\"192.168.0.114:9200\"] indices: - index: \"nginx-access-%{+yyyy.MM.dd}\" when: contains: tags: \"nginx-access\" - index: \"nginx-error-%{+yyyy.MM.dd}\" when: contains: tags: \"nginx-error\" # 禁用索引生命周期管理，默认为true。在启用 setup.ilm.enabled 状态下setup.template 配置不会生效 setup.ilm.enabled: false # 加载索引模版，默认为true setup.template.enabled: true # 设置索引模板的名称，默认值为 filebeat-%{[agent.version]} setup.template.name: \"nginx\" # 设置索引模板的匹配模式,默认值为 filebeat-%{[agent.version]}-* setup.template.pattern: \"nginx-*\" # 覆盖已有的索引模板，默认值为false setup.template.overwrite: true # 配置索引模板属性 setup.template.settings: # 设置索引分片数量 index.number_of_shards: 3 # 设置索引副本数量，要求小于集群的数量 index.number_of_replicas: 0 EOF ","categories":["ELK"],"description":"\n使用filebeat收集nginx日志\r\n","excerpt":"\n使用filebeat收集nginx日志\r\n","ref":"/elk/example/nginx.html","tags":["nginx","filebeat"],"title":"收集nginx日志"},{"body":" InnoDB 优先用 --single-transaction，避免锁表影响业务。 MyISAM 必须锁表（默认行为），需在低峰期操作。 大表备份：结合 --quick 和输出压缩（如 gzip）。 --备份前先执行flush tables 将内容写入磁盘（mysql必知必会中提到） flush tables ; 逻辑备份: 备份所有库 备份指定库 创建备份用户\ncreate user 'backup'@'localhost' identified by 'd988ec6a93!@#MYSQL'; GRANT SELECT, SHOW VIEW, RELOAD, LOCK TABLES, PROCESS ON *.* TO 'backup'@'localhost'; flush privileges; 备份命令\n# 备份所有数据库 /usr/local/mysql80/bin/mysqldump \\ -A \\ --single-transaction --master-data=2 \\ -u root -p'd988ec6a93!@#MYSQL' -h 10.128.99.157 -P 6301 \\ |gzip \u003e/Backup/fullbackup`date +%F-%H:%M:%S`.sql.gz 创建备份用户\ncreate user 'backup'@'localhost' identified by 'd988ec6a93!@#MYSQL'; GRANT SELECT, SHOW VIEW, RELOAD, LOCK TABLES, PROCESS ON *.* TO 'backup'@'localhost'; flush privileges; 备份命令\n# 备份指定数据库 /usr/local/mysql80/bin/mysqldump \\ -B infra_settle \\ --single-transaction --master-data=2 \\ -u root -p'd988ec6a93!@#MYSQL' -h 10.128.99.157 -P 6301 \\ |gzip \u003e/Backup/fullbackup`date +%F-%H:%M:%S`.sql.gz 参数 说明 示例 -u, --user 指定用户名 -u root -p, --password 密码（建议不加密码，后续输入） -p 或 -p'123' -h, --host MySQL 服务器地址 -h 127.0.0.1 -P, --port 端口号（默认3306） -P 6301 --socket 指定 Unix Socket 文件路径 --socket=/tmp/mysql.sock -A, --all-databases 备份所有数据库 mysqldump -A \u003e full_backup.sql -B, --databases 备份指定数据库（可多个） -B db1 db2 --tables 指定表（需先指定数据库） dbname --tables table1 table2 --ignore-table 排除指定表 --ignore-table=db.logs --no-data 仅备份表结构，不备份数据 --no-data --no-create-info 仅备份数据，不备份表结构 --no-create-info --routines 包含存储过程和函数 --routines --triggers 包含触发器 --triggers --events 包含事件调度器 --events --skip-triggers 排除触发器 --skip-triggers --single-transaction 开启事务保证 InnoDB 一致性快照 InnoDB --lock-tables 备份前锁定所有表（默认开启） MyISAM --skip-lock-tables 不锁表（可能导致不一致） 紧急情况 --flush-logs 备份前刷新日志（用于 Binlog 增量恢复） 所有引擎 --master-data 记录 Binlog 位置（=1注释，=2SQL语句） 主从复制 --result-file 指定输出文件（避免 Windows 换行问题） --result-file=backup.sql --compress 压缩传输（节省网络带宽） --compress --tz-utc 统一时区为 UTC（默认开启） --tz-utc --hex-blob 二进制数据以十六进制导出 --hex-blob --quick 逐行导出（避免内存溢出） 大表备份 --extended-insert 合并多行 INSERT（默认开启） 减少备份体积 --skip-extended-insert 每行单独 INSERT（可读性高） 调试用途 --opt 启用所有优化选项（默认开启） 综合优化 备份脚本 Details #!/bin/bash # 脚本名称: mysql_backup.sh # 描述: MySQL数据库备份脚本，支持全库备份、binlog同步及过期清理 # 作者: wangendao(1209233066@qq.com) # 创建日期: 2019/03/01 # 最后修改日期: $(date +%F) # 调试选项 (取消注释以启用) # set -euo pipefail # 更严格的错误检查 # set -x # 显示执行过程 # 配置变量 export PATH=/application/mysql5.6/bin:$PATH datadir=/application/mysql5.6/ backupdir=/backup logdir=/var/log/mysql_backup timestamp=$(date +%F_%H%M%S) user=root password='123' socket=/application/mysql5.6/mysql.sock rsync_ip=168.204.37.25 retention_days=7 # 备份保留天数 # 创建必要的目录 mkdir -p \"$backupdir\" \"$logdir\" # 日志函数 log() { echo \"[$(date '+%F %T')] $1\" \u003e\u003e \"$logdir/mysql_backup_${timestamp}.log\" } # 错误处理函数 error_exit() { log \"ERROR: $1\" sendEmail -f wangendao@crv.com.cn -t wangendao@crv.com.cn \\ -s 10.248.2.15 -u \"MySQL备份失败警报\" \\ -o tls=no -o message-content-type=html \\ -o message-charset=utf8 -xu wangendao@crv.com.cn \\ -xp Wed10203040 -m \"MySQL备份失败: $1\" exit 1 } # 获取数据库列表 DBlist=($(mysql -u \"${user}\" -p\"${password}\" -S \"${socket}\" \\ -e \"SHOW DATABASES;\" | \\ awk '!/^(information_schema|performance_schema|test|database|mysql)$/' | \\ tail -n +2)) if [ ${#DBlist[@]} -eq 0 ]; then error_exit \"未获取到数据库列表\" fi # 备份每个数据库 for db in \"${DBlist[@]}\"; do backup_file=\"${backupdir}/${db}_${timestamp}.gz\" md5_file=\"${backupdir}/${db}_${timestamp}.md5\" log \"开始备份数据库: $db\" if ! mysqldump -B \"$db\" \\ -u\"${user}\" -p\"${password}\" -S \"${socket}\" \\ --master-data=2 \\ --single-transaction \\ --routines \\ --triggers | gzip \u003e \"$backup_file\"; then error_exit \"数据库 $db 备份失败\" fi # 生成MD5校验文件 if ! md5sum \"$backup_file\" \u003e \"$md5_file\"; then error_exit \"无法生成MD5校验文件\" fi log \"数据库 $db 备份完成, 文件: $backup_file\" done # 同步备份文件 log \"开始同步备份文件到远程服务器\" if ! rsync -az --delete \"${backupdir}/\" \"rsync@${rsync_ip}::ftp\" \\ --password-file=/etc/rsync.password \u003e\u003e \"$logdir/rsync.log\" 2\u003e\u00261; then error_exit \"备份文件同步失败\" fi # 同步binlog文件 log \"开始同步binlog文件\" if ! rsync -az \"${datadir}/\" --include 'mysql-bin*' --exclude '*' \\ \"rsync@${rsync_ip}::ftp\" --password-file=/etc/rsync.password \\ \u003e\u003e \"$logdir/binlog_sync.log\" 2\u003e\u00261; then error_exit \"binlog文件同步失败\" fi # 清理过期备份 log \"清理${retention_days}天前的备份\" find \"$backupdir\" -type f -name \"*.gz\" -mtime +\"$retention_days\" -delete find \"$backupdir\" -type f -name \"*.md5\" -mtime +\"$retention_days\" -delete # 发送成功通知 log \"备份任务完成\" sendEmail -f wangendao@crv.com.cn -t wangendao@crv.com.cn \\ -s 10.248.2.15 -u \"MySQL备份成功通知\" \\ -o tls=no -o message-content-type=html \\ -o message-charset=utf8 -xu wangendao@crv.com.cn \\ -xp Wed10203040 -m \"MySQL备份任务已完成\u003cbr\u003e\u003cbr\u003e备份数据库列表:\u003cbr\u003e${DBlist[*]}\u003cbr\u003e\u003cbr\u003e备份文件已保留最近${retention_days}天\" ","categories":["mysql"],"description":"mysqldump逻辑备份工具\n","excerpt":"mysqldump逻辑备份工具\n","ref":"/mysql/mysqldump.html","tags":["备份","mysqldump"],"title":"mysqldump"},{"body":" 功能 命令 列出虚拟机 vim-cmd vmsvc/getallvms 强制关闭虚拟机 vim-cmd vmsvc/power.off \u003cVmid\u003e 正常关闭 vim-cmd vmsvc/power.shutdown \u003cVmid\u003e 重启 vim-cmd vmsvc/power.reset \u003cVmid\u003e 启动 vim-cmd vmsvc/power.on\u003cVmid\u003e 查看电源状态 vim-cmd vmsvc/power.getstate \u003cVmid\u003e 获取完整配置 vim-cmd vmsvc/get.config \u003cVmid\u003e 查看摘要信息 vim-cmd vmsvc/get.summary \u003cVmid\u003e 获取网络配置 vim-cmd vmsvc/get.networks \u003cVmid\u003e 创建快照 vim-cmd vmsvc/snapshot.create \u003cVmid\u003e \"Snapshot Name\" \"Description\" 列出快照 vim-cmd vmsvc/snapshot.get \u003cVmid\u003e 恢复快照 vim-cmd vmsvc/snapshot.revert \u003cVmid\u003e \u003cSnapshotId\u003e 删除快照 vim-cmd vmsvc/snapshot.remove \u003cVmid\u003e \u003cSnapshotId\u003e 注册新虚拟机 vim-cmd solo/registervm \u003c/vmfs/volumes/datastore1/Clone_VM_DIR/VM_NAME.vmx\u003e 取消注册 vim-cmd vmsvc/unregister 54 new_vm=node-1-s3 template_vm=template_vm # 创建新虚拟机目录 mkdir /vmfs/volumes/data/${new_vm}/ # 复制虚拟机文件 find /vmfs/volumes/data/${template_vm}/ -type f ! -name *.log -exec cp {} /vmfs/volumes/data/${new_vm}/ \\; # 修改虚拟机配置文件 sed -i -e \"/^displayName/c\\displayName = \\\"${new_vm}\\\"\" \\ -e '/^uuid\\.bios/c\\uuid.bios = \"\"' \\ -e '/^uuid\\.location/c\\uuid.location = \"\"' \\ -e '/^ethernet0\\.generatedAddress =/c\\ethernet0.generatedAddress = \"\"' \\ -e '/swap/d' \\ -e \"/^numvcpus/c\\numvcpus = 4\" \\ -e \"/^memSize/c\\memSize = 8192\" \\ /vmfs/volumes/data/${new_vm}/${template_vm}.vmx # 注册虚拟机 id=$(vim-cmd solo/registervm /vmfs/volumes/data/${new_vm}/${template_vm}.vmx) vim-cmd vmsvc/power.on ${id} vim-cmd vmsvc/power.getstate ${id} tidb 导出数据 dm 和 dumping\npump 组件的功能\ntiup dmctl 命令的使用\nbr backup 命令的使用\n# 创建新虚拟机目录 mkdir /vmfs/volumes/data/mongodb-uat02-s3/ cp -a /vmfs/volumes/data/mongodb-uat01-s3/* /vmfs/volumes/data/mongodb-uat02-s3/ mkdir /vmfs/volumes/data/mongodb-uat03-s3/ cp -a /vmfs/volumes/data/mongodb-uat01-s3/* /vmfs/volumes/data/mongodb-uat03-s3/ # 复制所有文件到克隆目录 find . -type f -exec cp --parents \"{}\" /vmfs/volumes/66f36c11-47342fc7-cae6-246e96cd12a4/vm-clone-49 \\; # 编辑克隆机的VMX文件 vi /vmfs/volumes/66f36c11-47342fc7-cae6-246e96cd12a4/vm-clone-49/源虚拟机名.vmx # 修改以下关键参数： displayName = \"克隆虚拟机名\" # 更改虚拟机名称 uuid.bios = \"\" # 清空BIOS UUID ethernet0.generatedAddress = \"\" # 清空MAC地址 scsi0:0.fileName = \"克隆目录名/磁盘名.vmdk\" # 更新磁盘路径 vim-cmd 主要命令空间解析 命令空间 功能描述 常用场景 vmsvc/ 虚拟机生命周期管理 启动/停止/重启/迁移虚拟机 hostsvc/ 主机系统管理 主机维护模式、服务控制 hbrsvc/ 基于主机的复制 虚拟机备份与复制 proxysvc/ 代理服务 网络和连接管理 internalsvc/ 内部服务 高级调试和诊断 solo/ 独立操作 特定硬件操作 vimsvc/ vCenter 服务 集群和高级功能 ","categories":["exis"],"description":"通过命令行工具vim-cmd 对虚拟机启动、停止、克隆、销毁\n","excerpt":"通过命令行工具vim-cmd 对虚拟机启动、停止、克隆、销毁\n","ref":"/2025-07-09/vim-cmd.html","tags":["vim-cmd"],"title":"exsi命令行工具"},{"body":" 前言 · ELKstack 中文指南 老男孩教育倾向于运维 尚硅谷讲解通过restful操作elasticsearch 07.Elasticsearch快速入门之基本概念_哔哩哔哩_bilibili 千锋教育ElasticSearch教程（深入讲解,一套掌握）_哔哩哔哩_bilibili ElasticSearch面试50问保姆级教程入门到精通（基于ELK技术栈elasticsearch 7.17.3最新教程通俗易懂）_哔哩哔哩_bilibili 马士兵教育ELK保姆级教程，从入门到精通！ElasticSearch+Logstash+Kibana精讲_哔哩哔哩_bilibili 【全网最细】耗时72小时整理的ElasticSearch教程入门到精通，让你少走99%的弯路！！_哔哩哔哩_bilibili 各组件兼容性： https://www.elastic.co/cn/support/matrix#matrix_compatibility 操作系统兼容性：https://www.elastic.co/cn/support/matrix 龙腾出行公司elasticsearch版本： 7.17.3\n日志量每天1.5TB\n面试题 使用的什么版本 日志量 遇到过哪些坑 elasticsearch 集群选主、数据复制原理 ","categories":"","description":"","excerpt":" 前言 · ELKstack 中文指南 老男孩教育倾向于运维 尚硅谷讲解通过restful操作elasticsearch 07.Elasticsearch快速入门之基本概念_哔哩哔哩_bilibili 千锋教育ElasticSearch教程（深入讲解,一套掌握）_哔哩哔哩_bilibili ElasticSearch面试50问保姆级教程入门到精通（基于ELK技术栈elasticsearch …","ref":"/docs/database/elasticsearch/","tags":"","title":""},{"body":"通常数据库需要服务器提供低延迟、高i/o的能力\n开启服务器的性能优化\necho performance | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor ","categories":"","description":"\ndatabase 文档中心\r\n","excerpt":"\ndatabase 文档中心\r\n","ref":"/docs/database/","tags":"","title":""},{"body":"\njava版本 zookeeper版本 kafka版本 java8 0.7 至 3.0 java11 0.7 至 3.7 java17 0.7 至 尚硅谷_Kafka入门_\nkafka可视化管理和监控工具EFAK，同样是使用jmx抓取数据\n","categories":"","description":"","excerpt":"\njava版本 zookeeper版本 kafka版本 java8 0.7 至 3.0 java11 0.7 至 3.7 java17 0.7 至 尚硅谷_Kafka入门_\nkafka可视化管理和监控工具EFAK，同样是使用jmx抓取数据\n","ref":"/docs/database/kafka/","tags":"","title":""},{"body":"任务需求：\n为开发人员设置k8s集群权限\n设计思路：\n将开发人员按照产品线分成小组，授予小组对应权限，开发人员加入到对应组实现授权。\ngraph TB\rdev01 -.-\u003e 开发组1\rdev02 -.-\u003e 开发组1\r开发组1 -.-\u003e |clusterrolebinding|clusterrole01\rsubgraph 权限\rclusterrole01\rclusterrole02\rend\rsubgraph 组\r开发组1\r开发组2\rend\rsubgraph 用户\rdev01\rdev02\rend 功能实现：\n允许同一用户加入多个组 用户权限回收 人员异动，权限调整 实现操作审计（k8s审计日志） 实施步骤：\n设计clusterrole 权限 设计组，把clusterrole 绑定到组中 新增用户，并生成kubeconfig文件 用户权限的回收【停用】 人员异动权限调整【调整用户所属组，无法做到动态调整只能停用用户重新创建】 实践案例：\n该案例用于理清思路，并不实用\n设计clusterrole 权限\n使用默认权限 cluster:admin\n设计组，把clusterrole 绑定到组中\n使用默认组 system:masters\n新增用户，并生成kubeconfig文件\nopenssl genrsa -out user01.key 1024 openssl req -new -key user01.key -out user01.csr -subj /CN=user01/O=system:masters openssl x509 -req -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -in user01.csr -out user01.crt -days 3650 -CAcreateserial openssl x509 -in user01.crt -text # 设置集群信息 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/pki/ca.crt \\ --embed-certs=true \\ --server=https://10.4.7.10:6443 \\ --kubeconfig=user01.kubeconfig # 设置用户信息 kubectl config set-credentials user01 \\ --client-certificate=./user01.crt \\ --client-key=./user01.key \\ --embed-certs=true \\ --kubeconfig=user01.kubeconfig # 把集权和用户信息绑定 kubectl config set-context user01@kubernetes \\ --cluster=kubernetes \\ --user=user01 \\ --kubeconfig=user01.kubeconfig # current-context kubectl config use-context user01@kubernetes \\ --kubeconfig=user01.kubeconfig root@master01:~/user01# kubectl get nodes --kubeconfig=./user01.kubeconfig NAME STATUS ROLES AGE VERSION master01 Ready control-plane 22d v1.28.2 以上已经完成了用户的新增及授权动作，为了便于理解我们再次新增用户user02\nopenssl genrsa -out user02.key 1024 openssl req -new -key user02.key -out user01.csr -subj /CN=user02/O=system:masters openssl x509 -req -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -in user01.csr -out user02.crt -days 3650 -CAcreateserial openssl x509 -in user02.crt -text # 设置用户信息 kubectl config set-credentials user02 \\ --client-certificate=./user02.crt \\ --client-key=./user02.key \\ --embed-certs=true \\ --kubeconfig=user01.kubeconfig # 把集权和用户信息绑定 kubectl config set-context user02@kubernetes \\ --cluster=kubernetes \\ --user=user02 \\ --kubeconfig=user01.kubeconfig kubectl config use-context user02@kubernetes --kubeconfig=./user01.kubeconfig root@master01:~/user01# kubectl get nodes --kubeconfig=./user01.kubeconfig NAME STATUS ROLES AGE VERSION master01 Ready control-plane 22d v1.28.2 以上创建的过程比较复杂，kubectl的插件功能实现了功能的简化：\n希望实现的调用方式：\nkubectl rbac --user=user01 --clusterrole=cluser-admin --kubeconfigfile=user01.kubeconfig cat /usr/local/bin/kubectl-rbac #!/bin/bash createuser(){ openssl genrsa -out user01.key 1024 openssl req -new -key user01.key -out user01.csr -subj /CN=user02/O=system:masters openssl x509 -req -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -in user01.csr -out user01.crt -days 3650 -CAcreateserial } createkubeconfig(){ # 设置集群信息 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/pki/ca.crt \\ --embed-certs=true \\ --server=https://10.4.7.10:6443 \\ --kubeconfig=user01.kubeconfig # 设置用户信息 kubectl config set-credentials user01 \\ --client-certificate=user01.crt \\ --client-key=user01.key \\ --embed-certs=true \\ --kubeconfig=user01.kubeconfig # 把集权和用户信息绑定 kubectl config set-context user01@kubernetes \\ --cluster=kubernetes \\ --user=user01 \\ --kubeconfig=user01.kubeconfig # current-context kubectl config use-context user01@kubernetes \\ --kubeconfig=user01.kubeconfig } check(){ kubectl get nodes --kubeconfig=./user01.kubeconfig } createuser createkubeconfig check ","categories":["kubernetes"],"description":"\nk8s RBAC实践\r\n","excerpt":"\nk8s RBAC实践\r\n","ref":"/kubernetes/rbac.html","tags":["kubernetes","RBAC"],"title":"RBAC实践"},{"body":"promQL 是prometheus时序数据库的查询语言，可以类比为关系数据库中的SQL。\n指标类型 gauge: 是一个状态的瞬时快照，值可增可减。配合 changes delta predict_linear deriv\ncounter: 是一个累加的指标，自metrics启动以后该值一直累加。 配合 rate irate increase\nsummary: 摘要可以展示数据分布 ,允许计算百分位数。 配合 count sum quantitle\nhistogram: 直方图可以展示数据分布，按照桶把数据分布投递到不同桶中。直方图的数值是累计的，后一个包含前一个桶中的数据。配合 histogram_quantile\n例如：histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))\n查询结果数据类型 即时矢量: 一组时间序列，其中包含每个时间序列的单个样本，所有时间序列共享相同的时间戳\n范围向量: 一组时间序列，其中包含每个时间序列随时间变化的数据点范围.\n例如：\n[5m]表示的时间范围是5分钟。 s表示秒 m表示分钟 h表示小时 d表示天 w表示周\n标量: 一个简单的数字浮点值\n字符串: 一个简单的字符串值;\n查询语句 基本语法: 示例格式一 示例格式二 示例格式三 示例格式四 http_requests_total{code=\"200\",method=\"get\"} != =~ 选择正则表达式与提供的字符串匹配的指标 !~ 选择正则表达式与提供的字符串不匹配的指标 通过label查询\n{__name__=\"http_requests_total\"} {__name__=~\".*total$\"} {job=~\".+\"} {job=~\".*\",method=\"get\"} {__name__=\"http_requests_total\"}[1m] ms s m h d w y 时间偏移量\nhttp_requests_total @1609746000 sum(http_requests_total{method=\"GET\"} @1609746000) http_requests_total offset 5m sum(http_requests_total{method=\"GET\"} offset 5m) 标签表达式\n表达式 释义 示例 = 精确匹配给定的值 != 精确不匹配给定的值 =~ 正则匹配给定的值 `http_requests_total{environment=~“staging !~ 正则不匹配给定的值 运算符：\n运算符: 算术运算 比较运算符 逻辑运算符 算术运算\n作用范围：（浮点数与浮点数 浮点数与瞬时向量 瞬时向量与瞬时向量），瞬时向量间标签k,v必须保持完全一致\n运算符 功能 示例 + 添加 - 减法 * 乘法 / 取商 % 取余数 ^ 幂 比较运算符\n作用范围：（浮点数与浮点数 浮点数与瞬时向量 瞬时向量与瞬时向量）\n两个标量之间必须使用 bool 装饰器进行比较，返回0(flase) 或 1(true)\n运算符 功能 示例 == 等于 != 不相等 \u003e 大于 \u003c 小于 \u003e= 大于等于 \u003c= 小于等于 逻辑运算符\n逻辑运算两边的标签保持一致\n运算符 功能 示例 and 交集 or 并集 unless 补集,左侧有右侧没有的 聚合操作：\n基于标签聚合 分类 运算符 功能 示例 聚合操作分组 by sum by(code) (method_code:http_errors:rate5m) without sum without(code) (method_code:http_errors:rate5m) 聚合操作符 sum 求和 count 计数 avg 平均值 max 最大值 min 最小值 topk 按样本值计算的最大 k 个元素 topk(2,method_code:http_errors:rate5m) bottomk 按样本值排列的最小 k 个元素 quantile 计算φ分位数（0 ≤ φ ≤ 1）的尺寸 count_values 计算具有相同值的元素数 stddev 标准偏差 stdvar 标准与维度的方差 group 结果向量中的所有值均为 1 基于时间聚合\n_over_time 可以做到保留标签\n向量匹配\none-to-one many-to-one and group_left many-to-many 向量匹配\n必须具有相同的标签，如果一方标签过多可以使max by 保留指定标签\n--collector.textfile --collector.textfile.directory=\".\" vi httpcod.prom #输入示例： method_code:http_errors:rate5m{method=\"get\", code=\"500\"} 24 method_code:http_errors:rate5m{method=\"get\", code=\"404\"} 30 method_code:http_errors:rate5m{method=\"put\", code=\"501\"} 3 method_code:http_errors:rate5m{method=\"post\", code=\"500\"} 6 method_code:http_errors:rate5m{method=\"post\", code=\"404\"} 21 method:http_requests:rate5m{method=\"get\"} 600 method:http_requests:rate5m{method=\"del\"} 34 method:http_requests:rate5m{method=\"post\"} 120 One-to-one: 两侧如果具有相同的标签和键 =\u003e 则匹配。on 在指定标签上进行匹配，ignoring则忽略指定标签\n语法：\n\u003cvector expr\u003e \u003cbin-op\u003e ignoring(\u003clabel list\u003e) \u003cvector expr\u003e \u003cvector expr\u003e \u003cbin-op\u003e on(\u003clabel list\u003e) \u003cvector expr\u003e 事例:\n# method_code:http_errors:rate5m{method=\"get\", code=\"500\"} 24 # method_code:http_errors:rate5m{method=\"post\", code=\"500\"} 6 # method:http_requests:rate5m{method=\"get\"} 600 # method:http_requests:rate5m{method=\"post\"} 120 method_code:http_errors:rate5m{code=\"500\"} / ignoring(code) method:http_requests:rate5m 结果\n{method=\"get\"} 0.04 // 24 / 600 {method=\"post\"} 0.05 // 6 / 120 Many-to-one and one-to-many : 在向量的一侧可以找到多个与之对应的记录\n少的一次都能在多的一侧找到对应的 指标。\n使用 group_left 和 group_right 指定哪一侧位多的一侧\n使用on 和 ignore 指定参照哪个标签进行运算\n语法：\n\u003cvector expr\u003e \u003cbin-op\u003e ignoring(\u003clabel list\u003e) group_left(\u003clabel list\u003e) \u003cvector expr\u003e \u003cvector expr\u003e \u003cbin-op\u003e ignoring(\u003clabel list\u003e) group_right(\u003clabel list\u003e) \u003cvector expr\u003e \u003cvector expr\u003e \u003cbin-op\u003e on(\u003clabel list\u003e) group_left(\u003clabel list\u003e) \u003cvector expr\u003e \u003cvector expr\u003e \u003cbin-op\u003e on(\u003clabel list\u003e) group_right(\u003clabel list\u003e) \u003cvector expr\u003e 事例:\n#group_left 和 group_right 参照 one-to-many 的many 一侧 method_code:http_errors:rate5m / ignoring(code) group_left method:http_requests:rate5m 结果\n{method=\"get\", code=\"500\"} 0.04 // 24 / 600 {method=\"get\", code=\"404\"} 0.05 // 30 / 600 {method=\"post\", code=\"500\"} 0.05 // 6 / 120 {method=\"post\", code=\"404\"} 0.175 // 21 / 120 函数 up target是否存活 absent 即时向量指标存在什么都不返回，不存在返回1 abs 绝对值 absent_over_time 用于判断范围向量absent_over_time(up1[10m]) ceil 四舍五入 floor 舍小数取整 time 返回时间戳 changes 记录变化的次数，比如一个任务从up 变为down 后又变为up 此时应该返回2 changes(up[5m])\u003e0 increase【用于counter】 截取 counter 类型一个时间段的增量 increase(node_cpu_secound_total[1m]) irate 【用于counter】 每秒变化量 rate 【用于counter】 每秒变化量 sort sort_desc topk bottomk delta 截取 counter 类型一个时间段的增量 delta(node_cpu_seconds_total[10m]) last_over_time 获取指定时间范围内最后一个非空值 last_over_time(stock_price_change[60m]) label_replace\n替换标签和内容\n语法\nlabel_replace(__input_vector__,\"__dst__\",\"__replacement__\",\"__src__\",\"__regex__\") 举例\nlabel_replace(lv_pool_total,\"lv_name_new\",\"$1\",\"lv_name\",\"(.*)\") label_join\n语法\n举例\n","categories":["prometheus","监控"],"description":"\nPromeQL|prometheus\r\n\r\n","excerpt":"\nPromeQL|prometheus\r\n\r\n","ref":"/prometheus/promeql.html","tags":["prometheus","PromeQL"],"title":"PromeQL"},{"body":"time 时间操作 获取当前时间、时间格式化、字符串解析\npackage main import ( \"fmt\" \"time\" ) func main() { // 获取当前时间 now := time.Now() fmt.Println(now) //2024-05-12 16:03:54.379957201 +0800 CST m=+0.000041572 // 获取指定时间 年 月 日 时 分 秒，纳秒， 时区 t1 := time.Date(2024, 5, 12, 16, 07, 19, 0, time.Local) fmt.Println(t1) //2024-05-12 16:07:19 +0800 CST // 时间的格式化 Format() t2 := time.Now().Format(\"2006/01/02 15:04:05 PM\") fmt.Println(t2) //2024/05/12 16:21:02 PM // 解析 Parse() if t3, err := time.Parse(\"2006-01-02 15:04:05\", \"2024-05-12 16:03:54\"); err != nil { fmt.Println(err) } else { fmt.Println(t3) //2024-05-12 16:03:54 +0000 UTC } // sleep() for i := 1; i \u003c 10; i++ { fmt.Println(i) time.Sleep(10 * time.Second) } } ","categories":["golang"],"description":"time|标准库\n","excerpt":"time|标准库\n","ref":"/golang/package/time.html","tags":["golang","time"],"title":"time"},{"body":"函数 函数定义基本语法 func func_name(params) (return_types) { } func 关键字表示定义一个函数 func_name 函数名称 params 形参 return_types 返回值类型 示例：\npackage main import \"fmt\" func main() { sum := add(1, 2) fmt.Println(sum) //3 } func add(a, b int) int { return a + b } 基本函数 带参数的函数 值传递和引用传递 返回值的函数 package main import \"fmt\" func main() { add() } func add() { fmt.Println(1 + 2) } package main import \"fmt\" func main() { add(1, 2) } func add(a, b float32) { fmt.Println(a + b) } // 可变参数 package main import \"fmt\" func main() { add(1, 2, 3, 4, 5) } func add(num ...int) { sum := 0 for _, v := range num { sum += v } fmt.Println(sum) } // 可变参数 package main import \"fmt\" func main() { add([]int{1, 2, 3, 4, 5}) } func add(num []int) { sum := 0 for _, v := range num { sum += v } fmt.Println(sum) } //值传递，复制变量值给函数，不会影响原有变量 package main import \"fmt\" func main() { num1 := 1 num2 := 2 swap(num1, num2) fmt.Printf(\"num1的值：%d\\nnum2的值：%d\\n\", num1, num2) // num1的值：1 // num2的值：2 } func swap(a, b int) { a, b = b, a } //引用传递，会修改原有变量 package main import \"fmt\" func main() { num1 := 1 num2 := 2 swap(\u0026num1, \u0026num2) fmt.Printf(\"num1的值：%d\\nnum2的值：%d\\n\", num1, num2) // num1的值：2 // num2的值：1 } func swap(a, b *int) { *a, *b = *b, *a } package main import \"fmt\" func main() { result, mesg := add(1, 2, 3, 4, 56) fmt.Println(result, mesg) } func add(num ...int) (int, string) { sum := 0 for _, v := range num { sum += v } return sum, \"计算完成\" } package main import ( \"fmt\" ) func main() { result, mesg := add(1, 2, 3, 4, 56) fmt.Println(result, mesg) } func add(num ...int) (sum int, mesg string) { for _, v := range num { sum += v } mesg = \"计算完成\" return } 递归函数 函数体内调用函数自身 称为递归函数\npackage main import \"fmt\" func main() { // 斐波那契数列是指这样一个数列：1，1，2，3，5，8，13，21，34，55，89……这个数列从第3项开始 ，每一项都等于前两项之和 for i := 1; i \u003c= 10; i++ { fmt.Println(sum(i)) } } // 求第n项 func sum(n int) int { if n \u003c= 2 { return 1 } else { return sum(n-1) + sum(n-2) } } package main import \"fmt\" func main() { for i := 1; i \u003c= 10; i++ { fmt.Println(test4(i)) } } // 递归函数 func test4(a int) int { if a == 1 { return 1 } return test4(a-1) + a } 匿名函数 函数定义基本语法\n// 省略了函数的名称 func(params) (return_types) { } func 关键字表示定义一个函数 params 形参 return_types 返回值类型 示例：\npackage main import \"fmt\" func main() { result, mesg := func(num ...int) (int, string) { sum := 0 for _, v := range num { sum += v } return sum, \"计算完毕\" }(1, 2, 3, 4, 5) fmt.Println(result, mesg) } 将匿名函数赋值给一个变量\npackage main import \"fmt\" func main() { f1 := func(num ...int) (sum int, mesg string) { for _, v := range num { sum += v } mesg = \"计算完毕\" return } result, mesg := f1(1, 2, 3, 4, 5) fmt.Println(result, mesg) } 回调函数 利用匿名函数实现回调\npackage main import \"fmt\" func main() { operator(func() { fmt.Println(\"hello world\") }) } func operator(f func()) { // f func() 表示 f 变量是一个无参数，无返回值的函数 f() // 在此处回调了函数f } 带参数的回调函数\npackage main import \"fmt\" func main() { // 把函数作为参数,回调函数 add := func(a, b int) int { return a + b } fmt.Print( operator(1, 2, add), ) } func operator(a, b int, fun func(int, int) int) int { result := fun(a, b) return result } package main import \"fmt\" func main() { // 把函数作为参数,回调函数 fmt.Print( operator(1, 2, func(a, b int) int { return a + b }), ) } func operator(a, b int, fun func(int, int) int) int { result := fun(a, b) return result } 闭包函数 把函数作为返回值，这个函数保存了变量不被释放\n利用匿名函数实现闭包\npackage main import \"fmt\" func main() { result := add() result() } func add() func() { return func() { fmt.Println(123) } } package main import \"fmt\" func main() { // add 函数已经结束，但是result 保留了变量num的变量值 result := add(123) for i := 0; i \u003c 10; i++ { result() // 124 // 125 // 126 // 127 // 128 // 129 // 130 // 131 // 132 // 133 } } func add(num int) func() { return func() { num++ fmt.Println(num) } } 函数延迟调用 defer 声明位置，暂存当前变量状态。类似一个快照\npackage main import \"fmt\" func main() { a := 1 defer fmt.Println(a) //1 a++ fmt.Println(a) //2 } 多个defer 函数按照顺序逆序执行，暂存defer任务的地方类似栈\npackage main func main() { a := []string{\"1\", \"2\", \"3\"} for _, v := range a { defer println(v) // 3 // 2 // 1 } } 冒泡排序函数示例 package main import \"fmt\" func main() { result := sort([]string{\"3\", \"0\", \"9\", \"1\", \"4\"}) fmt.Println(result) //[0 1 3 4 9] } // 一个冒泡比较的函数 func sort(a []string) []string { for i := 1; i \u003c len(a); i++ { //比较的轮数 for j := 0; j \u003c len(a)-i; j++ { // 每轮比较的次数 if a[j] \u003e a[j+1] { a[j], a[j+1] = a[j+1], a[j] } } } return a } ","categories":["golang"],"description":"go|函数\n","excerpt":"go|函数\n","ref":"/golang/function.html","tags":["golang","函数"],"title":"函数"},{"body":" 操作系统： Ubuntu 22.04.4 5.15.0-160-generic\ncpu型号：Intel(R) Xeon(R) CPU E5-2699 v3 @ 2.30GHz\npython版本：3.8+\nqemu版本：v8.2.10\nqemu虚拟机操作系统: https://mirror.lzu.edu.cn/debian-cd/13.1.0/amd64/iso-dvd/debian-13.1.0-amd64-DVD-1.iso\n问题： 如何监控vf的分配情况\n网络架构 ubuntu 网络配置 外部网络 (192.168.0.0/24) │ ├── ESXi 主机 │ │ │ └── Ubuntu VM │ │ │ ├── br0 桥接 (192.168.0.116) │ │ ├── ens34 (物理接口) │ │ └── tap0 (QEMU 连接) │ │ │ └── Debian VM (QEMU) │ └── 6个 VF (包括 enp1s0v6: 192.168.0.121) root@qemu-ubuntu:~# cat /etc/netplan/00-installer-config.yaml # This is the network config written by 'subiquity' network: version: 2 ethernets: ens34: {} bridges: br0: interfaces: [ens34] dhcp4: false addresses: [192.168.0.116/24,] routes: - to: default via: 192.168.0.1 metric: 100 on-link: true nameservers: addresses: [192.168.0.1] 第一步 安装qemu软件|github中给出如何使用qemu模拟sriov网卡参数\napt-get update apt-get install -y build-essential ninja-build pkg-config libglib2.0-dev libpixman-1-dev python3-pip python3-setuptools flex bison git clone https://gitlab.com/qemu-project/qemu.git git checkout remotes/origin/stable-8.2 ./configure make -j 4 第二步 准备虚拟机镜像\n./build/qemu-img create -f qcow2 /sriov.qcow2 100G # 关闭虚拟机调整磁盘空间 #./build/qemu-img resize /sriov.qcow2 200G 安装qemu虚拟机\nDetails 参数 功能 -m 1G 指定虚拟机内存配额 -smp 1 指定虚拟机cpu配额 -hda /disk.qcow2 指定磁盘 -cdrom /tmp/alpine-extended-3.22.2-x86_64.iso 加载镜像文件 -boot d 从光驱引导启动 -enable-kvm 使用kvm虚拟化加速 -netdev bridge,id=net1,br=br0 -netdev: 定义网络后端设备 bridge: 使用桥接模式 id=net1: 后端标识符（在 QEMU 内部使用） br=br0: 连接到宿主机的 br0 桥接 -device igb,bus=pcie_port.2,netdev=net1 -device igb: 模拟 Intel IGB 网卡 bus=pcie_root.2: 连接到 PCIe 根端口 netdev=net1: 🔑 关键 - 将 igb 设备连接到网络后端 ./build/qemu-system-x86_64 \\ -m 8G \\ -smp 4 \\ -hda /sriov.qcow2 \\ -enable-kvm \\ -cdrom /tmp/debian-13.1.0-amd64-DVD-1.iso \\ -boot d \\ -netdev bridge,id=net0,br=br0 \\ -device virtio-net-pci,netdev=net0 \\ -M q35 \\ -device pcie-root-port,slot=2,id=pcie_port.2 \\ -device igb,bus=pcie_port.2,netdev=net1 \\ -netdev bridge,id=net1,br=br0 \\ -daemonize 第三步 验证sriov功能: Sriov(kernel) Sriov(dpdk) 重启虚拟机取消从光盘引导系统\n./build/qemu-system-x86_64 \\ -m 8G \\ -smp 4 \\ -hda /sriov.qcow2 \\ -enable-kvm \\ -netdev bridge,id=net0,br=br0 \\ -device virtio-net-pci,netdev=net0 \\ -M q35 \\ -device pcie-root-port,slot=2,id=pcie_port.2 \\ -device igb,bus=pcie_port.2,netdev=net1 \\ -netdev bridge,id=net1,br=br0 \\ -daemonize 划分vf\nroot@localhost:~# lspci |grep 82576 01:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01) root@localhost:~# lsmod |grep igb igb 315392 0 i2c_algo_bit 16384 1 igb dca 16384 1 igb # vf 数量不能动态调整，需要重置后再次分配。重置方式 # echo 0 \u003e /sys/bus/pci/devices/0000:01:00.0/sriov_numvfs root@localhost:~# echo 7 \u003e /sys/bus/pci/devices/0000:01:00.0/sriov_numvfs root@localhost:~# lspci |grep 82576 01:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01) 01:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01) 01:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01) 01:10.4 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01) 01:10.6 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01) 01:11.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01) 01:11.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01) 01:11.4 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01) 【实验一】通过linux自身能力配置网络\nip netns a n1 ip link set enp1s0v6 netns n1 name eth0 ip netns exec n1 ip link set eth0 up ; ip netns exec n1 ip a a 192.168.0.121/24 dev eth0 # 二层互通 root@localhost:~# ip netns exec n1 ping 192.168.0.1 -c 2 PING 192.168.0.1 (192.168.0.1) 56(84) bytes of data. 64 bytes from 192.168.0.1: icmp_seq=1 ttl=64 time=1.02 ms 64 bytes from 192.168.0.1: icmp_seq=2 ttl=64 time=1.03 ms --- 192.168.0.1 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1002ms rtt min/avg/max/mdev = 1.023/1.027/1.031/0.004 ms root@localhost:~# pc@qemu-ubuntu:~$ ping 192.168.0.121 -c2 PING 192.168.0.121 (192.168.0.121) 56(84) bytes of data. 64 bytes from 192.168.0.121: icmp_seq=1 ttl=64 time=0.841 ms 64 bytes from 192.168.0.121: icmp_seq=2 ttl=64 time=0.866 ms --- 192.168.0.121 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1001ms rtt min/avg/max/mdev = 0.841/0.853/0.866/0.012 ms # 三层互通 root@localhost:~# ip netns exec n1 ping 8.8.8.8 -c 2 ping: connect: Network is unreachable root@localhost:~# ip netns exec n1 ip r a default via 192.168.0.1 root@localhost:~# ip netns exec n1 ping 8.8.8.8 -c 2 PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data. 64 bytes from 8.8.8.8: icmp_seq=1 ttl=109 time=226 ms 64 bytes from 8.8.8.8: icmp_seq=2 ttl=109 time=225 ms --- 8.8.8.8 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1001ms rtt min/avg/max/mdev = 225.498/225.794/226.090/0.296 ms 【实验二】使用sriov-cni配置网络\n# 安装sriov-cni # 添加cni配置文件/etc/cni/net.d/10-sriov.conf 【文件可以是任何位置任何命名】 { \"cniVersion\": \"0.3.1\", \"name\": \"sriov-network\", \"type\": \"sriov\", \"deviceID\": \"0000:01:10.4\", \"ipam\": { \"type\": \"host-local\", \"subnet\": \"192.168.0.0/24\", \"rangeStart\": \"192.168.0.123\", \"rangeEnd\": \"192.168.0.123\", \"routes\": [{\"dst\": \"0.0.0.0/0\"}], \"gateway\": \"192.168.0.1\" } } # 手动调用sriov-cni添加网卡到指定名称空间 ip netns a n1 export CNI_COMMAND=ADD export CNI_NETNS=/var/run/netns/n1 export CNI_IFNAME=eth1 export CNI_PATH=/opt/cni/bin export CNI_CONTAINERID=12345 /opt/cni/bin/sriov \u003c/etc/cni/net.d/10-sriov.conf # 检查是否成功添加 root@localhost:~# ip netns exec n1 ip a 1: lo: \u003cLOOPBACK\u003e mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 9: eth1: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 56:0a:d8:24:8e:70 brd ff:ff:ff:ff:ff:ff inet 192.168.0.122/24 brd 192.168.0.255 scope global eth1 valid_lft forever preferred_lft forever inet6 fe80::540a:d8ff:fe24:8e70/64 scope link proto kernel_ll valid_lft forever preferred_lft forever 【实验三】使用multus + flannel + sriov-network-device-plugin + sriov-cni配置网络\ngithub\necho \"deb [trusted=yes] https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main\" | tee /etc/apt/sources.list.d/kubernetes.list apt-get update apt-get install -y kubelet=1.23.17-* kubeadm=1.23.17-* kubectl=1.23.17-* # kubeadm token create --print-join-command kubeadm join 192.168.0.144:16443 --token kp3bem.7svhcpdjp70kj7wi --discovery-token-ca-cert-hash sha256:3e77b765acb011d12cfb2649040ba6b51dd07ac6ee1746ce540740fc9d31bea3 # 安装flannel 和 multus # 安装sriov-cni 【 用于分配和实现sriov网卡通路】 # 安装sriov-network-device-plugin【 用于发现和公告节点sriov的能力】 # 参数： # resourceName 暴露和公开的sriov名称，自定义 # selectors 中的内容通过root@localhost:~/dpdk# ./usertools/dpdk-devbind.py --status # vendors 网卡厂商id # devices 具体网卡型号id git https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin.git cd sriov/sriov-network-device-plugin/deployments cat \u003c\u003cEOF |kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: sriovdp-config namespace: kube-system data: config.json: | { \"resourceList\": [{ \"resourceName\": \"intel_sriov\", \"selectors\": { \"vendors\": [\"8086\"], \"devices\": [\"10ca\"], \"drivers\": [\"igbvf\"], \"pfNames\": [\"enp1s0\"] } }] } EOF kubectl apply -f sriovdp-daemonset.yaml # 通过multus-cni 定义一个NetworkAttachmentDefinition cat \u003c\u003cEOF|kubectl apply -f - apiVersion: \"k8s.cni.cncf.io/v1\" kind: NetworkAttachmentDefinition metadata: name: sriov-net1 namespace: default spec: config: '{ \"type\": \"sriov\", \"resourceName\": \"intel_sriov\", \"deviceID\": \"0000:01:10.2\", \"ipam\": { \"type\": \"host-local\", \"subnet\": \"192.168.0.0/24\", \"rangeStart\": \"192.168.0.123\", \"rangeEnd\": \"192.168.0.123\", \"routes\": [{\"dst\": \"0.0.0.0/0\"}], \"gateway\": \"192.168.0.1\" } }' EOF # 功能验证 cat \u003c\u003cEOF|kubectl apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test1 namespace: default spec: selector: matchLabels: app: test template: metadata: labels: app: test annotations: k8s.v1.cni.cncf.io/networks: sriov-net1 spec: tolerations: - effect: \"\" key: \"node-role.kubernetes.io/master\" containers: - name: test securityContext: privileged: true image: quay.io/libpod/alpine:3.10.2 command: [\"sleep\",\"1000\"] resources: limits: intel.com/intel_sriov: 1 EOF 前面创建的sriov虚拟机是继续内核的技术实现，另一种更为高效的方式是将vf 绑定dpdk(Data Plane Development Kit) 从而提升网卡性能\n实施步骤：\n首先确保在ubuntu上开启硬件虚拟机化\nlscpu | grep -E 'vmx|svm' 其次在ubuntu上引导文件中添加iommu的支持，此处为intel cpu其他cpu有所差异\nvi /etc/default/grub GRUB_CMDLINE_LINUX_DEFAULT=\"quiet iommu=on intel_iommu=on\" update-grub init 6 再次在ubuntu上开启操作系统的大页内存\necho 0 \u003e /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages echo 4096 \u003e /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages mkdir -p /dev/hugepages mount -t hugetlbfs hugetlbfs /dev/hugepages 最后修改启动文件\n./build/qemu-system-x86_64 \\ -m 8G \\ -smp 4 \\ -hda /sriov.qcow2 \\ -enable-kvm \\ -netdev bridge,id=net0,br=br0 \\ -device virtio-net-pci,netdev=net0 \\ -M q35 \\ -device pcie-root-port,slot=2,id=pcie_port.2 \\ -device igb,bus=pcie_port.2,netdev=net1,rombar=0 \\ -netdev bridge,id=net1,br=br0 \\ -mem-prealloc -mem-path /dev/hugepages \\ -device intel-iommu \\ -daemonize 进入debain虚拟机执行\nvi /etc/default/grub GRUB_CMDLINE_LINUX_DEFAULT=\"quiet iommu=on intel_iommu=on\" update-grub init 6 分配vf\nroot@localhost:~# modprobe vfio ;modprobe vfio-pci ;lsmod |grep vfio vfio_pci 16384 0 vfio_pci_core 94208 1 vfio_pci irqbypass 12288 1 vfio_pci_core vfio_iommu_type1 45056 0 vfio 61440 3 vfio_pci_core,vfio_iommu_type1,vfio_pci root@localhost:~# echo 7 \u003e /sys/bus/pci/devices/0000:01:00.0/sriov_numvfs root@localhost:~# lspci |grep 82576 01:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01) 01:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01) 01:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01) 01:10.4 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01) 01:10.6 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01) 01:11.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01) 01:11.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01) 01:11.4 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01) 可以看到01:10.6 vf 已经被绑定到了vfio-pci的驱动上\nroot@localhost:~/dpdk# git clone https://github.com/DPDK/dpdk.git root@localhost:~/dpdk# cd dpdk/ root@localhost:~/dpdk# ./usertools/dpdk-devbind.py -b vfio-pci 0000:01:10.6 root@localhost:~/dpdk# ./usertools/dpdk-devbind.py --status Network devices using DPDK-compatible driver ============================================ 0000:01:10.6 '82576 Virtual Function 10ca' drv=vfio-pci unused=igbvf 【实验一】通过linux自身能力配置网络\n【实验二】使用sriov-cni配置网络\n【实验三】使用multus + flannel + sriov-network-device-plugin + sriov-cni配置网络\n","categories":["kubernetes","network"],"description":"\n*通过qemu虚拟化工具模拟intel厂家sriov功能(sriov kernel和 sriov dpdk),并将该节点添加到kubernetes集群中*\r\n","excerpt":"\n*通过qemu虚拟化工具模拟intel厂家sriov功能(sriov kernel和 sriov dpdk),并将该节点添加到kubernetes集群中*\r\n","ref":"/kubernetes/sriov.html","tags":["qemu","sriov"],"title":"sriov"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\njdk版本：openjdk 11\nzookeeper版本: 3.4.14\nkafka版本：3.3.2\n2011年初，美国领英公司（LinkedIn）开源了一款基础架构软件，以奥地利作家弗兰兹·卡夫卡（Franz Kafka）的名字命名，之后 LinkedIn将其贡献给 Apache基金会，随后该软件于2012年10月完成孵化并顺利晋升为Apache顶级项目——这便是大名鼎鼎的Apache Kafka\n雅虎开源的kafka管理工具CMAK\nexport ZK_HOSTS=\"192.168.0.151:2181,192.168.0.152:2181,192.168.0.153:2181\" wget https://github.com/yahoo/CMAK/releases/download/3.0.0.6/cmak-3.0.0.6.zip unzip cmak-3.0.0.6.zip cmak-3.0.0.6/bin/cmak 启动报错 025-10-13 10:11:20,623 - [ERROR] k.m.KafkaManager - Failed on input : KMAddCluster(ClusterConfig(demo,CuratorConfig(192.168.0.151:2181,192.168.0.152:2181,192.168.0.153:2181,10,100,1000),true,3.1.0,false,None,None,false,false,true,true,false,false,Some(ClusterTuning(Some(30),Some(2),Some(100),Some(2),Some(100),Some(2),Some(100),Some(30),Some(5),Some(4),Some(1000),Some(4),Some(1000),Some(4),Some(1000),Some(30000),Some(1000000),Some(7))),PLAINTEXT,None,None)) org.apache.zookeeper.KeeperException$UnimplementedException: KeeperErrorCode = Unimplemented for /kafka-manager/mutex at org.apache.zookeeper.KeeperException.create(KeeperException.java:106) at org.apache.zookeeper.KeeperException.create(KeeperException.java:54) at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1538) at org.apache.curator.utils.ZKPaths.mkdirs(ZKPaths.java:291) at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:746) at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:723) at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109) at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:720) at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:484) at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:474) 2025-10-13 10:11:20,623 - [ERROR] k.m.ApiError$ - error : KeeperErrorCode = Unimplemented for /kafka-manager/mutex org.apache.zookeeper.KeeperException$UnimplementedException: KeeperErrorCode = Unimplemented for /kafka-manager/mutex at org.apache.zookeeper.KeeperException.create(KeeperException.java:106) at org.apache.zookeeper.KeeperException.create(KeeperException.java:54) at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1538) at org.apache.curator.utils.ZKPaths.mkdirs(ZKPaths.java:291) at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:746) at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:723) at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109) at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:720) at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:484) at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:474) 解决办法\n# 创建 kafka-manager 基础路径 create /kafka-manager \"\" create /kafka-manager/mutex \"\" create /kafka-manager/mutex/locks \"\" create /kafka-manager/mutex/leases \"\" 滴滴开源的kafka管理工具\nOffset Explorer (kafkatool.com)\n","categories":["kafka"],"description":"\n*如何可视化管理kafka,给开发使用*\r\n","excerpt":"\n*如何可视化管理kafka,给开发使用*\r\n","ref":"/elk/kafka/kafka-manager.html","tags":["kafka-manager\""],"title":"kafka可视化管理工具"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\nfilebeat版本：7.17.19\nelasticsearch版本： 7.15.0\n日志 路径 日志格式 查看系统启动、硬件 dmesg 或 /var/log/dmesg 系统日志，记录内核、服务、应用程序的非关键信息 /var/log/messages 用户登录日志 /var/log/secure cron 定时任务日志 /var/log/cron Sep 17 15:58:01 mysql-uat02-s3 CROND[16632]: (root) CMD (/usr/bin/echo 1 \u0026\u003e/dev/null) 审计日志 /var/log/audit/audit.log yum日志 /var/log/yum.log postfix 或 sendmail 等邮件服务的日志 /var/log/maillog 修改启动文件\ncat \u003e/data/elk/filebeat.sh\u003c\u003c'EOF' #!/bin/bash export power_time=$(uptime -s) export power_timestamp=$(date -d \"$power_time\" +%s) /data/elk/filebeat/filebeat --environment systemd $@ EOF chmod +x /data/elk/filebeat.sh tee /etc/systemd/system/filebeat.service \u003c\u003c'EOF' [Unit] Description=Filebeat sends log files to Logstash or directly to Elasticsearch. Documentation=https://www.elastic.co/products/beats/filebeat Wants=network-online.target After=network-online.target [Service] Environment=\"BEAT_LOG_OPTS=\" Environment=\"BEAT_CONFIG_OPTS=-c /data/elk/filebeat/filebeat.yml\" Environment=\"BEAT_PATH_OPTS=--path.home /data/elk/filebeat --path.config /data/elk/filebeat --path.data /data/elk/filebeat/data --path.logs /data/elk/filebeat/logs\" ExecStart=/data/elk/filebeat.sh $BEAT_LOG_OPTS $BEAT_CONFIG_OPTS $BEAT_PATH_OPTS Restart=always [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable filebeat --now systemctl status filebeat cat \u003e/data/elk/filebeat/filebeat.yml \u003c\u003c'EOF' filebeat.inputs: - type: filestream enabled: true id: osMessage paths: - \"/var/log/messages\" tags: [\"osMessage\"] exclude_lines: - \"^$\" - type: filestream enabled: true id: osDmesg paths: - \"/var/log/dmesg\" tags: [\"osDmesg\"] exclude_lines: - \"^$\" processors: - dissect: tokenizer: \"[%{dmesg_seconds}] %{msg}\" target_prefix: \"\" - script: lang: javascript id: dmesg_ts params: power_timestamp: ${power_timestamp} # 环境变量 source: \u003e var params = {}; function register(scriptParams) { params = scriptParams; } function process(event) { // 操作系统启动时间 var bootSec = parseFloat(event.Get(\"dmesg_seconds\")); event.Put( \"LOG_RECORD_TIME\", new Date(parseFloat(params.power_timestamp)*1000 + bootSec*1000).toISOString() ) } processors: - rename: fields: - from: log.file.path to: FILE_PATH - from: message to: LOG output.elasticsearch: hosts: [\"192.168.0.114:9200\"] indices: - index: \"zero-dew-linux-os-message-%{+yyyy.MM.dd}\" when: contains: tags: \"osMessage\" - index: \"zero-dew-linux-os-dmesg-%{+yyyy.MM.dd}\" when: contains: tags: \"osDmesg\" setup.ilm.enabled: false setup.template.enabled: true setup.template.name: \"zero-dew\" setup.template.pattern: \"zero-dew-*\" setup.template.overwrite: true setup.template.settings: index.number_of_shards: 3 index.number_of_replicas: 0 EOF ","categories":["ELK"],"description":"\n使用filebeat收集linux操作系统日志\r\n","excerpt":"\n使用filebeat收集linux操作系统日志\r\n","ref":"/elk/example/linux.html","tags":["linux","filebeat"],"title":"收集操作系统日志"},{"body":"洋葱炒牛肉 姜丝、小米辣椒、牛里脊肉、洋葱 牛肉切薄片攥干水分加入料酒、生抽用手搅拌均匀,静待10min后加入淀粉和植物油 急火爆炒，洋葱/姜丝/小米辣椒–\u003e牛肉变色就熟了 清蒸鲈鱼/鲳鱼 广东人对鱼的喜爱，可谓是情有独钟。粤式蒸鱼，选用新鲜的鲈鱼或鲳鱼，搭配姜丝、葱丝，用清蒸的方式保留鱼的鲜美，是广东家庭中常见的健康菜品。制作这道菜，关键在于鱼的新鲜度和蒸的火候。新鲜的鱼，肉质细嫩，蒸制时只需简单调味，便能凸显其原味。蒸鱼时，先用大火将水烧开，再将鱼放入蒸锅，保持中火蒸制，一般10分钟左右即可。出锅前，撒上葱花，淋上热油，一道色香味俱全的粤式蒸鱼便完成了。\n冰糖葫芦 微波炉 糖:水=3:1\n电磁炉熬糖：\n开中火，开火后不要搅拌。水：糖 = 1：1\n煮沸–\u003e成黄色，冒泡。\n测试糖稀是否熬好，将变成黄色的糖稀取少量放在冷水中，如果变硬了吃起来是脆的说明可以了\n此时电磁炉调小火候至100°，准备让水果沾糖稀\n沾糖稀时仅仅沾取糖稀中冒泡的泡泡，防止糖稀过厚或者把水果烫熟。成功的冰糖葫芦表面光滑清脆\n水果表面不能有水分，不能破损\n","categories":["娱乐"],"description":"学习做饭\n","excerpt":"学习做饭\n","ref":"/2025-07-27/cook.html","tags":["做饭"],"title":"食谱"},{"body":"xtrabackup xtrabackup 与mysql版本矩阵：\nXtraBackup 版本 支持的 MySQL 版本 关键限制 8.0 系列 MySQL 8.0.x（推荐 8.0.20+） 完全兼容 MySQL 8.0，不支持 MySQL 5.7 及以下 2.4 系列 MySQL 5.6、5.7 不支持 MySQL 8.0 注意：\nXtraBackup 8.0 不支持备份 MySQL 5.7。若需备份 MySQL 5.7，必须使用 XtraBackup 2.4。 Percona 强烈建议 XtraBackup 小版本号与 MySQL 大版本一致（例如 MySQL 8.0.32 搭配 XtraBackup 8.0.32）。 安装xtrabackup cat \u003e/etc/yum.repos.d/percona-original-release.repo \u003c\u003c'EOF' [percona-release-x86_64] name = Percona Original release/x86_64 YUM repository baseurl = http://repo.percona.com/percona/yum/release/$releasever/RPMS/x86_64 enabled = 1 gpgcheck = 0 gpgkey = file:///etc/pki/rpm-gpg/PERCONA-PACKAGING-KEY [percona-release-noarch] name = Percona Original release/noarch YUM repository baseurl = http://repo.percona.com/percona/yum/release/$releasever/RPMS/noarch enabled = 1 gpgcheck = 0 gpgkey = file:///etc/pki/rpm-gpg/PERCONA-PACKAGING-KEY [percona-release-sources] name = Percona Original release/sources YUM repository baseurl = http://repo.percona.com/percona/yum/release/$releasever/SRPMS enabled = 0 gpgcheck = 1 gpgkey = file:///etc/pki/rpm-gpg/PERCONA-PACKAGING-KEY EOF yum install -y percona-xtrabackup-80.x86_64 全量备份 170G 数据压缩后16g,全备耗时7分钟,占用1.5核心cpu\nxtrabackup \\ --defaults-file=/etc/mysql80/mysql80_6301.cnf \\ --backup \\ --user=\"root\" --password='d988ec6a93!@#MYSQL' --port=6301 \\ --target-dir=\"/Backup/fullbackup-$(date +%F)\" \\ --compress --compress-threads=2 2\u003e\u00261 | tee /tmp/mysql6301_$(date +%F_%H%M%S).log 恢复全量备份 限速100Mb 传输16G, 耗时3分钟\nscp -l 819200 -rp /Backup/fullbackup-2025-07-22/* bak@10.128.99.157:/Data/bak/ rm -fr /Data/mysql6301/{binlog,data,log} mkdir /Data/mysql6301/{binlog,data,log} -p touch /Data/mysql6301/log/mysqld-error.log # 解压 16gb 耗时 2分钟 xtrabackup --decompress --parallel=2 --target-dir=/Data/bak --remove-original # 应用undolog xtrabackup --prepare --apply-log-only --target-dir=/Data/bak xtrabackup --prepare --target-dir=/Data/bak # 拷贝全量备份到datadir xtrabackup --defaults-file=/etc/my.cnf --copy-back --target-dir=/Data/bak chown -R mysql.mysql /Data/mysql6301 service mysql start chkconfig mysql on 增量同步数据 主库 从库 创建复制用户\nCREATE USER 'replUser'@'%' IDENTIFIED BY 'Repl#0001'; GRANT REPLICATION SLAVE ON *.* TO 'replUser'@'%'; flush privileges ; 设置只读，预防多点写入\n/usr/local/mysql80/bin/mysql -uroot -p'd988ec6a93!@#MYSQL' --socket=/Data/mysql6301/mysql6301.sock --port=6301 mysql\u003e set global read_only=on; Query OK, 0 rows affected (0.00 sec) mysql\u003e set global super_read_only=on; Query OK, 0 rows affected (0.00 sec) mysql\u003e show variables like '%read_only%'; +-----------------------+-------+ | Variable_name | Value | +-----------------------+-------+ | innodb_read_only | OFF | | read_only | ON | | super_read_only | ON | | transaction_read_only | OFF | +-----------------------+-------+ 4 rows in set (0.00 sec) 找到全备时的binlog位置\n[root@db3-mysql80-m data]# cat xtrabackup_binlog_info mysql-bin.000362 156 /*从库*/ reset slave all; CHANGE MASTER TO MASTER_HOST='10.128.111.157', MASTER_PORT=6301, MASTER_USER='replUser', MASTER_PASSWORD='Repl#0001', MASTER_LOG_FILE='mysql-bin.000362', MASTER_LOG_POS=156; start slave ; show slave status \\G 检查slave状态\n[root@db3-mysql80 ~]# /usr/local/mysql80/bin/mysql -uroot -p'd988ec6a93!@#MYSQL' --socket=/Data/mysql6301/mysql6301.sock --port=6301 -e \"show slave status\\G\"|grep -E \"Slave_IO_Running|Slave_SQL_Running:|Seconds_Behind_Master\" mysql: [Warning] Using a password on the command line interface can be insecure. Slave_IO_Running: Yes Slave_SQL_Running: Yes Seconds_Behind_Master: 0 ","categories":["mysql"],"description":"mysql 备份工具之xtrabackup\n","excerpt":"mysql 备份工具之xtrabackup\n","ref":"/mysql/xtrabackup.html","tags":["备份","xtrabackup"],"title":"xtrabackup"},{"body":"RabbitMQ安装、集群\n安全\n监控\n学习前3章\nAMQP advanced message queuing protocol 2004公布协议标准\n交换机 exchange\n队列 queue\n绑定\n虚拟机 vhost： 是一个迷你版的rabbitMQ，可以隔离队列、交换机、绑定以及权限。默认vhost 为 “/“\nrabbitmqctl add_vhost test\nrabbitmqctl list_vhost test\nrabbitmqctl delete_vhost test\n消息持久化\n交换机和队列同时开启持久化参数durable = true\n消息投递时开启持久化参数 delivery_mode=2\n","categories":"","description":"","excerpt":"RabbitMQ安装、集群\n安全\n监控\n学习前3章\nAMQP advanced message queuing protocol 2004公布协议标准\n交换机 exchange\n队列 queue\n绑定\n虚拟机 vhost： 是一个迷你版的rabbitMQ，可以隔离队列、交换机、绑定以及权限。默认vhost 为 “/“\nrabbitmqctl add_vhost test\nrabbitmqctl …","ref":"/docs/database/rabbitmq/","tags":"","title":""},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/database/rocketmq/","tags":"","title":""},{"body":"通过自行构建image，快速体验夜莺\ngit clone https://github.com/ccfos/nightingale.git 位置 nightingale/Dockerfile.nightingale\nFROM node:16-alpine AS fe-builder RUN apk add git RUN git clone https://github.com/n9e/fe.git \u0026\u0026 cd fe \u0026\u0026 \\ git checkout $(git describe --tags --abbrev=0) \u0026\u0026 \\ npm config set registry https://registry.npmmirror.com \u0026\u0026\\ npm install \u0026\u0026 \\ npm run build FROM golang:1.23.4-alpine3.20 AS builder COPY --from=fe-builder /fe/pub /pub RUN apk add git RUN git clone https://github.com/ccfos/nightingale.git \u0026\u0026 cd nightingale \u0026\u0026\\ git checkout $(git describe --tags --abbrev=0) \u0026\u0026\\ export GOPROXY=\"https://goproxy.cn,direct\" \u0026\u0026\\ go install github.com/rakyll/statik@latest \u0026\u0026\\ statik --src=/pub -dest=./front \u0026\u0026\\ go build -ldflags \"-w -s -X github.com/ccfos/nightingale/v6/pkg/version.Version=$(git describe --tags --abbrev=0)\" -o /n9e ./cmd/center/main.go FROM python:3-slim WORKDIR /app COPY --from=builder /n9e /app/ ADD etc /app/etc/ ADD integrations /app/integrations/ RUN pip install requests -i https://pypi.tuna.tsinghua.edu.cn/simple EXPOSE 17000 CMD [\"/app/n9e\", \"-h\"] 位置 nightingale/docker/compose-bridge/docker-compose.yaml\nnetworks: nightingale: driver: bridge services: mysql: image: \"mysql:8\" container_name: mysql hostname: mysql restart: always environment: TZ: Asia/Shanghai MYSQL_ROOT_PASSWORD: 1234 volumes: - ./data/mysqldata:/var/lib/mysql/ - ../initsql:/docker-entrypoint-initdb.d/ - ./etc-mysql/my.cnf:/etc/my.cnf networks: - nightingale ports: - \"3306:3306\" redis: image: \"redis:6.2\" container_name: redis hostname: redis restart: always environment: TZ: Asia/Shanghai networks: - nightingale ports: - \"6379:6379\" # prometheus: # image: prom/prometheus # container_name: prometheus # hostname: prometheus # restart: always # environment: # TZ: Asia/Shanghai # volumes: # - ./etc-prometheus:/etc/prometheus # command: # - \"--config.file=/etc/prometheus/prometheus.yml\" # - \"--storage.tsdb.path=/prometheus\" # - \"--web.console.libraries=/usr/share/prometheus/console_libraries\" # - \"--web.console.templates=/usr/share/prometheus/consoles\" # - \"--enable-feature=remote-write-receiver\" # - \"--query.lookback-delta=2m\" # networks: # - nightingale # ports: # - \"9090:9090\" victoriametrics: image: victoriametrics/victoria-metrics:v1.79.12 container_name: victoriametrics hostname: victoriametrics restart: always environment: TZ: Asia/Shanghai ports: - \"8428:8428\" networks: - nightingale command: - \"--loggerTimezone=Asia/Shanghai\" volumes: - \"./data/victoriametrics:/victoria-metrics-data\" loki: image: grafana/loki:3.5.0 container_name: loki hostname: loki restart: always user: root environment: TZ: Asia/Shanghai ports: - \"3100:3100\" volumes: - ./data/loki:/loki networks: - nightingale nightingale: build: context: ../.. dockerfile: Dockerfile.nightingale container_name: nightingale hostname: nightingale restart: always environment: GIN_MODE: release TZ: Asia/Shanghai WAIT_HOSTS: mysql:3306, redis:6379 volumes: - ./etc-nightingale:/app/etc networks: - nightingale ports: - \"17000:17000\" - \"20090:20090\" depends_on: - mysql - redis - victoriametrics command: \u003e sh -c \"/app/n9e\" categraf: image: \"flashcatcloud/categraf:latest\" container_name: \"categraf\" hostname: \"categraf01\" restart: always environment: TZ: Asia/Shanghai HOST_PROC: /hostfs/proc HOST_SYS: /hostfs/sys HOST_MOUNT_PREFIX: /hostfs WAIT_HOSTS: nightingale:17000, nightingale:20090 volumes: - ./etc-categraf:/etc/categraf/conf - /:/hostfs networks: - nightingale depends_on: - nightingale cd nightingale/docker/compose-bridge/ docker-compose up # root root.2020 ","categories":["夜莺·监控系统","监控"],"description":"\n夜莺·监控系统\r\n","excerpt":"\n夜莺·监控系统\r\n","ref":"/prometheus/n9e.html","tags":["夜莺·监控系统"],"title":"夜莺·监控系统"},{"body":"安装部署\nfederation 可以实现全局视图。主节点不仅可以提取聚合指标，还可以为Grafana等工具暴露指标或者作为可视化的默认数据源\n联邦配置: 节点1 节点2 节点3 主节点 global: external_labels: worker: 0 rule_files: - \"rules/node_rules.yml\" scrape_configs: - job_name: 'node' file_sd_configs: - files: - targets/nodes/*.json refresh_interval: 5m relabel_configs: - source_labels: [__address__] modulus: 3 target_label: __tmp_hash action: hashmod - source_labels: [__tmp_hash] regex: ^0$ action: keep global: external_labels: worker: 1 rule_files: - \"rules/node_rules.yml\" scrape_configs: - job_name: 'node' file_sd_configs: - files: - targets/nodes/*.json refresh_interval: 5m relabel_configs: - source_labels: [__address__] modulus: 3 target_label: __tmp_hash action: hashmod - source_labels: [__tmp_hash] regex: ^1$ action: keep global: external_labels: worker: 1 rule_files: - \"rules/node_rules.yml\" scrape_configs: - job_name: 'node' file_sd_configs: - files: - targets/nodes/*.json refresh_interval: 5m relabel_configs: - source_labels: [__address__] modulus: 3 target_label: __tmp_hash action: hashmod - source_labels: [__tmp_hash] regex: ^2$ action: keep scrape_configs: - job_name: 'federate' scrape_interval: 15s honor_labels: true metrics_path: '/federate' params: 'match[]': - '{job=\"prometheus\"}' - '{__name__=~\"job:.*\"}' static_configs: - targets: - 'source-prometheus-1:9090' - 'source-prometheus-2:9090' - 'source-prometheus-3:9090' ","categories":["prometheus","监控"],"description":"\nfederation|prometheus\r\n\r\n","excerpt":"\nfederation|prometheus\r\n\r\n","ref":"/prometheus/federation.html","tags":["prometheus","federation"],"title":"federation"},{"body":"prometheus 兼容API 查询: 查询瞬时向量 查询范围向量 查询告警/记录规则 查询活动的警报 curl -s http://127.0.0.1:9090/api/v1/query?query=prometheus_build_info curl -X POST -d \"query=prometheus_build_info\" http://127.0.0.1:9090/api/v1/query # curl -s http://127.0.0.1:9090/api/v1/query?query=node_cpu|jq curl -s -g 'http://127.0.0.1:9090/api/v1/query?query=node_cpu{cpu=\"cpu0\"}' |jq '.data.result[3].value' curl -s -g 'http://127.0.0.1:9090/api/v1/query_range?query=rate(prometheus_tsdb_head_samples_appended_total[5m])\u0026start=1514764800\u0026end=1762830278\u0026step=60' curl -XPOST -d 'query=rate(prometheus_tsdb_head_samples_appended_total[5m])\u0026start=1514764800\u0026end=1762830278\u0026step=60' http://127.0.0.1:9090/api/v1/query_range curl -s http://127.0.0.1:9090/api/v1/rules | jq curl -s http://127.0.0.1:9090/api/v1/alerts | jq 故障分析 问题描述：\n部分主机cpu使用率持续40分钟超94% 并，告警没有正常发出来。同一个告警下又有主机能够正常发出告警。 排查过程:\nflowchart TB\rA[\"通过promQL 执行告警规则中的语句，确实达到了触发告警规则的基准\"]\rB[\"检查alertmanger,发现alertmanger并没有收到该告警\"]\rC[\"\u003cfont color=red\u003e【隐蔽问题：重复第一步通过频繁查询发现有30%的概率查询不到符合告警规则的主机】\u003c/font\u003e\"]\rD[\"Prometheus开启了远程写入和查询，开始怀疑是不是远程查询导致的查询失败。\r通过promQL 执行其他查询语句，未发现上述【隐蔽问题】\"]\rE[\"尝试修改该告警规则中promQL 语句，意外发现上述隐蔽问题修复了!!\"]\rA -.-\u003eB-.-\u003eC-.-\u003eD-.-\u003eE 修改前 修改后 label_replace(sum by(node) (rate(container_cpu_usage_seconds_total{id=\"/\"}[2m])),\"hostname\", \"$1\", \"node\", \"(.*)\") / label_replace(sum by(node) (machine_cpu_cores), \"hostname\", \"$1\", \"node\", \"(.*)\") * 100 \u003e 80 label_replace(sum by(node) (rate(container_cpu_usage_seconds_total{id=\"/\"}[5m])),\"hostname\", \"$1\", \"node\", \"(.*)\") / label_replace(sum by(node) (machine_cpu_cores), \"hostname\", \"$1\", \"node\", \"(.*)\") * 100 \u003e 80 故障总结\n根本原因是rate计算速率的函数基于最后2个数据点执行计算，由于rate()[2m] 时间跨度刚好等于scrape_interval 因此存在一定概率2分钟内没有两个数据采集点位导致数据点缺失或时间对齐问题返回空值。\n解决办法建议将范围扩大到 scrape_interval*4\n","categories":["prometheus","监控"],"description":"\n附录|prometheus\r\n\r\n","excerpt":"\n附录|prometheus\r\n\r\n","ref":"/prometheus/appendix.html","tags":["prometheus","附录"],"title":"附录"},{"body":"net/http 发送GET请求\npackage main import ( \"fmt\" \"io\" \"net/http\" ) func main() { // 发送Get 请求 response, err := http.Get(\"http://175.178.65.213:7292\") if err != nil { fmt.Println(err) } // 关闭连接 defer response.Body.Close() // 读取响应体内容 result, err := io.ReadAll(response.Body) if err != nil { fmt.Println(err) } fmt.Println(string(result)) } 创建http服务\n简单的http 多路复用http package main import ( \"fmt\" \"net/http\" ) func main() { http.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) { fmt.Printf(\"接收到%s一个请求\\n\", r.Method) fmt.Fprintf(w, \"你执行了一个%s方法\", r.Method) }) http.ListenAndServe(\":8080\", nil) } package main import \"net/http\" func main() { mux := http.NewServeMux() mux.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\"Hello, World!\")) }) http.ListenAndServe(\":8080\", mux) } ","categories":["golang"],"description":"net/http|标准库\n","excerpt":"net/http|标准库\n","ref":"/golang/package/http.html","tags":["golang","net/http"],"title":"net/http"},{"body":"golang 除了包含内置数据类型，同样还允许用户通过type 关键字自定义数据类型。自定义数据类型是内置数据类型一个或多个类型的组合。\n示例：\npackage main import \"fmt\" func main() { type newstring string var name newstring name = \"张三\" fmt.Printf(\"%s的数据类型是：%T\\n\", name, name) //张三的数据类型是：main.newstring } package main import \"fmt\" func main() { type newstring = string var name newstring name = \"张三\" fmt.Printf(\"%s的数据类型是：%T\\n\", name, name) // 张三的数据类型是：string } 结构体的基本语法 结构体定义基本语法\ntype \u003cname\u003e struct {} 示例：\npackage main import ( \"fmt\" ) type Persion struct { Name string Age int } func main() { var p Persion p.Name = \"张三\" p.Age = 33 fmt.Println(p) //{张三 33} } 结构体赋值的语法\n使用 “.” 赋值\n// 使用 . 对字段赋值 package main import \"fmt\" type Persion struct { Name string Age int } func main() { var p Persion p.Name = \"张三\" p.Age = 33 fmt.Println(p) } 声明变量时赋值\npackage main import \"fmt\" type Persion struct { Name string Age int } func main() { var p = Persion{ Name: \"张三\", Age: 33, } fmt.Println(p) } 也可以省略字段直接赋值\npackage main import \"fmt\" type Persion struct { Name string Age int } func main() { var p = Persion{ \"张三\", 33, } fmt.Println(p) } 实例化结构体 使用new()函数实例化\n// 结构体定义后并不会分配内存空间，一旦结构体变量完成实例化，就完成了内存地址的分配 package main import \"fmt\" type Persion struct { Name string Age int } func main() { // 通过new() 返回数据类型的指针 p := new(Persion) fmt.Printf(\"实例化后p的内存地址为：%p\\n保存的值为: %+v\\n\", p, *p) // 实例化后p的内存地址为：0xc0000a8018 // 保存的值为: {Name: Age:0} } 使用取地址符号 \u0026 实例化\npackage main import \"fmt\" type Persion struct { Name string Age int } func main() { var p = \u0026Persion{} fmt.Printf(\"实例化后p的内存地址为：%p\\n保存的值为: %+v\\n\", p, *p) // 实例化后p的内存地址为：0xc000010030 // 保存的值为: {Name: Age:0} } [waring]\nvar p *Persion并没有完成实例化，go程序并未分配内存给p,此时p 就是空指针\npackage main import \"fmt\" type Persion struct { Name string Age int } func main() { var p *Persion fmt.Printf(\"实例化后p的内存地址为：%p\\n保存的值为: %+v\\n\", p, p) // 实例化后p的内存地址为：0x0 // 保存的值为: \u003cnil\u003e } 结构体方法 通过构造函数返回结构体变量，进而通过结构体变量操作结构体方法\n示例：\npackage main import \"fmt\" type Persion struct { Name string Age int } func initFunction(name string, age int) *Persion { return \u0026Persion{ Name: name, Age: age, } } func (p *Persion) run() { fmt.Println(p.Name, \"可以跑步\") } func main() { p := initFunction(\"张三\", 33) p.run() } 方法同样支持传参，和定义返回值\npackage main import \"fmt\" type Persion struct { Name string Age int } func initFunction(name string, age int) *Persion { return \u0026Persion{ Name: name, Age: age, } } // 不带参数的方法 func (p *Persion) run() { fmt.Println(p.Name, \"可以跑步\") } // 带参数的方法 func (p *Persion) work(w string) { fmt.Printf(\"做一份%s的工作\\n\", w) } // 带返回值的方法 func (p *Persion) rename(newName string) *Persion { p.Name = newName return p } func main() { p := initFunction(\"张三\", 33) p.run() p.work(\"挑战\") newPersion := p.rename(\"王五\") fmt.Printf(\"%+v\\n\", *newPersion) } 匿名结构体 结构体定义基本语法\n\u003cname\u003e:=struct{ field_name definition }{ field_name: value } 举例：\npackage main import \"fmt\" func main() { p := struct { Name string Age int }{ Name: \"张三\", Age: 33, } fmt.Println(p) } 结构体嵌套 匿名字段\npackage main import \"fmt\" func main() { // 没有定义名称的字段称为匿名字段 type Persion struct { string int } var p = Persion{ string: \"张三\", int: 33, } fmt.Println(p) //{张三 33} } 结构体嵌套\npackage main import \"fmt\" type Alert struct { Status string Alerts []AlertItem } type AlertItem struct { AlertName string } func main() { var alert = \u0026Alert{ Status: \"firing\", Alerts: []AlertItem{{ AlertName: \"cpu\", }, }, } fmt.Printf(\"%+v\\n\", alert) //\u0026{Status:firing Alerts:[{AlertName:cpu}]} } ","categories":["golang"],"description":"结构体|数据类型\n","excerpt":"结构体|数据类型\n","ref":"/golang/struct.html","tags":["golang","结构体"],"title":"结构体"},{"body":"from fastapi import FastAPI import uvicorn app = FastAPI() @app.get(\"/\") async def root(): return {\"username\": \"admin\",\"password\":\"111111\"} if __name__ == \"__main__\": uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8000, reload=True) ","categories":["fastapi"],"description":"fastapi\n","excerpt":"fastapi\n","ref":"/20251009/fastapi.html","tags":["fastapi"],"title":"fastapi"},{"body":"微信开发者工具\n微信公众平台\nAppID(小程序ID) wxca40a0ec9d888178\nAppSecret(小程序密钥) 19fb61a0a5d06ea7021d114a808a4afe\nvue项目学些\n开发环境：ESLint + vscode\ndocker run --net=host -v `pwd`:/opt/ -it node:20.19.0 bash npm config set registry https://registry.npmmirror.com # 安装Vue CLI npm install -g @vue/cli # 创建项目 cd /opt vue create my-vue-project # 进入项目目录 cd my-vue-project # 启动开发服务器 npm run serve vue3讲解\nvue-admin-template模版讲解\nPanJiaChen/vue-admin-template: a vue2.0 minimal admin template\nzxwk1998/vue-admin-better: 🎉 vue admin,vue3 admin,vue3.0 admin,vue后台管理,vue-admin,vue3.0-admin,admin,vue-admin,vue-element-admin,ant-design,vab admin pro,vab admin plus,vue admin plus,vue admin pro\n","categories":["vue"],"description":"\nvue\r\n\r\n","excerpt":"\nvue\r\n\r\n","ref":"/20251009/vue.html","tags":["vue"],"title":"vue"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\nfilebeat版本：7.17.19\nlogstash版本：7.15.0\nelasticsearch版本： 7.15.0\nLogstash 使用jruby 语言开发，软件运行依赖jdk兼容矩阵，如果无法确认该使用什么版本可以下载带有jdk环境的logstash安装包。\n安装java（非必须）\nwget https://download.java.net/java/ga/jdk11/openjdk-11_linux-x64_bin.tar.gz tar xf openjdk-11_linux-x64_bin.tar.gz -C /opt ln -svf /opt/{jdk-11,jdk} cat\u003e\u003e/etc/profile\u003c\u003c'EOF' export JAVA_HOME=/opt/jdk export JAVA_JRE=$JAVA_HOME/jre export CLASSPATH=$JAVA_HOME/lib:$JAVA_HOME/jre/lib export PATH=$JAVA_HOME/bin:$JAVA_JRE/bin:$PATH:. EOF source /etc/profile java -version 安装logstash\nversion=7.15.0 wget https://mirrors.huaweicloud.com/logstash/${version}/logstash-${version}-linux-x86_64.tar.gz tar xf logstash-${version}-linux-x86_64.tar.gz -C /data/elk ln -svf /data/elk/logstash-${version} /data/elk/logstash 测试运行\n/data/elk/logstash/bin/logstash -e 'input { stdin {} } output { stdout { codec=\u003erubydebug }}' cat \u003e/data/elk/logstash/config/logstash.conf\u003c\u003c'EOF' # Sample Logstash configuration for creating a simple # Beats -\u003e Logstash -\u003e Elasticsearch pipeline. input { beats { port =\u003e 5044 } } output { elasticsearch { hosts =\u003e [\"http://192.168.0.114:9200\"] index =\u003e \"%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}\" #user =\u003e \"elastic\" #password =\u003e \"changeme\" } } EOF tee /etc/systemd/system/logstash.service \u003c\u003c'EOF' [Unit] Description= logstash serveice After=network.target [Service] WorkingDirectory=/data/elk/logstash ExecStart=/data/elk/logstash/bin/logstash \\ -f /data/elk/logstash/config/logstash.conf \\ --config.reload.automatic User=root LimitNOFILE=65535 [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable logstash --now systemctl status logstash ","categories":["ELK"],"description":"\n使用filebeat收集linux操作系统日志\r\n","excerpt":"\n使用filebeat收集linux操作系统日志\r\n","ref":"/elk/example/logstash/logstash_install.html","tags":["linux","filebeat"],"title":"logstash_install"},{"body":"[mysqld] innodb_lock_wait_timeout = 60 sort_buffer_size = 8M query_cache_limit = 32M auto_increment_offset = 1 auto_increment_increment = 1 character_set_database = utf8 character_set_results = utf8 character_set_server = utf8 init_connect='启动时自动执行的内容' #php设置为短连接 interactive_timeout = 30; #合作的超时时间 wait_tiemout = 30;\t#闲置的超时时间 #java 设置长连接 interactive_timeout = 28800; wait_tiemout = 28800; #一个端口最大连接数 max_connections = 800 #一个用户的最大连接数 #max_user_connections = 800 #连接错误的最大数量 max_connect_errors = 3000 #兼容旧的密码模式 #old_password = on read_only = on #默认情况下从库启动自动开始同步，添加参数后需要手动start slave ; skip_slave_start = on #更新master_info 信息 sync_master_info = 1 记录relaylog，默认开启 sync_relay_log = 1 记录relaylog info sync_relay_log_info = 1 [client] character_set_client =utf8 character_set_connection =utf8 character_set_results =utf8 [mysql] character_set_connection=utf8 mysql 乱码字符集设置\n字符集设置 set names utf8 ; 等同于 set @@session.character_set_client =utf8; set @@session.character_set_connection =utf8; set @@session.character_set_results =utf8; set @@session.character_set_database =utf8; set @@session.character_set_server =utf8; [client] character_set_client =utf8 character_set_connection =utf8 character_set_results =utf8 [mysql] character_set_connection=utf8 #导入表时关闭外检检查 set foreign_key_check = 0 #查询语句的性能 show profing; show profie all; #设置为分页查询 mysql\u003e page less mysql\u003e show master logs ; mysql\u003e purge master logs to 'binlog文件' 修复表 mysql\u003e repair table 表名 myisamchk -r /var/lib/mysql/*.MYI 查询 innodb 的状态(比如死锁) show engine innodb status \\G 查询运行状态 show global status \\G 查询增删改的数量 mysqladmin -uroot -i 3 -r exten|egrep -i '(com_insert\\b)|(com_select)|(com_delete\\b)|(com_update\\b)' #查询进程号 pidof mysqld gdb -p `pidof mysqld` -ex \"set opt_log_slave_updates=1\" IO PS 每秒吞吐量 flashcache 开源软件 可以把ssd和sas磁盘虚拟化一块 把热数据存在ssd上把sas放在sas上 查询 cp io mpstats -P ALL 1 100 查询所有cpu的使用情况 每秒打印一个打印100次 sar -u 1 100 查询所有cpu的使用情况 每秒打印一个打印100次 iostat -x 1 100 sar -b 1 100 sar -d 1 100 sar -q sar -n DEV 查看网路 sar -W 查看内存 sar -r sar -B top dmesg -H --level=err,warn -k 优化 numad 不使用swap sysctl -a|grep swap echo \"vm.swappiness = 0\" \u003e\u003e/etc/sysctl.conf sysctl -p io调度 [root@localhost ~]# cat /sys/block/sda/queue/scheduler noop [deadline] cfq ulimit 设定 软硬文件打开数 ulimit -HSn 65535 #stack size ulimit -s 65535 查看raid信息 megacil [root@localhost ~]# top -b -n 1 | grep Cpu %Cpu(s): 0.0 us, 6.2 sy, 0.0 ni, 93.8 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st 性能调优 https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/6/html/performance_tuning_guide/index https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/performance_tuning_guide/index 《mysql反范式》 数据类型 timestamp mysql\u003e select current_time(),now(),sysdate(); +----------------+---------------------+---------------------+ | current_time() | now() | sysdate() | +----------------+---------------------+---------------------+ | 12:08:14 | 2020-03-08 12:08:14 | 2020-03-08 12:08:14 | +----------------+---------------------+---------------------+ 1 row in set (0.00 sec) mysql\u003e select inet_aton('192.168.0.1'); +--------------------------+ | inet_aton('192.168.0.1') | +--------------------------+ | 3232235521 | +--------------------------+ 1 row in set (0.00 sec) mysql\u003e select inet_ntoa(3232235521); +-----------------------+ | inet_ntoa(3232235521) | +-----------------------+ | 192.168.0.1 | +-----------------------+ 1 row in set (0.00 sec) 1.xtrabackup 安装 wget https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.4.4/binary/redhat/7/x86_64/percona-xtrabackup-24-2.4.4-1.el7.x86_64.rpm yum install libev-devel.x86_64 yum install numactl-libs.x86_64 yum install perl-DBD-MySQL.x86_64 rpm -ivh percona-xtrabackup-24-2.4.4-1.el7.x86_64.rpm which xtrabackup 备份原理 xtrabackup 首先启动备份innodb 引擎的线程 redolog 的备份和监听线程， 并开始备份ibd 数据文件。 当innodb引擎的数据文件备份完毕后， flust table with read lock; 备份 .frm .myd .myi 文件。非innodb引擎数据文件备份完毕后 通知redolog 线程结束并记录数据文件的lsn号，便于下次增量 备份 性能测试工具 mysqlslap 日志分析工具 https://www.jb51.net/article/75064.htm ### Centos7,centos6适用 yum groupinstall \"X Window System\" yum groupinstall \"GNOME Desktop\" 通过命令 startx 进入图形界面 图形到dos：ctrl+alt+f2 dos到图形：输入startx 或者 在命令上输入 init 3 命令 切换到dos界面 输入 init 5命令 切换到图形界面 ### centos 7 systemctl get-default graphical.target代表开机时启动图形化界面 multi-user.target代表开机时启动dos界面 systemctl set-default graphical.target systemctl set-default multi-user.target ssh 连接客户端长时间不使用自动断开 export PATH=\\$PATH:. export PS1='\\`hostname\\`:\\$PWD\u003e' set -o vi stty erase ^H back_log = 600 external-locking = FALSE max_allowed_packet =8M thread_stack = 192K thread_stack = 192 [mysql] no-auto-rehash 过滤 change master to Replicate_Ignore_DB = 'test'; Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: http://itindex.net/index/mha http://mysqlserverteam.com/ https://www.cnblogs.com/xiaoboluo768/p/5973827.html https://blog.51cto.com/8370646/2150179 https://www.cnblogs.com/zengkefu/p/5600776.html http://www.mamicode.com/info-detail-1488956.html?__cf_chl_jschl_tk__=864bd8082fbcc44cff88faa95540ef89c8b1b1ed-1584599149-0-AZzn7zH3F9Q-fMosxg8_z2LhSKfhM8v8fHU4CwyiFxVIk0VzRZI1cOMDw3H6MDQvflAFPsEL9s1MKiUp0hv-9DgUhwYnONrP7mGzp5ZLoSQBlxx9PK4W74ZQrzdsLbUqj3x_clEVUJwdXTPNq-3NiQ0Oz08ONGXWoF2Bx9WfddC9sPngJHUB9CgTE-TKk6o1ktyutexm0QT_zn1jyvml2RK4N133vdGwewmb4QttA_CjOSV4ftDAhu5284B86HQd3Q8VGbn0W9aIC0VGHARUdb5JE0tb2usYTx_IbtaG-QglcJIVYJD5eFWX8Vf7_Okbtw ","categories":["mysql"],"description":"\nmysql 配置文件解析\r\n","excerpt":"\nmysql 配置文件解析\r\n","ref":"/mysql/my_cnf.html","tags":["mysql配置"],"title":"config"},{"body":"电影下载网站：https://xuexizhinan.com/ 4k高清通过夸克下载\n2025-7-27 «希特勒-恶魔崛起»\n","categories":["娱乐"],"description":"电影观看时间表\n","excerpt":"电影观看时间表\n","ref":"/2025-07-27/movie.html","tags":["电影"],"title":"电影"},{"body":"package main /* 功能： 通过flag,实现用户传参 通过os/exec，实现调用操作系统命令 */ import ( \"flag\" \"fmt\" \"os/exec\" ) var ( ip = flag.String(\"ip\", \"127.0.0.1\", \"--ip \u003cip地址\u003e\") port = flag.Int(\"port\", 80, \"--port \u003c端口\u003e\") ssl = flag.Bool(\"ssl\", false, \"--ssl \u003ctrue|false\u003e\") ) func GenerateUrl() string { var url string if *ssl { // Sprintf() 实现字符串格式化 url = fmt.Sprintf(\"https://%s:%d\", *ip, *port) } else { url = fmt.Sprintf(\"http://%s:%d\", *ip, *port) } return url } func main() { // 解析flag定义参数 flag.Parse() url := GenerateUrl() cmd := exec.Command(\"curl\", url) output, _ := cmd.CombinedOutput() fmt.Print(string(output)) } ","categories":["golang"],"description":"flag|标准库\n","excerpt":"flag|标准库\n","ref":"/golang/package/flag.html","tags":["golang","flag"],"title":"flag"},{"body":"接口 接口是一种抽象类型，它定义了一组方法，但没有实现。接口可以被任何类型实现，这意味着一个类型可以实现多个接口。通过接口，我们可以实现多态性，即一个对象可以具有多种类型。\n接口定义\ntype systemd interface { Start() error Stop() error } 接口实现，需要实现接口中定义的所有方法\npackage main import \"fmt\" type systemd interface { Start() error Stop() error } type Server struct { Name string } func (s *Server) Start() error { fmt.Println(s.Name, \"starting...\") return nil } func (s *Server) Stop() error { fmt.Println(s.Name, \"stoping...\") return nil } // 构造函数,实现了接口定义的所有方法的对象 也就是接口数据类型 func newServer() systemd { s := \u0026Server{\"Redis\"} return s } // 同样方法在定义一个client 结构体并实现 接口定义 type Client struct { Name string } func (c *Client) Start() error { fmt.Println(c.Name, \"starting...\") return nil } func (c *Client) Stop() error { fmt.Println(c.Name, \"stoping...\") return nil } func newClient() systemd { s := \u0026Client{\"Redis-cli\"} return s } func main() { server := newServer() server.Start() server.Stop() // client := newClient() client.Start() client.Stop() } 接口作为函数参数\n接口作为函数返回值\n接口类型的数据\n接口类型切片\npackage main import \"fmt\" type systemd interface { Start() error Stop() error } var system []systemd type Server struct { Name string } func (s *Server) Start() error { return nil } func (s *Server) Stop() error { return nil } func main() { // 实现了所有接口方法的结构体实例，都是这种数据类型 s := \u0026Server{\"Redis\"} system = append(system, s) fmt.Println(system[0]) //\u0026{Redis} } 接口类型map\n空接口\npackage main import \"fmt\" func main() { // 定义一个空接口 type empty interface { } //定义一个函数，参数为空接口类型。 func(test empty) { fmt.Println(test) }(3.14) //可变参数 func(test ...empty) { fmt.Println(test) }(\"hello world\", 1234) // 定义一个结构体,使用空接口类型 type test struct { a interface{} } t1 := test{\"hello world\"} t2 := test{3.14} fmt.Println(t1, t2) } 接口继承\n","categories":["golang"],"description":"接口|数据类型\n","excerpt":"接口|数据类型\n","ref":"/golang/interface.html","tags":["golang","接口"],"title":"接口"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\nfilebeat版本：7.17.19\nlogstash版本：7.15.0\nelasticsearch版本： 7.15.0\nINPUT 配置 redis\ndata_type[必选参数，数据类型为字符串] 可选值 list channel pattern_channel\nkey[必选参数，数据类型为字符串] 日志记录在那个名字的list /channel中\nhost[必选参数] redis绑定的服务ip\nport redis绑定的服务端口，默认6379\npassword redis服务密码\nthreads [数据类型为整型]多线程消费redis数据，默认1\ntags[所有类型intput都支持，数据类型为列表] 添加tag\ntags =\u003e [\"linux\",\"dev\"] add_field[所有类型intput都支持，数据类型为hash]添加自定义字段\nadd_field =\u003e { \"env\" =\u003e \"pro\" \"app\" =\u003e \"mobile\" } 示例\ncat \u003e/data/elk/logstash/config/logstash.conf\u003c\u003c'EOF' input { redis { data_type =\u003e \"list\" key =\u003e \"logstash:beat:logstash\" host =\u003e \"192.168.0.161\" password =\u003e \"mypass\" threads =\u003e 4 add_field =\u003e { \"env\" =\u003e \"pro\" \"app\" =\u003e \"mobile\" } tags =\u003e [\"linux\",\"dev\"] } } output { elasticsearch { hosts =\u003e [\"http://192.168.0.114:9200\"] index =\u003e \"logstash-%{+YYYY.MM.dd}\" #user =\u003e \"elastic\" #password =\u003e \"changeme\" } } EOF file\n将日志读取位置记录在sincedb 文件中，当重启logstash 后仍然可以继续读取。示例中记录在\n/data/elk/logstash/data/plugins/inputs/file/.sincedb_452905a167cf4509fd08acb964fdb20c\npath[必选参数，数据类型为数组]\nstart_position[数据类型字符串] logstash从哪里开始读取日志，可选参数begining end 默认为end。如果需要从头读取数据请设置为 begining\n该参数只作用在logstash 首次启动，也就是不存在sincedb 文件时\nexclude[数据类型为数组] 排除匹配的文件不跟踪\ninput { file { path =\u003e [\"/var/log/*\"] exclude =\u003e [\"*.gz\"] } } tags[所有类型intput都支持，数据类型为列表] 添加tag\ntags =\u003e [\"linux\",\"dev\"] add_field[所有类型intput都支持，数据类型为hash]添加自定义字段\nadd_field =\u003e { \"env\" =\u003e \"pro\" \"app\" =\u003e \"mobile\" } 示例\ncat \u003e/data/elk/logstash/config/logstash.conf\u003c\u003c'EOF' input { file { path =\u003e [\"/var/log/messages\"] start_position =\u003e \"beginning\" } } output { elasticsearch { hosts =\u003e [\"http://192.168.0.114:9200\"] index =\u003e \"logstash-%{+YYYY.MM.dd}\" #user =\u003e \"elastic\" #password =\u003e \"changeme\" } } EOF exec 周期调用系统命令，logstash收集命令输出\ncommand[必选参数，数据类型为字符串] 要执行的命令\ninterval 执行命令的间隔，单位s\ninput { exec { command =\u003e \"TERM=xterm /usr/bin/top -n 1 -b\" interval =\u003e 30 } } schedule按照crontab格式周期执行命令\ninput { exec { command =\u003e \"TERM=xterm /usr/bin/top -n 1 -b\" schedule =\u003e \"*/1 * * * *\" } } tags[所有类型intput都支持，数据类型为列表] 添加tag\ntags =\u003e [\"linux\",\"dev\"] add_field[所有类型intput都支持，数据类型为hash]添加自定义字段\nadd_field =\u003e { \"env\" =\u003e \"pro\" \"app\" =\u003e \"mobile\" } 示例\ncat \u003e/data/elk/logstash/config/logstash.conf\u003c\u003c'EOF' input { exec { command =\u003e \"TERM=xterm /usr/bin/top -n 1 -b\" interval =\u003e 30 } } output { elasticsearch { hosts =\u003e [\"http://192.168.0.114:9200\"] index =\u003e \"logstash-%{+YYYY.MM.dd}\" #user =\u003e \"elastic\" #password =\u003e \"changeme\" } } EOF http 用户 可以传递纯文本、JSON 或任何格式化数据\nhost 默认0.0.0.0\nport 默认8080\nthreads 线程数默认 1\nuser/password 开启用户名密码认证\ncat \u003e/data/elk/logstash/config/logstash.conf\u003c\u003c'EOF' input { http { host =\u003e \"0.0.0.0\" port =\u003e \"8080\" threads =\u003e 4 user =\u003e \"log\" password =\u003e \"Q!123\" } } output { elasticsearch { hosts =\u003e [\"http://192.168.0.114:9200\"] index =\u003e \"logstash-%{+YYYY.MM.dd}\" #user =\u003e \"elastic\" #password =\u003e \"changeme\" } } EOF curl -u 'log:Q!123' -X POST --data '{\"a\":\"123\"}' 192.168.0.115:8080 kafka 从kafka消费信息\ntopics[数据类型数组] 默认值 [\"logstash\"]\ntopics_pattern[数据类型字符串] 正则模式匹配\nconsumer_threads 默认值 1\n示例\ncat \u003e/data/elk/logstash/config/logstash.conf\u003c\u003c'EOF' input { kafka { bootstrap_servers =\u003e \"192.168.0.161:9092\" # Kafka 集群地址，多个用逗号分隔 topics =\u003e [\"app-log\"] # 消费的 topic group_id =\u003e \"logstash-consumer\" # 消费者组 ID auto_offset_reset =\u003e \"earliest\" # 从头开始（第一次无 offset 时） codec =\u003e \"json\" # 消息体是 JSON 就加，纯文本可删 decorate_events =\u003e true # 给事件加 @metadata[kafka] 字段（topic/partition/offset） } } output { elasticsearch { hosts =\u003e [\"http://192.168.0.114:9200\"] index =\u003e \"logstash-%{+YYYY.MM.dd}\" #user =\u003e \"elastic\" #password =\u003e \"changeme\" } } EOF ","categories":["ELK"],"description":"\nlogstash之input配置\r\n","excerpt":"\nlogstash之input配置\r\n","ref":"/elk/logstash/input.html","tags":["linux","logstash"],"title":"logstash-input"},{"body":"安装jupyter-notebook\nFROM python:3.11.12-alpine3.21 LABEL maintainer=1209233066@qq.com RUN apk add gcc musl-dev linux-headers RUN pip3 install akshare jupyter notebook pandas -i https://mirrors.aliyun.com/pypi/simple/ EXPOSE 80 WORKDIR /opt CMD [\"jupyter\",\"notebook\",\"--ip\",\"0.0.0.0\",\"--port\",\"80\",\"--allow-root\"] 启动jupyter-notebook\njupyter-notebook --ip 0.0.0.0 --port 80 --allow-root ","categories":["python"],"description":"jupyter-notebook 运行python程序\n","excerpt":"jupyter-notebook 运行python程序\n","ref":"/2025-08-12/jupyter-notebook.html","tags":["jupyter-notebook"],"title":"jupyter-notebook"},{"body":" 版本 旧环境 新环境 root密码 8.0.22 192.168.100.157 主库 4C 32GB 350G + 100GB 192.168.100.158 从库. 4C 32GB 350G + 0GB 已部署10.128.99.157 主库 主库 4C 32GB /Data= 350G + /Backup = 100GB 10.128.99.158 从库 从库. 4C 32GB /Data= 350G + /Backup = null d988ec6a93!@#MYSQL 全量迁移 并发请求不高，在主节点执行全量备份\nyum install -y percona-xtrabackup-80.x86_64 耗时14分钟\nxtrabackup \\ --defaults-file=/etc/mysql80/mysql80_6301.cnf \\ --backup \\ --user=\"root\" --password='d988ec6a93!@#MYSQL' --port=6301 \\ --target-dir=\"/Backup/fullbackup-$(date +%F)\" \\ --compress --compress-threads=1 2\u003e\u00261 | tee /tmp/mysql6301_$(date +%F_%H%M%S).log # 限速50MB # 10.128.99.157 10.128.99.158 #### service mysql stop #### rm -fr /Data/bak/* #### rm -fr /Data/mysql6301/* \u0026\u0026 mkdir /Data/mysql6301/{binlog,data,log} -p ;touch /Data/mysql6301/log/mysqld-error.log # 10分 scp -l 409600 -rp /Backup/fullbackup-2025-08-01/* bak@10.128.99.157:/Data/bak/ scp -l 409600 -rp /Backup/fullbackup-2025-08-01/* bak@10.128.99.158:/Data/bak/ 全量恢复\n从库备份时会将主从复制关系备份过来, 恢复数据时希望手动指定复制关系，可以通过只读参数取消slave进程启动\n[mysqld] skip_slave_start = ON # 解压 16gb 耗时 2分钟 xtrabackup --decompress --parallel=2 --target-dir=/Data/bak --remove-original # 应用undolog xtrabackup --prepare --apply-log-only --target-dir=/Data/bak xtrabackup --prepare --target-dir=/Data/bak # 拷贝全量备份到datadir xtrabackup --defaults-file=/etc/my.cnf --copy-back --target-dir=/Data/bak chown -R mysql.mysql /Data/mysql6301 service mysql start chkconfig mysql on 设置只读\nmysql -h 10.128.99.157 --port=6301 -uroot -p'd988ec6a93!@#MYSQL' -e \" set global super_read_only=on; set global read_only=on; show variables like 'super_read_only'; show variables like 'read_only'; \" mysql -h 10.128.99.158 --port=6301 -uroot -p'd988ec6a93!@#MYSQL' -e \" set global super_read_only=on; set global read_only=on; show variables like 'super_read_only'; show variables like 'read_only'; \" 增量同步 CREATE USER 'replUser'@'%' IDENTIFIED BY 'RsU#58d1@' WITH mysql_native_password ; GRANT REPLICATION SLAVE,REPLICATION CLIENT ON *.* TO 'replUser'@'%'; flush privileges; 查找全备时binlog位置\n[root@db3-mysql80-m ~]# cat /Data/bak/xtrabackup_binlog_info mysql-bin.000451 156 mysql -h 10.128.99.157 --port=6301 -uroot -p'd988ec6a93!@#MYSQL' -e \" reset slave all; CHANGE MASTER TO MASTER_HOST='192.168.100.157', MASTER_PORT=6301, MASTER_USER='replUser', MASTER_PASSWORD='RsU#58d1@', MASTER_LOG_FILE='mysql-bin.000451', MASTER_LOG_POS=156; start slave ; show slave status \\G \" mysql -h 10.128.99.158 --port=6301 -uroot -p'd988ec6a93!@#MYSQL' -e \" reset slave all; CHANGE MASTER TO MASTER_HOST='10.128.99.157', MASTER_PORT=6301, MASTER_USER='replUser', MASTER_PASSWORD='RsU#58d1@', MASTER_LOG_FILE='mysql-bin.000453', MASTER_LOG_POS=156; start slave ; show slave status \\G \" 数据对比 检查主从同步状态\nmysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 10.128.99.157 -e \"show slave status\\G show variables like '%read_only%'\"|grep -E \"Slave_IO_Running:|Slave_SQL_Running:|Seconds_Behind_Master:|read_only\" mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 10.128.99.158 -e \"show slave status\\G show variables like '%read_only%'\"|grep -E \"Slave_IO_Running:|Slave_SQL_Running:|Seconds_Behind_Master:|read_only\" 统计数据库元数据信息。确保数据库数量、表数量、索引数量、触发器、函数、存储过程、用户数量一致\nmysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 10.128.99.158 -e \" SELECT object_type, COUNT(*) AS count FROM ( SELECT 'Users' AS object_type FROM mysql.user UNION ALL SELECT 'Databases' FROM information_schema.SCHEMATA WHERE SCHEMA_NAME NOT IN ('mysql', 'information_schema', 'performance_schema', 'sys') UNION ALL SELECT 'Tables' FROM information_schema.TABLES WHERE TABLE_TYPE = 'BASE TABLE' AND TABLE_SCHEMA NOT IN ('mysql', 'information_schema', 'performance_schema', 'sys') UNION ALL SELECT 'Views' FROM information_schema.VIEWS WHERE TABLE_SCHEMA NOT IN ('mysql', 'information_schema', 'performance_schema', 'sys') UNION ALL SELECT 'Triggers' FROM information_schema.TRIGGERS WHERE TRIGGER_SCHEMA NOT IN ('mysql', 'information_schema', 'performance_schema', 'sys') UNION ALL SELECT 'Routines' FROM information_schema.ROUTINES WHERE ROUTINE_TYPE IN ('FUNCTION', 'PROCEDURE') AND ROUTINE_SCHEMA NOT IN ('mysql', 'information_schema', 'performance_schema', 'sys') UNION ALL SELECT 'Events' FROM information_schema.EVENTS WHERE EVENT_SCHEMA NOT IN ('mysql', 'information_schema', 'performance_schema', 'sys') UNION ALL SELECT 'Indexes' FROM information_schema.STATISTICS WHERE TABLE_SCHEMA NOT IN ('mysql', 'information_schema', 'performance_schema', 'sys') ) AS all_objects GROUP BY object_type WITH ROLLUP; \" 应用切割 网络层做隔离(网络工程师)\n旧库设置只读，杀连接 192.168.100.157\nmysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 192.168.100.157 -e \"set global read_only=on;set global super_read_only=on;show variables like '%read_only%';\" mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 192.168.100.157 -e \"SELECT CONCAT('KILL ', id, ';') FROM information_schema.processlist WHERE user NOT IN ('root', 'replUser','event_scheduler');\" | grep -v CONCAT \u003e /tmp/kill192.168.100.157 mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 192.168.100.157 \u003c /tmp/kill192.168.100.157 mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 192.168.100.157 -e \"show processlist;\" 旧库设置只读，杀连接 192.168.100.158\nmysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 192.168.100.158 -e \"set global read_only=on;set global super_read_only=on;show variables like '%read_only%';\" mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 192.168.100.158 -e \"SELECT CONCAT('KILL ', id, ';') FROM information_schema.processlist WHERE user NOT IN ('root', 'replUser','event_scheduler');\" | grep -v CONCAT \u003e /tmp/kill192.168.100.158 mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 192.168.100.158 \u003c /tmp/kill192.168.100.158 mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 192.168.100.158 -e \"show processlist;\" 停旧库和新库之间的数据同步\nmysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 10.128.99.157 -e \"show slave status\\G\" \u003e/tmp/last_slave_state sleep 5; mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 10.128.99.157 -e \"stop slave;reset slave all;show slave status\\G\" 记录当前binglog位置用于回滚\nmysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 10.128.99.157 -e \"show master status;\" \u003e/tmp/binlog_rollback_info 创建双主关系 10.128.99.157 –\u003e 10.128.99.158\n多次执行确认master 位置不在变化 mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 10.128.99.158 -e \"show master status;\" |tee /tmp/master1012899158 cat /tmp/master1012899158 mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 10.128.99.157 -e \" CHANGE MASTER TO MASTER_HOST='10.128.99.158', MASTER_PORT=6301, MASTER_USER='replUser', MASTER_PASSWORD='RsU#58d1@', MASTER_LOG_FILE='', MASTER_LOG_POS=; \" 启动主从关系\nmysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 10.128.99.157 -e \"start slave ; show slave status \\G\" 新库关闭只读\nmysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 10.128.99.157 -e \" set global read_only=off; set global super_read_only=off; show variables like '%read_only%'; \" mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 10.128.99.158 -e \" set global read_only=off; set global super_read_only=off; show variables like '%read_only%'; \" 观察连接情况\nmysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 10.128.99.157 -e \"show processlist;show variables like '%read_only%';\" mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 10.128.99.158 -e \"show processlist;show variables like '%read_only%';\" mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 192.168.100.157 -e \"show processlist;show variables like '%read_only%';\" mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 192.168.100.158 -e \"show processlist;show variables like '%read_only%';\" 回退环境搭建 mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 192.168.100.157 -e \" stop slave; reset slave all; show slave status \\G \" cat /tmp/binlog_rollback_info mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 192.168.100.157 -e \\ \"CHANGE MASTER TO MASTER_HOST='10.128.99.157', MASTER_PORT=6301, MASTER_USER='replUser', MASTER_PASSWORD='RsU#58d1@', MASTER_LOG_FILE='', MASTER_LOG_POS=;\" mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 192.168.100.157 -e \"start slave;show slave status\\G\" 执行回退 旧库设置只读，杀连接\nmysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 10.128.99.157 -e \"set global read_only=on;set global super_read_only=on;show variables like '%read_only%';\" mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 10.128.99.157 -e \"SELECT CONCAT('KILL ', id, ';') FROM information_schema.processlist WHERE user NOT IN ('root', 'replUser','event_scheduler');\" | grep -v CONCAT \u003e /tmp/kill10.128.99.157 mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 10.128.99.157 \u003c /tmp/kill10.128.99.157 mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 10.128.99.157 -e \"show processlist;\" 停旧库和新库之间的数据同步\n# 没有延迟后，重置主从同步，取消只读。 mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 192.168.100.157 -e \"stop slave;reset slave all;\" 恢复 192.168.100.157 –\u003e 192.168.100.158 的数据同步\n多次执行确认master 位置不在变化 mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 192.168.100.158 -e \"show master status;\" |tee /tmp/master192168100158 cat /tmp/master192168100158 mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 192.168.100.157 -e \" CHANGE MASTER TO MASTER_HOST='192.168.100.158', MASTER_PORT=6301, MASTER_USER='replUser', MASTER_PASSWORD='RsU#58d1@', MASTER_LOG_FILE='', MASTER_LOG_POS=; \" mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 192.168.100.157 -e \" start slave ;show slave status \\G \" mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 192.168.100.157 -e \" set global super_read_only=off; set global read_only=off;\" mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 192.168.100.158 -e \" set global super_read_only=off; set global read_only=off;\" 查看连接状态\nmysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 10.128.99.157 -e \"show processlist;show variables like '%read_only%';\" mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 10.128.99.158 -e \"show processlist;show variables like '%read_only%';\" mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 192.168.100.157 -e \"show processlist;show variables like '%read_only%';\" mysql -uroot -p'd988ec6a93!@#MYSQL' --port=6301 -h 192.168.100.158 -e \"show processlist;show variables like '%read_only%';\" ","categories":["mysql"],"description":"数据中心搬迁，龙腾出行mysql数据库迁移\n","excerpt":"数据中心搬迁，龙腾出行mysql数据库迁移\n","ref":"/mysql/longtengchuxing.html","tags":["迁移案例"],"title":"mysql数据库搬迁"},{"body":" 旧环境 新环境 密码 192.168.100.66、192.168.100.67 11G 10.128.100.66（主）、10.128.100.67（从） YSxeEr3e4ZdF 192.168.100.34 11G 10.128.100.34(主）、10.128.99.34(从) 192.168.100.57, 192.168.100.58, 192.168.100.59 6G 10.128.100.57, 10.128.100.58, 10.128.100.59 xb91830913039bhdk 内存空余的情况下，被同步的源端执行 # 默认 256m 64m 60 调整为 512m 128m 60 CONFIG set client-output-buffer-limit \"slave 536870912 134217728 60\" CONFIG GET client-output-buffer-limit # 默认1m 调整为1gb config set repl-backlog-size 1073741824 CONFIG GET repl-backlog-size 切换 redis01 redis02 redis01登录 10.128.100.66 杀连接\nredis-cli -a YSxeEr3e4ZdF -h 192.168.100.66 client list|awk -F \" |=\" '!/127.0.0.1/ \u0026\u0026 !/10.128.100.66/ \u0026\u0026 !/10.128.100.67/ {print \"client kill\", $4}'| redis-cli -a YSxeEr3e4ZdF -h 192.168.100.66 # redis-cli -a YSxeEr3e4ZdF -h 192.168.100.66 client list|awk -F \" |=\" '!/127.0.0.1/ \u0026\u0026 !/10.128.100.66/ \u0026\u0026 !/10.128.100.67/ {print \"client kill\", $4}' 停同步\n数据校验\nredis-cli -a YSxeEr3e4ZdF -h 192.168.100.66 info keyspace redis-cli -a YSxeEr3e4ZdF -h 10.128.100.66 info keyspace redis_check.sh --source-ip 192.168.100.67 --source-pwd YSxeEr3e4ZdF \\ --target-ip 10.128.100.66 --target-pwd YSxeEr3e4ZdF \\ --compare-mode 4 \\ --qps 5000 \\ --db-file redis01.db redis-cli -a YSxeEr3e4ZdF -h 10.128.100.66 client list redis-cli -a YSxeEr3e4ZdF -h 10.128.100.66 slaveof no one redis-cli -a YSxeEr3e4ZdF -h 10.128.100.66 info replication echo \"set a b\\n del a\" | redis-cli -a YSxeEr3e4ZdF -h 10.128.100.66 添加从节点\nredis-cli -a YSxeEr3e4ZdF -h 10.128.100.67 slaveof 10.128.100.66 6379 redis-cli -a YSxeEr3e4ZdF -h 10.128.100.67 info replication redis02登录 10.128.99.34 杀连接\nredis-cli -h 192.168.100.34 client list|awk -F \" |=\" '!/127.0.0.1/ \u0026\u0026 !/10.128.99.34/ \u0026\u0026 !/10.128.100.34/{print \"client kill\", $4}'|redis-cli -h 192.168.100.34 # redis-cli -h 192.168.100.34 client list|awk -F \" |=\" '!/127.0.0.1/ \u0026\u0026 !/10.128.99.34/ \u0026\u0026 !/10.128.100.34/{print \"client kill\", $4}' 停同步\n数据校验\nredis-cli -h 192.168.100.34 info keyspace redis-cli -h 10.128.100.34 info keyspace redis_check.sh --source-ip 192.168.100.34 \\ --target-ip 10.128.100.34 \\ --compare-mode 4 \\ --qps 5000 \\ --db-file redis02.db redis-cli -h 10.128.100.34 client list redis-cli -h 10.128.100.34 slaveof no one redis-cli -h 10.128.100.34 info replication echo \"set a b\\n del a\" | redis-cli -h 10.128.100.34 回退 ## redis01登录 10.128.100.66 清理旧库数据 # redis-cli -a YSxeEr3e4ZdF -h 192.168.100.66 bgsave # mv /dump.rdb /dump.rdb.$(date +%F\\ %H:%M:%S) # 确认没有从节点连接 # redis-cli -a YSxeEr3e4ZdF -h 192.168.100.66 info replication # redis-cli -a YSxeEr3e4ZdF -h 192.168.100.66 slaveof 10.128.100.66 6379 # redis-cli -a YSxeEr3e4ZdF -h 192.168.100.66 info replication ## redis02登录 10.128.99.34 清理旧库数据 # redis-cli -h 192.168.100.34 bgsave # mv /dump.rdb /dump.rdb.$(date +%F\\ %H:%M:%S) # 确认没有从节点连接 # redis-cli -h 192.168.100.34 info replication # redis-cli -a YSxeEr3e4ZdF -h 192.168.100.34 slaveof 10.128.100.34 6379 # redis-cli -a YSxeEr3e4ZdF -h 192.168.100.34 info replication redis01 redis02 redis01登录 10.128.100.66 杀连接\nredis-cli -a YSxeEr3e4ZdF -h 10.128.100.66 client list|awk -F \" |=\" '!/127.0.0.1/ \u0026\u0026 !/10.128.100.66/ \u0026\u0026 !/10.128.100.67/ \u0026\u0026 !/192.168.100.66/{print \"client kill\", $4}'| redis-cli -a YSxeEr3e4ZdF -h 10.128.100.66 # redis-cli -a YSxeEr3e4ZdF -h 10.128.100.66 client list|awk -F \" |=\" '!/127.0.0.1/ \u0026\u0026 !/10.128.100.66/ \u0026\u0026 !/10.128.100.67/ \u0026\u0026 !/192.168.100.66/{print \"client kill\", $4}' 停同步\n数据校验\nredis-cli -a YSxeEr3e4ZdF -h 192.168.100.66 info keyspace redis-cli -a YSxeEr3e4ZdF -h 10.128.100.66 info keyspace redis_check.sh --source-ip 10.128.100.66 --source-pwd YSxeEr3e4ZdF \\ --target-ip 192.168.100.66 --target-pwd YSxeEr3e4ZdF \\ --compare-mode 4 \\ --qps 50000 \\ --db-file redis01.db redis-cli -a YSxeEr3e4ZdF -h 192.168.100.66 client list redis-cli -a YSxeEr3e4ZdF -h 192.168.100.66 slaveof no one redis-cli -a YSxeEr3e4ZdF -h 192.168.100.66 info replication echo \"set a b\\n del a\" | redis-cli -a YSxeEr3e4ZdF -h 192.168.100.66 redis02登录 10.128.99.34 杀连接\n# redis02登录 10.128.99.34 杀连接 redis-cli -h 10.128.100.34 client list|awk -F \" |=\" '!/127.0.0.1/ \u0026\u0026 !/10.128.99.34/ \u0026\u0026 !/192.168.100.34/{print \"client kill\", $4}'|redis-cli -h 10.128.100.34 redis-cli -h 10.128.100.34 client list|awk -F \" |=\" '!/127.0.0.1/ \u0026\u0026 !/10.128.99.34/ \u0026\u0026 !/192.168.100.34/{print \"client kill\", $4}' 停同步\n数据校验\nredis-cli -h 192.168.100.34 info keyspace redis-cli -h 10.128.100.34 info keyspace redis_check.sh --source-ip 10.128.100.34 \\ --target-ip 192.168.100.34 \\ --compare-mode 4 \\ --qps 50000 \\ --db-file redis02.db # 停同步 redis-cli -h 192.168.100.34 client list redis-cli -h 192.168.100.34 slaveof no one redis-cli -h 192.168.100.34 info replication echo \"set a b\\n del a\" | redis-cli -h 192.168.100.34 收尾工作 停止集群的同步工具\n/Data/redis7/redis7_6379/bin/redis-cli -a xb91830913039bhdk --cluster call 192.168.100.58:6379 dbsize systemctl status redis_sync_cluster01 从节点接入到master\nredis-cli -h 10.128.99.34 slaveof 10.128.100.34 6379 redis-cli -h 10.128.100.67 -a YSxeEr3e4ZdF slaveof 10.128.100.66 6379 redis-cli -h 10.128.99.34 config set save \"900 1 300 10 60 10000\" redis-cli -h 10.128.99.34 config rewirte redis-cli -h 10.128.100.67 -a YSxeEr3e4ZdF config set save \"900 1 300 10 60 10000\" redis-cli -h 10.128.100.67 -a YSxeEr3e4ZdF config rewirte ","categories":["redis"],"description":"数据中心搬迁，龙腾出行redis数据库迁移\n","excerpt":"数据中心搬迁，龙腾出行redis数据库迁移\n","ref":"/redis/longtengchuxing2.html","tags":["迁移案例"],"title":"redis数据库搬迁方案2"},{"body":"","categories":["golang"],"description":"pflag|第三方库\n","excerpt":"pflag|第三方库\n","ref":"/golang/package/pflag.html","tags":["golang","pflag"],"title":"pflag"},{"body":"docker network create 8.4.0 --subnet 100.100.1.0/24 for i in {2..7};do docker rm -fv cmbredis${i};done for i in {2..7};do docker run -d \\ --name=cmbredis${i} \\ --net=8.4.0 --ip=\"100.100.1.${i}\" \\ -e SALT_MASTER_IP_LIST=\"192.168.0.1\" \\ -e RENAME_CMD_CONFIG=pcloud_config \\ -e RENAME_CMD_BGSAVE=bgsave \\ -e RENAME_CMD_DEBUG=debug \\ -e RENAME_CMD_SAVE=save \\ -e RENAME_CMD_SHUTDOWN=shutdown \\ -e RENAME_CMD_SLAVEOF=slaveof \\ -e RENAME_CMD_BGREWRITEAOF=bgrewriteaof \\ -e REDIS_PASSWORD=123 \\ cmb-redis:8.4.0-alpine313; done docker exec -it cmbredis2 redis-cli --user default -a 123 --cluster create \\ --cluster-replicas 1 \\ 100.100.1.2:6379 100.100.1.3:6379 100.100.1.4:6379 100.100.1.5:6379 100.100.1.6:6379 100.100.1.7:6379 ","categories":["Redis"],"description":"背景:Redis5.0.14 被爆出存在多种安全漏洞，为规避漏洞考虑升级为8.4.0\n","excerpt":"背景:Redis5.0.14 被爆出存在多种安全漏洞，为规避漏洞考虑升级为8.4.0\n","ref":"/redis/8.4.0.html","tags":["8.x"],"title":"Redis8.4.0版本"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\nfilebeat版本：7.17.19\nlogstash版本：7.15.0\nelasticsearch版本： 7.15.0\nOUTPUT配置 elasticsearch\nhosts[非必须，数据类型数组] 默认值http://127.0.0.1:9200\nindex[非必须] 默认值 logstash-%{+YYYY.MM.dd}\nuser\npassword\n示例：\ncat \u003e/data/elk/logstash/config/logstash.conf\u003c\u003c'EOF' input { kafka { bootstrap_servers =\u003e \"192.168.0.161:9092\" # Kafka 集群地址，多个用逗号分隔 topics =\u003e [\"app-log\",\"sys-log\"] # 消费的 topic group_id =\u003e \"logstash-consumer\" # 消费者组 ID auto_offset_reset =\u003e \"earliest\" # 从头开始（第一次无 offset 时） codec =\u003e \"json\" # 消息体是 JSON 就加，纯文本可删 decorate_events =\u003e true # 给事件加 @metadata[kafka] 字段（topic/partition/offset） consumer_threads =\u003e 5\t# 开启多线程消费kafka # --------- SASL/SCRAM 认证（与 Kafka 保持一致） --------- #security_protocol =\u003e \"SASL_PLAINTEXT\" #sasl_mechanism =\u003e \"SCRAM-SHA-256\" #sasl_jaas_config =\u003e 'org.apache.kafka.common.security.scram.ScramLoginModule required # username=\"test\" # password=\"pytc@2026\";' } } output { elasticsearch { hosts =\u003e [\"http://192.168.0.114:9200\"] index =\u003e \"logstash-%{+YYYY.MM.dd}\" #user =\u003e \"elastic\" #password =\u003e \"changeme\" } } EOF 上面的示例中所有topic日志都收集到了同一个索引中，接下来我们将不同日志分流到不同的index\n都有哪些@metadata 元数据\ncat \u003e/data/elk/logstash/config/logstash.conf\u003c\u003c'EOF' input { kafka { bootstrap_servers =\u003e \"192.168.0.161:9092\" # Kafka 集群地址，多个用逗号分隔 topics =\u003e [\"app-log\",\"sys-log\"] # 消费的 topic group_id =\u003e \"logstash-consumer\" # 消费者组 ID auto_offset_reset =\u003e \"earliest\" # 从头开始（第一次无 offset 时） codec =\u003e \"json\" # 消息体是 JSON 就加，纯文本可删 decorate_events =\u003e true # 给事件加 @metadata[kafka] 字段（topic/partition/offset） consumer_threads =\u003e 5\t# 开启多线程消费kafka # --------- SASL/SCRAM 认证（与 Kafka 保持一致） --------- #security_protocol =\u003e \"SASL_PLAINTEXT\" #sasl_mechanism =\u003e \"SCRAM-SHA-256\" #sasl_jaas_config =\u003e 'org.apache.kafka.common.security.scram.ScramLoginModule required # username=\"test\" # password=\"pytc@2026\";' } } output { if [@metadata][kafka][topic] == \"app-log\" { elasticsearch { hosts =\u003e [\"http://192.168.0.114:9200\"] index =\u003e \"app-log-%{+YYYY.MM.dd}\" } } if [@metadata][kafka][topic] == \"sys-log\" { elasticsearch { hosts =\u003e [\"http://192.168.0.114:9200\"] index =\u003e \"sys-log-%{+YYYY.MM.dd}\" } } } EOF 通过自定义索引模版，并将sys-log-*和 app-log-* 索引关联该模版\n# 如果 ES 7.x 也可用老接口 _template，这里用新版 _index_template curl -X PUT localhost:9200/_index_template/host-logs-template \\ -H 'Content-Type: application/json' \\ -d '{ \"template\": { \"settings\": { \"number_of_shards\": 3, \"number_of_replicas\": 0, \"refresh_interval\": \"5s\" } }, \"index_patterns\": [ \"sys-log-*\", \"app-log-*\" ] }' cat \u003e/data/elk/logstash/config/logstash.conf\u003c\u003c'EOF' input { kafka { bootstrap_servers =\u003e \"192.168.0.161:9092\" # Kafka 集群地址，多个用逗号分隔 topics =\u003e [\"app-log\",\"sys-log\"] # 消费的 topic group_id =\u003e \"logstash-consumer\" # 消费者组 ID auto_offset_reset =\u003e \"earliest\" # 从头开始（第一次无 offset 时） codec =\u003e \"json\" # 消息体是 JSON 就加，纯文本可删 decorate_events =\u003e true # 给事件加 @metadata[kafka] 字段（topic/partition/offset） consumer_threads =\u003e 5\t# 开启多线程消费kafka # --------- SASL/SCRAM 认证（与 Kafka 保持一致） --------- #security_protocol =\u003e \"SASL_PLAINTEXT\" #sasl_mechanism =\u003e \"SCRAM-SHA-256\" #sasl_jaas_config =\u003e 'org.apache.kafka.common.security.scram.ScramLoginModule required # username=\"test\" # password=\"pytc@2026\";' } } output { if [@metadata][kafka][topic] == \"app-log\" { elasticsearch { hosts =\u003e [\"http://192.168.0.114:9200\"] index =\u003e \"app-log-%{+YYYY.MM.dd}\" } } if [@metadata][kafka][topic] == \"sys-log\" { elasticsearch { hosts =\u003e [\"http://192.168.0.114:9200\"] index =\u003e \"sys-log-%{+YYYY.MM.dd}\" } } } EOF ","categories":["ELK"],"description":"\nlogstash之output配置\r\n","excerpt":"\nlogstash之output配置\r\n","ref":"/elk/logstash/output.html","tags":["linux","logstash"],"title":"logstash-output"},{"body":"📅 2025年8月15日09:04:21\nremote [adj]\n(时间/距离) 遥远的、偏僻的 ，同义词distant。示例: in remote future 在遥远的未来 (关系)疏远的、远离的、冷淡的，同义词distant。示例：his wife was a remote woman 他的妻子是个冷淡的人 极不相同的，相差很大的。示例：this question is remote from the topic under discussion 这个问题不在我讨论范围内 remove [v]\n搬迁、移动。示例：Due to the flood,the residents removed to a safer location 居民因洪水被搬迁到更安全的地方\n移开，挪走。 示例: The waiter removed a pile of the dishes to the kitchen 服务员把一堆餐具挪到了厨房\n去除、消除、脱去（衣服等）。示例：\nremove grease from clothes 去除衣服上的污渍 remove EdWin's doubt 打消edwin的疑虑 the threat of redundancy was suddenly removed 裁员威胁一下子消除了 开除、撤职。 示例：the chief of police was removed from office for failing to do his duty 警察局长因渎职而被免职\n📅2025年8月19日08:58:23\nremoval [n]\n迁移、迁居。示例：A van come for the removal of our furniture 使用一辆货车搬走了我们的家具\n去除、切除。示例：After removal soup,fish was served. 撤掉汤，鱼就上了\n​\tthe removal of tonisls 扁桃体切除\n免职、开除。示例：A surge of popular discontent led to the removal of the president from office 民众不满情绪的激增导致总统被免职\nremain [v]\n剩下、余留。示例：In the late autumn,few apple remined on the tree. 晚秋，树上没剩几个苹果\n仍然是、依旧是、保持不变。示例：to remain slient.保持沉默\n​\tThe affair remained a complete mystery. 整件事仍然是个谜\nit remains to be seen 情况仍未明确，要看怎么发展【固定词组】\nthere are many different types of automatic door, with each relying on specific signals to tell them when to open. although these methods differ,the main principles remain the same.自动门有许多不同类型，每种类型都依靠特定的信号来告诉它们如何打开。尽管这些方法不同，但主要原理保持相同 remainder [n]\n剩余物，剩余时间，残余部分，其他人员。示例：The old woman will spend the remiander of her life in the country. 老妇人将在乡下度过余生 余数、余项、差数。示例：The remainder when 17 divided by 4 is 1. 17除4余数为1 remains [n]\n残留(物)、废墟、遗迹、遗体。示例：here is a remains of a temple. 这里是一座寺庙的废墟 📅 2025年8月28日15:53:58\nremedy\n【n】\n治疗、治疗法、药品。示例：this doctor often use herbal remedies 这位医生经常使用草药 补救、纠正（办法）。示例 your only remedy is to appeal to law 你唯一的补救措施是诉诸法律 【vt】\n医治、治疗。示例：Aspilin may remedy headache. 阿司匹林可以治疗头痛 补救、纠正。示例：remedy a loss 补偿损失 革除、消除。示例：The government took over the railways hoping to remedy confusion and inefficiency 政府接管了铁路希望消除混乱和低效问题。 remedy for ... 对xxx的补救 remember [v]\n记得、回想起、不忘记。示例：It's to be remembered that life isn't always a bed of roses 记住,生活并非总是一帆风顺的 向…送礼、代…问候。示例：He always remembers me on my birthday 他总是在我生日的时候送我礼物 remember as ... 记得（某人、某物原来样子）是... remember in 遵照遗嘱把财产留给某人 remind [vt]\n提醒、使想起。示例：She asked me over telephone to remind you about the party. 她电话通知我提醒你参加聚会\nThe old lady reminded me of my dead mother. 这位老太太让我想起了已经去世的母亲\n派生词 reminder [n] 引起回忆的事、提示信。示例：Britain's new rule is a reminder to bankers that society has an interest in their perfrmance ,not just for the short term but for the long term. 英国的新规定提醒银行家们，社会对他们的业绩感兴趣，不仅是短期业绩，还包括长期业绩\nrender [v]\n使得，使称为。示例：The tone rendered the statement in unsult.这种语气使这句话成为一种侮辱\n给予，回报。示例：If we can at any time render you a similar service,we shall be glad to do so .如果我们能在任何时候为您提供类似的服务，我们将很乐意这样做 提出，呈报。示例：she needed him to hear her out and render advice. 她需要他倾听，并清楚建议\nrender into 翻译\nrender a service to... 帮助…\nrender up 移交或交出某物，屈服、放弃\n📅 2025年9月5日16:54:56\ncontemporary [adj]\n同时代的、同时发生的。示例: The event contemporary with the Second world War.这个事件与第二次时间大战同时发生\n现代的，当代的。示例：contemporary cars are more streamlined than older ones 当代汽车更具流线型\ncontemporary art 当代艺术\ncontemporary music 当代音乐\ncontempt [n]\n轻视、蔑视。示例：His contempt for foreigners is obvious 他对外国人的蔑视是显而易见的 content\n📅 2025年11月17日21:19:07\nhints [n] 提示、建议、提示词。示例：The hints given by the teacher are very helpful. 教师给出的提示非常有帮助 ","categories":["英语"],"description":"每日积累5个单词，预计一年学习1000 个\n","excerpt":"每日积累5个单词，预计一年学习1000 个\n","ref":"/2025-08-15/word.html","tags":["英语单词"],"title":"英语单词"},{"body":"","categories":["golang"],"description":"cobar|第三方库 处理命令行参数\n","excerpt":"cobar|第三方库 处理命令行参数\n","ref":"/golang/package/cobra.html","tags":["golang","cobra"],"title":"cobra"},{"body":"","categories":["golang"],"description":"viper|第三方库 处理配置管理\n","excerpt":"viper|第三方库 处理配置管理\n","ref":"/golang/package/viper.html","tags":["golang","viper"],"title":"viper"},{"body":" 文档环境 os 版本： CentOS Linux release 7.9.2009 (Core)\nfilebeat版本：7.17.19\nlogstash版本：7.15.0\nelasticsearch版本： 7.15.0\nfilter配置 grok 将非结构化的日志格式化为结构化可查询的内容\napache 访问日志示例\n39.156.66.10 - - [27/Jun/2023:20:04:38 +0800] \"GET /images/apache_pb.gif HTTP/1.1\" 200 2326 \"http://10.4.7.250/\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36 Edg/113.0.1774.57\" 使用grok 对message执行解析\ncat \u003e/data/elk/logstash/config/logstash.conf\u003c\u003c'EOF' input { kafka { bootstrap_servers =\u003e \"192.168.0.161:9092\" # Kafka 集群地址，多个用逗号分隔 topics =\u003e [\"app-log\",\"sys-log\"] # 消费的 topic group_id =\u003e \"logstash-consumer\" # 消费者组 ID auto_offset_reset =\u003e \"earliest\" # 从头开始（第一次无 offset 时） codec =\u003e \"json\" # 消息体是 JSON 就加，纯文本可删 decorate_events =\u003e true # 给事件加 @metadata[kafka] 字段（topic/partition/offset） } } filter { grok { match =\u003e {\"message\" =\u003e \"%{COMBINEDAPACHELOG}\" } } geoip { source =\u003e \"clientip\" } } output { if [@metadata][kafka][topic] == \"app-log\" { elasticsearch { hosts =\u003e [\"http://192.168.0.114:9200\"] index =\u003e \"app-log-%{+YYYY.MM.dd}\" } } if [@metadata][kafka][topic] == \"sys-log\" { elasticsearch { hosts =\u003e [\"http://192.168.0.114:9200\"] index =\u003e \"sys-log-%{+YYYY.MM.dd}\" } } } EOF ","categories":["ELK"],"description":"\nlogstash之filter配置\r\n","excerpt":"\nlogstash之filter配置\r\n","ref":"/elk/logstash/filter.html","tags":["linux","logstash"],"title":"logstash-filter"},{"body":" 壶口瀑布（AAAAA级）: 概述 最佳游览时间 主要景点 位于陕西省宜川县，晋陕大峡谷中段\n春季（3-5月）：冰地解冻，水势增大，桃花盛开，有“三月桃花汛”之称。 秋季（9-11月）：雨季过去，河水奔腾，有“壶口秋风”之说。 孟门山：位于壶口瀑布下游，是两块梭形巨石，俯瞰如舟，远眺如门。 龙洞：位于壶口瀑布下游，是一个天然石洞，可以仰观壶口瀑布，感受“黄河之水天上来”的壮丽景观。 空中悬壶：黄河水奔腾呼啸，跃入深潭，溅起巨大的浪花，如巨壶翻滚 小径湾: 概述 最佳游览时间 主要景点 惠州市大亚湾区霞涌街道，适合游泳、日光浴和赶海\n最佳季节：5-10月海水温暖，适合下水\n摩托艇、帆船、桨板、沿海绿道骑行\n盐田海滨栈道: 概述 最佳游览时间 主要景点 杨梅坑: 概述 最佳游览时间 主要景点 西涌天文台: 概述 最佳游览时间 主要景点 张掖七彩丹霞\n敦煌鸣沙山月牙泉\n黑独山: 概述 最佳游览时间 主要景点 黑独山——地球上最像月球的“水墨丹青“，位于青海省海西州茫崖市冷湖镇\n全年可拍：4–10月气候温和；冬季冷至−20℃但雪景别有风味。\n黄金时段：\n日出前30 min蓝调→山体呈银灰； 傍晚17:30–19:30低角度侧光→黑白灰层次最丰富，夕阳照胭脂山显粉； 夜晚22:00后肉眼可见银河，可拍“月面+星空”剪影。 孟门山：位于壶口瀑布下游，是两块梭形巨石，俯瞰如舟，远眺如门。 龙洞：位于壶口瀑布下游，是一个天然石洞，可以仰观壶口瀑布，感受“黄河之水天上来”的壮丽景观。 空中悬壶：黄河水奔腾呼啸，跃入深潭，溅起巨大的浪花，如巨壶翻滚 ","categories":["假期"],"description":"收集值得去的地方\n","excerpt":"收集值得去的地方\n","ref":"/2025-10-6/vacation.html","tags":["旅游","美食"],"title":"吃喝玩乐"},{"body":"股市 航天 机器人 智能驾驶 通过python 爬虫获取涨停板信息实现选股\n新知识 老男孩教育的阿里云使用方法 白丁client-go Client-Go 之 Indexer 原理分析及示例演示_哔哩哔哩_bilibili 1. 控制器是什么？_哔哩哔哩_bilibili 4. Kubebuilder 的介绍与安装_哔哩哔哩_bilibili client-go 是与k8s api交互的golang客户端\noperator = crd+ controller\n华菱线缆 看多 l不低于22.4 加仓\n","categories":["2026"],"description":"2026年规划和指引\n","excerpt":"2026年规划和指引\n","ref":"/2026-01-01/plan.html","tags":["计划"],"title":"2026年指引"},{"body":"raft package - github.com/coreos/etcd/raft - Go Packages\nRaft 是一种协议，节点集群可以使用它来维护复制的状态机。 状态机通过使用复制的日志保持同步。 有关 Raft 的更多详细信息，请参阅“寻找可理解的共识算法” （https://ramcloud.stanford.edu/raft.pdf） 迭戈·翁加罗 （Diego Ongaro） 和约翰·奥斯特豪特 （John Ousterhout）。\n","categories":["golang"],"description":"raft|第三方库 实现选举和数据复制\n","excerpt":"raft|第三方库 实现选举和数据复制\n","ref":"/golang/package/raft.html","tags":["golang","raft"],"title":"raft"},{"body":"package main import ( \"fmt\" \"github.com/google/uuid\" ) func main() { fmt.Printf(\"%s\",uuid.New().String()) } ","categories":["golang"],"description":"raft|第三方库 生成uuid\n","excerpt":"raft|第三方库 生成uuid\n","ref":"/golang/package/uuid.html","tags":["golang","uuid"],"title":"uuid"},{"body":"package main import ( \"fmt\" \"github.com/go-redis/redis/v7\" ) func main() { cluster := redis.NewClusterClient(\u0026redis.ClusterOptions{ Addrs: []string{\"10.4.7.250:6379\", \"10.4.7.250:6380\", \"10.4.7.250:6381\"}, }) defer cluster.Close() pong, err := cluster.Ping().Result() if err != nil { fmt.Println(err) } for i := 0; i \u003c 255; i++ { cluster.LPush(\"ip_list\", i) } fmt.Println(pong) } ","categories":["golang"],"description":"raft|第三方库 操作redis数据库\n","excerpt":"raft|第三方库 操作redis数据库\n","ref":"/golang/package/redis.html","tags":["golang","redis"],"title":"redis"},{"body":" 范围 常用 Exporter promethues组件 prometheus(9090)、pushgateway(9091)、alertmanager(9093)、grafana(3000)、consul(8500) k8s组件 etcd(2379)、apiserver(6443)、scheduler、controllermanager、kubeproxy、kubelet、coredns(9153)、flanneld、traefik/nginx-ingress、cadvisor(8080)、kube_state_metrics 数据库 MySQL Exporter、Redis Exporter(9121)、MongoDB Exporter、MSSQL Exporter 等 硬件 Apcupsd Exporter、IoT Edison Exporter、IPMI Exporter、Node Exporter(9100) 等 消息队列 Zookeeper Exporter、Kafka Exporter、RabbitMQ Exporter、Beanstalkd Exporter 、NSQ Exporter等 存储 Minio、 Ceph Exporter、Gluster Exporter、HDFS Exporter、ScaleIO Exporter 等 HTTP服务 Apache Exporter、HAProxy Exporter、Nginx Exporter 等 API服务 AWS ECS Exporter、Docker Cloud Exporter、Docker Hub Exporter、GitHub Exporter 等 日志 Fluentd Exporter、Grok Exporter 等 监控系统 Collectd Exporter、Graphite Exporter、InfluxDB Exporter、Nagios Exporter、SNMP Exporter 等 java jmx_exporter、tomcat 其它 Blockbox Exporter、JIRA Exporter、Jenkins Exporter、Confluence Exporter 等 ","categories":"","description":"","excerpt":" 范围 常用 Exporter promethues组件 prometheus(9090)、pushgateway(9091)、alertmanager(9093)、grafana(3000)、consul(8500) k8s组件 …","ref":"/prometheus/exporter/","tags":"","title":"exporter"},{"body":"提供操作系统级别的监控指标，cpu memory disk space diskio network\n部署安装: 二进制 kubernetes wget https://github.com/prometheus/node_exporter/releases/download/v1.6.1/node_exporter-1.6.1.linux-amd64.tar.gz tar xf node_exporter-1.6.1.linux-amd64.tar.gz mv node_exporter-*.linux-amd64/node_exporter /usr/bin tee /usr/lib/systemd/system/node_exporter.service \u003c\u003c'EOF' [Unit] Description=node_exporter service https://prometheus.io/ After=network.target [Service] ExecStart=/usr/bin/node_exporter \\ --web.listen-address=:9100 \\ --web.telemetry-path=/metrics \\ --collector.systemd \\ --collector.systemd.unit-include=\"(sshd|docker|rsyslog|kubelet|kube-proxy).service\" \\ --no-collector.arp \\ --log.format=json User=root [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable node_exporter --now systemctl status node_exporter apiVersion: apps/v1 kind: DaemonSet metadata: name: node-exporter namespace: kube-system spec: selector: matchLabels: app: node-exporter template: metadata: labels: app: node-exporter spec: tolerations: - key: \"\" operator: \"Exists\" volumes: - name: root hostPath: path: / hostNetwork: true hostPID: true containers: - name: node-exporter image: quay.io/prometheus/node-exporter:v1.6.1 args: - --path.rootfs=/host volumeMounts: - mountPath: /host name: root ​\n添加prometheus配置: 静态配置 kuernetes_sd_configs - job_name: node-exporter honor_timestamps: true scrape_interval: 1m scrape_timeout: 20s metrics_path: /metrics scheme: http static_configs: - targets: - 127.0.0.1:9100 kubeconfig_file 和 api_server 二选其一\n- job_name: node-exporter kubernetes_sd_configs: - kubeconfig_file: /root/.kube/config role: node relabel_configs: - source_labels: [__address__] regex: (.*):(.*) replacement: \"$1:9100\" target_label: __address__ - job_name: node-exporter-ca kubernetes_sd_configs: - api_server: https://192.168.0.244:6443 tls_config: ca_file: /etc/kubernetes/pki/ca.crt cert_file: /etc/kubernetes/pki/apiserver-kubelet-client.crt key_file: /etc/kubernetes/pki/apiserver-kubelet-client.key # 测试改成false也没问题 insecure_skip_verify: true role: node relabel_configs: - source_labels: [__address__] regex: (.*):(.*) replacement: \"$1:9100\" target_label: __address__ 由于node_export 返回大量指标，通过prometheus配置文件的collect 收集指定的指标\ncurl -g -X GET 127.0.0.1:9100/metrics?collect[]=xfs curl -g -X GET 127.0.0.1:9100/metrics?collect[]=cpu\nglobal: external_labels: prometheus: prom-xxx scrape_configs: - job_name: node_exporter params: collect[]: - cpu - meminfo - netstat - systemd - xfs - filefd - filesystem kubernetes_sd_configs: - api_server: https://127.0.0.1:6443 tls_config: ca_file: /etc/kubernetes/pki/ca.pem cert_file: /etc/kubernetes/pki/cert.pem key_file: /etc/kubernetes/pki/cert.key # 测试改成false也没问题 insecure_skip_verify: true role: node relabel_configs: - source_labels: [__address__] regex: (.*):(.*) replacement: \"$1:9100\" target_label: __address__ 常用指标\n指标 释义 指标类型 irate(node_cpu_seconds_total{job=\"node_exporter\",cpu=\"0\"}[5m]) cpu0 每秒使用率 counter avg(irate(node_cpu_seconds_total{job=\"node_exporter\",cpu=\"0\"}[5m]))by(mode) cpu0 平均使用率 count(node_cpu_seconds_total{mode=\"idle\"})by(instance) 查询有几个逻辑核心 node_memory_MemTotal_bytes 内存总量 node_memory_Buffers_bytes buffer node_memory_Cached_bytes cache node_memory_MemFree_bytes free node_memory_MemAvailable_bytes 可用内存 node_memory_SUnreclaim_bytes 不可回收slab node_vmstat_pswpin 磁盘加载到内存的字节数/s node_vmstat_pswpout 内存换出到磁盘的字节数/s node_filesystem_size_bytes mount的文件系统大小 node_filesystem_free_bytes mount的文件系统空闲 node_systemd_unit_state systemd管理服务状态 node_uname_info 主机名/os版本的信心 node_filesystem_readonly 文件系统只读 node_netstat_Tcp_CurrEstab tcp连接数 # 入栈流量 rate(node_network_receive_bytes_total{device=\"eth0\"}[5m]) # 出栈流量 rate(node_network_transmit_bytes_total{device=\"eth0\"}[5m]) 告警规则\n# 1. 运行时间 # 2. cpu # 3. 内存 # 4. 磁盘 # 5. tcp 连接 # 6. 网络流量 groups: - name: node rules: # 机器宕机告警 - alert: NodeDown expr: up{job=~\"node(_|-|)exporter\"} == 0 for: 5m keep_firing_for: 1m labels: severity: critical annotations: summary: \"主机宕机 ({{ $labels.instance }})\" description: \"实例 {{ $labels.instance }} 已宕机超过5分钟 (Job: {{ $labels.job }})\" # 文件系统只读告警 - alert: FilesystemReadOnly expr: node_filesystem_readonly{mountpoint=\"/\"} == 1 for: 5m keep_firing_for: 1m labels: severity: critical annotations: summary: \"主机 ({{ $labels.instance }})挂载点 {{ $labels.mountpoint }} 只读\" description: \"实例 {{ $labels.instance }} 挂载点 {{ $labels.mountpoint }} 变为只读超过5分钟 (Job: {{ $labels.job }})\" # CPU使用率告警 - alert: NodeCpuUsageHigh expr: | 100 - ( avg by(instance) ( rate(node_cpu_seconds_total{mode=\"idle\"}[5m]) ) * 100 ) \u003e 85 for: 5m keep_firing_for: 1m labels: severity: warning annotations: summary: \"主机CPU使用率过高 ({{ $labels.instance }})\" description: \"实例 {{ $labels.instance }} CPU使用率持续5分钟超过85%（当前值：{{ $value | printf \\\"%.2f\\\"}}%）\" # 系统负载告警 - alert: NodeLoad1High expr: | node_load1 \u003e on(instance) count by(instance) (node_cpu_seconds_total{mode=\"idle\"}) * 2 for: 5m keep_firing_for: 1m labels: severity: warning annotations: summary: \"主机1分钟负载过高 ({{ $labels.instance }})\" description: \"实例 {{ $labels.instance }} 1分钟负载超过CPU核心数2倍（当前值：{{ $value | printf \\\"%.2f\\\"}}）\" - alert: NodeLoad15High expr: | node_load15 \u003e on(instance) count by(instance) (node_cpu_seconds_total{mode=\"idle\"}) * 2 for: 5m keep_firing_for: 1m labels: severity: critical annotations: summary: \"主机15分钟负载过高 ({{ $labels.instance }})\" description: \"实例 {{ $labels.instance }} 15分钟负载持续超过CPU核心数2倍（当前值：{{ $value | printf \\\"%.2f\\\"}}）\" # 内存使用率告警 - alert: NodeMemoryUsageHigh expr: | (1 - node_memory_MemAvailable_bytes{job=~\"node.*exporter\"} / node_memory_MemTotal_bytes ) * 100 \u003e 90 for: 5m keep_firing_for: 1m labels: severity: critical annotations: summary: \"主机内存使用率过高 ({{ $labels.instance }})\" description: \"实例 {{ $labels.instance }} 内存使用率持续5分钟超过90%（当前值：{{ $value | printf \\\"%.2f\\\"}}%）\" # 磁盘使用率告警 - alert: NodeDiskUsageHigh expr: | (1 - node_filesystem_free_bytes{ fstype!~\"^(tmpfs|rootfs|autofs|devpts|devtmpfs|overlay)$\", mountpoint!~\"^/(boot|run|var/lib/docker).*\" } / node_filesystem_size_bytes ) * 100 \u003e 85 for: 5m keep_firing_for: 1m labels: severity: warning annotations: summary: \"主机磁盘使用率过高 ({{ $labels.instance }}:{{ $labels.mountpoint }})\" description: \"实例 {{ $labels.instance }} 挂载点 {{ $labels.mountpoint }} 使用率超过85%（当前值：{{ $value | printf \\\"%.2f\\\"}}%）\" # 磁盘空间预测告警 - alert: NodeDiskWillFillIn8H expr: | predict_linear( node_filesystem_free_bytes{ fstype!~\"^(tmpfs|rootfs|autofs|devpts|devtmpfs|overlay)$\", mountpoint!~\"^/(boot|run|var/lib/docker).*\" }[6h], 8*3600 ) \u003c 0 for: 5m keep_firing_for: 1m labels: severity: warning annotations: summary: \"主机磁盘空间即将耗尽 ({{ $labels.instance }}:{{ $labels.mountpoint }})\" description: \"实例 {{ $labels.instance }} 挂载点 {{ $labels.mountpoint }} 预计8小时内空间将耗尽（当前预测值：{{ $value | printf \\\"%.2f\\\"}}）\" # 网络流量 cpu饱和度\n通过1分钟 5分钟 15 分钟的负载展示。一般负载小于cpu 核心数1.5倍\nnode_load1 \u003eon (instance) ( count by(instance) (node_cpu_seconds_total{mode=\"idle\"})*1 ) node_load5 node_load15 内存使用率\n[INFO]\nnode_memmory_MemeTotal_bytes node_memmory_MemFress_bytes node_memmory_Buffers_bytes node_memmory_Cached_bytes\n1-(node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) \u003e0.85 内存饱和度,判断内存的繁忙程度\n磁盘到到内存的 字节数/s node_vmstat_pswpin 内存到到磁盘的 字节数/s node_vmstat_pswpout rate(node_vmstat_pswpout[1m])+rate(node_vmstat_pswpin[1m]) sum(rate(node_vmstat_pswpout[1m])+rate(node_vmstat_pswpin[1m]))by(instance) *1024 磁盘使用率\nnode_filesystem_size_bytes{mountpoint=\"/\"} # 总大小 node_filesystem_free_bytes{mountpoint=\"/\"} # 空闲大小 #使用比例 (1-(node_filesystem_free_bytes{mountpoint=\"/\"}/node_filesystem_size_bytes{mountpoint=\"/\"}))\u003e0.85 使用predict_linear线性函数预测未来4个小时中磁盘的剩余空间\npredict_linear(node_filesystem_free_bytes{mountpoint=\"/\"}[1h],4*3600)\u003c0 入栈流量速率\nirate(node_network_receive_bytes_total{device=\"eth0\"}[5m]) 出栈网络速率\nirate(node_network_transmit_bytes_total{device=\"eth0\"}[5m]) 服务异常\nnode_systemd_unit_state{name=\"docker.service\"} == 0 bond 文件描述符 sr-iov 监控 tcp\n附录： 自定义指标\nnode_exporter 允许用户自定义监控指标，具体方法如下：\n修改node_exportrer启动文件,添加如下选项\n--collector.textfile \\ --collector.textfile.directory=\".\" 在 --collector.textfile.directory= 定义的目录下写入要提供的指标内容，文件以.prom 结尾\nvi httpcod.prom #输入示例： method_code:http_errors:rate5m{method=\"get\", code=\"500\"} 24 method_code:http_errors:rate5m{method=\"get\", code=\"404\"} 30 method_code:http_errors:rate5m{method=\"put\", code=\"501\"} 3 method_code:http_errors:rate5m{method=\"post\", code=\"500\"} 6 method_code:http_errors:rate5m{method=\"post\", code=\"404\"} 21 method:http_requests:rate5m{method=\"get\"} 600 method:http_requests:rate5m{method=\"del\"} 34 method:http_requests:rate5m{method=\"post\"} 120 重启node_exporter\n","categories":["prometheus","监控","exporter"],"description":"\nnode_exporter|exporter|prometheus\r\n\r\n","excerpt":"\nnode_exporter|exporter|prometheus\r\n\r\n","ref":"/prometheus/exporter/node_exporter.html","tags":["prometheus","exporter","node_exporter"],"title":"node_exporter"},{"body":"部署安装 验证通过版本7.5.17 、9.5.21、 10.4.14 docker方式安装grafanna rpm 包安装grafana docker pull grafana/grafana:7.5.17 docker pull grafana/grafana:9.5.21 docker pull grafana/grafana:10.4.14 docker pull grafana/grafana-enterprise:7.5.17 docker pull grafana/grafana-enterprise:9.5.21 docker pull grafana/grafana-enterprise:10.4.14 docker run -d -p 3000:3000 --name=grafana \\ --volume /data/grafana:/var/lib/grafana \\ grafana/grafana-enterprise:10.4.14 yum install -y https://dl.grafana.com/enterprise/release/grafana-enterprise-10.4.14-1.x86_64.rpm systemctl enable grafana-server.service --now 配置文件 可以通过修改配置文件/etc/grafana/grafana.ini或 环境变量的方式修改默认配置 Details # 运行在生产模式 app_mode = production # 实例名 instance_name = ${HOSTNAME} [paths] # 定义存放临时文件、session、sqllit data = /var/lib/grafana # 临时文件存放24h后删除 temp_data_lifetime = 24h # 日志文件 logs = /var/log/grafana # 插件文件 plugins = /var/lib/grafana/plugins # /usr/share/grafana/conf/provisioning 定义启动时加载配置，例如数据源、报表等 provisioning = conf/provisioning [server] # 支持（http, https, h2, socket) protocol = http # 绑定的服务ip,默认所有 http_addr = # 绑定的服务port http_port = 3000 # 服务绑定的域名 domain = localhost # 当访问时与指定的域名不一致时，拒绝访问。默认关闭 enforce_domain = false # 定义服务的url root_url = %(protocol)s://%(domain)s:%(http_port)s/grafana # 从root_url 的配置路径中加载 serve_from_sub_path = true # 开启gzip 用于节省带宽 enable_gzip = false [database] # Either \"mysql\", \"postgres\" or \"sqlite3\", it's your choice type = sqlite3 # 数据库名 name = grafana # 数据文件名称 path = grafana.db [security] # 第一次启动时不创建用户 disable_initial_admin_creation = false # 默认管理员用户 admin_user = admin # 默认密码 admin_password = admin # 默认邮件 admin_email = admin@localhost # 签名秘钥，例如： 对密码 admin 字符串执行加盐后保存和验证，防止通过数据库看到原始密码 secret_key = SW2YcwTIb9zpOOhoPsMm # 加密算法 encryption_provider = secretKey.v1 # 允许将grafana报表嵌入到 \u003cframe\u003e, \u003ciframe\u003e, \u003cembed\u003e or \u003cobject\u003e 等html中，默认不允许 allow_embedding = true [users] allow_sign_up = true # 允许普通用户创建organizations allow_org_create = true # 自动为新用户分配一个组织 auto_assign_org = true # 自动为新用户分配一个组织，该组织id为1，需要开启 auto_assign_org = true auto_assign_org_id = 1 # 自动为新用户分配一个角色 auto_assign_org_role = Viewer # 不要求有邮箱认证 verify_email_enabled = false # 默认主题为深色 default_theme = dark # 默认语言改为简体中文 default_language = zh-Hans # 设置首页 home_page = # 不允许 viewers 角色执行编辑动作 viewers_can_edit = false # 不允许 editors 角色执行管理动作 editors_can_admin = false [auth.anonymous] # 开启匿名访问,默认禁用 enabled = true # 匿名访问用户所属组织 org_name = Main Org. # 匿名用户拥有的角色 org_role = Viewer # 对于为匿名用户隐藏版本号 hide_version = false QA 🇶🇦如何让grafana默认中文显示\n修改配置文件/etc/grafana/grafana.ini 并重启grafana\n[users] # 默认语言改为简体中文 default_language = zh-Hans 🇶🇦如何通过nginx代理访问grafana\n如图所示，是由于grafana跨域请求问题。可以在nginx代理中正确配置\r修改配置文件/etc/grafana/grafana.ini 并重启grafana\n[server] # 支持（http, https, h2, socket) protocol = http # 绑定的服务ip,默认所有 http_addr = # 绑定的服务port http_port = 3000 # 服务绑定的域名 domain = localhost # 当访问时与指定的域名不一致时，拒绝访问。默认关闭 enforce_domain = false # 定义服务的url root_url = %(protocol)s://%(domain)s:%(http_port)s/grafana # 从root_url 的配置路径中加载 serve_from_sub_path = true # 开启gzip 用于节省带宽 enable_gzip = false 配置nginx代理\nserver { listen 80; root /usr/share/nginx/html; location /grafana { proxy_set_header Host $host; proxy_set_header X-Forwarded-For $remote_addr; proxy_pass http://172.16.0.3:3000; } 🇶🇦如何将dashboard嵌入到html\n修改配置文件/etc/grafana/grafana.ini 并重启grafana\n[security] # 允许将grafana报表嵌入到 \u003cframe\u003e, \u003ciframe\u003e, \u003cembed\u003e or \u003cobject\u003e 等html中，默认不允许 allow_embedding = true [auth.anonymous] # 开启匿名访问,默认禁用 enabled = true # 匿名访问用户所属组织 org_name = Main Org. # 匿名用户拥有的角色 org_role = Viewer # 对于为匿名用户隐藏版本号 hide_version = false html嵌入验证\n\u003chtml\u003e \u003c!-- 单个图表嵌入语句--\u003e \u003ciframe src=\"http://10.4.7.10:3000/d/a666a82f-47a1-4467-8e3f-c5d142f33927/new-dashboard?orgId=1\u0026viewPanel=1\" width=\"450\" height=\"200\" frameborder=\"0\"\u003e\u003c/iframe\u003e \u003c!-- 整个Dashboard嵌入语句--\u003e \u003ciframe src=\"http://10.4.7.10:3000/d/a666a82f-47a1-4467-8e3f-c5d142f33927/new-dashboard?orgId=1\" width=\"100%\" height=\"100%\" frameboader=\"0\"\u003e\u003c/iframe\u003e \u003c/html\u003e 🇶🇦如何在安装完毕grafana后配置好默认数据源\n修改配置文件/etc/grafana/grafana.ini 并重启grafana\n# 配置文件开启该配置 [paths] # 开启该配置 provisioning = conf/provisioning # vi /usr/share/grafana/conf/provisioning/datasources/prometheus.yml apiVersion: 1 datasources: # 第一个数据源 - name: prometheus01 type: prometheus access: proxy url: http://localhost:9090/prom editable: true isDefault: true # 第二个数据源 - name: prometheus02 type: prometheus access: proxy url: http://127.0.0.1:9090/prom editable: true 🇶🇦如何在安装完毕grafana后配置好默认dashboard\n修改配置文件/etc/grafana/grafana.ini 并重启grafana\n# 配置文件开启该配置 [paths] # 开启该配置 provisioning = conf/provisioning # /usr/share/grafana/conf/provisioning/dashboards/dashboards.yaml apiVersion: 1 providers: - name: 'node-exporter' orgId: 1 folder: 'k8s' folderUid: '' type: file options: path: /var/lib/grafana/dashboards mkdir /var/lib/grafana/dashboards # vi /var/lib/grafana/dashboards/node-exporter.json 文件格式为导出的报表格式 🇶🇦如何将指定dashboard作为首页\n修改配置文件/etc/grafana/grafana.ini 并重启grafana\n[users] # 设置首页 # http://prometheus.pytc.com:3000/grafana/d/rYdddlPWk/node-exporter-full?orgId=1\u0026refresh=1m home_page = /d/rYdddlPWk/node-exporter-full?orgId=1\u0026refresh=1m 🇶🇦dashbaord 定义动态变量\nlabel_values(promethues表达式,需要取值的label名称) 举例： label_values(up{job=\"node-exporter\"},instance) 🇶🇦dashbaord 标签转换\n问题描述： 在标签中存在 node01:9100 的字样。而现在只需要保留 node01\n解决办法：Transform(转换) –\u003e Rename by regex\n🇶🇦如何在grafana中安装插件\n# 安装插件 grafana-cli plugins install agenty-flowcharting-panel # 查看插件 grafana-cli plugins ls Grafana OSS 和企业 |Grafana 文档\nDownload https://grafana.com/docs/grafana/latest/cli/#plugins-commands\nRestricting Access with HTTP Basic Authentication | NGINX Documentation\n","categories":["grafana\""],"description":"\nquitstart|grafana|prometheus\r\n\r\n","excerpt":"\nquitstart|grafana|prometheus\r\n\r\n","ref":"/grafana/quitstart.html","tags":["grafana"],"title":"quitstart"},{"body":"blackbox是一个黑盒监控的代表，他通过各种模块来探测监控对象，并将结果返回给prometheus\n部署安装\nwget https://github.com/prometheus/blackbox_exporter/releases/download/v0.24.0/blackbox_exporter-0.24.0.linux-amd64.tar.gz tar xf blackbox_exporter-0.24.0.linux-amd64.tar.gz mkdir /data/blackbox_exporter/{bin,conf} -p mv blackbox_exporter-*linux-amd64/blackbox_exporter /data/blackbox_exporter/bin/ mv blackbox_exporter-*linux-amd64/blackbox.yml /data/blackbox_exporter/conf/ tee /usr/lib/systemd/system/blackbox_exporter.service \u003c\u003cEOF [Unit] Description=blackbox_exporter service https://prometheus.io/ After=network.target [Service] ExecStart=/data/blackbox_exporter/bin/blackbox_exporter \\ --web.listen-address=:9115 \\ --config.file=/data/blackbox_exporter/conf/blackbox.yml User=root [Install] WantedBy=multi-user.target EOF 启动blackbox\nsystemctl daemon-reload systemctl enable blackbox_exporter --now systemctl status blackbox_exporter 默认配置\n功能验证,probe_success 1表示探测成功\ncurl -s \"http://127.0.0.1:9115/probe?module=http_2xx\u0026target=www.baidu.com\"|grep probe_success # probe_success 1 添加prometheus配置\n- job_name: 'blackbox-http' metrics_path: /probe params: module: [http_2xx] # 指定模块 static_configs: - targets: - http://192.168.0.172:9999/magic/web/index.html # Target to probe with http. - https://sg-web-bjuat.pytech.cn/qc/dash # Target to probe with https. relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: 127.0.0.1:9115 - job_name: 'blackbox-icmp' metrics_path: /probe params: module: [icmp] # 指定模块 static_configs: - targets: - 192.168.0.172 relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: 127.0.0.1:9115 常用指标\n主要指标 解释 probe_dns_lookup_time_seconds dns解析耗时 probe_duration_seconds 探测耗时 probe_http_status_code 解析状态码 200 probe_success 探测是否成功1成功 ","categories":["prometheus","监控","exporter"],"description":"\nblackbox|exporter|prometheus\r\n\r\n","excerpt":"\nblackbox|exporter|prometheus\r\n\r\n","ref":"/prometheus/exporter/blackbox.html","tags":["prometheus","exporter","blackbox"],"title":"blackbox"},{"body":" 部署安装\n# 下载地址1： https://github.com/prometheus/jmx_exporter/tree/release-0.20.0 # 下载地址2： https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/ wget https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/1.0.1/jmx_prometheus_javaagent-1.0.1.jar 配置文件：\ntee javaagent.yaml \u003c\u003cEOF rules: - pattern: \".*\" EOF 以javaagent方式启动\njava -jar -Duser.timezone=Asia/Shanghai \\ -javaagent:jmx_prometheus_javaagent-1.0.1.jar=$(hostname -i):${M_PORT:-\"12345\"}:javaagent.yaml \\ \u003cyourjar\u003e 添加prometheus配置\n常用指标\n# 堆内存使用量 jvm_memory_used_bytes{area=\"heap\"} # 堆内存提交量 jvm_memory_committed_bytes{area=\"heap\"} # 堆内存最大可用量 jvm_memory_max_bytes{area=\"heap\"} # Young GC 触发次数 jvm_gc_collection_seconds_count{gc=\"G1 Young Generation\"} # Old GC 总耗时 jvm_gc_collection_seconds_sum{gc=\"G1 Old Generation\"} # 当前活动线程数 jvm_threads_current # 守护线程数 jvm_threads_daemon # 峰值线程数 jvm_threads_peak # 总加载的类数量 jvm_classes_loaded_total # 总卸载类数量 jvm_classes_unloaded_total # 当前加载类数量 jvm_classes_currently_loaded ","categories":["prometheus","监控","exporter"],"description":"\njmx_exporter|exporter|prometheus\r\n\r\n","excerpt":"\njmx_exporter|exporter|prometheus\r\n\r\n","ref":"/prometheus/exporter/jmx_exporter.html","tags":["prometheus","exporter","jmx_exporter"],"title":"jmx_exporter"},{"body":"nginx-exporter 支持对nginx 、nginx plugin(nginx的企业版)、nginx-ingress 暴露符合prometheus格式的指标。该文档只讨论开源版本的nginx的实施方案。\n前提条件nginx 需要开启stub_status 指令,样例格式如下：\nserver { listen 127.0.0.1:8080; location /stub_status { stub_status on; access_log off; allow 127.0.0.1; deny all; } } 部署安装\ndocker 镜像：nginx/nginx-prometheus-exporter:1.4.0\n二进制文件： https://github.com/nginx/nginx-prometheus-exporter/releases/download/v1.4.0/nginx-prometheus-exporter_1.4.0_linux_amd64.tar.gz\ndocker run -d \\ --net=host \\ nginx/nginx-prometheus-exporter:1.4.0 \\ --nginx.scrape-uri=http://127.0.0.1:8080/stub_status 添加prometheus配置\n- job_name: nginx static_configs: - targets: - 127.0.0.1:9113 常用指标\nName Type Description Labels nginx_up Gauge Shows the status of the last metric scrape: 1 for a successful scrape and 0 for a failed one [] Stub status metrics\nName Type Description Labels nginx_connections_accepted Counter 接受的客户端连接总数 [] nginx_connections_active Gauge 活跃的连接数，包括处于等待状态的连接 [] nginx_connections_handled Counter Handled client connections. [ nginx_connections_reading Gauge nginx 读取的客户端请求头 [] nginx_connections_waiting Gauge 空闲的连接数 [] nginx_connections_writing Gauge nginx 返回给客户端的响应. [] nginx_http_requests_total Counter 客户端请求总数 [] 其他第三方方案\nnginx-lua-prometheus\nnginx-module-vts\nyum install -y GeoIP-devel.x86_64 pcre-devel openssl-devel wget http://nginx.org/download/nginx-1.22.1.tar.gz tar xf nginx-1.22.1.tar.gz \u0026\u0026 cd nginx-1.22.1 git clone git://github.com/vozlt/nginx-module-vts.git ./configure \\ --prefix=/opt/nginx \\ --with-http_ssl_module \\ --with-http_realip_module \\ --with-http_geoip_module \\ --with-http_stub_status_module \\ --with-stream \\ --with-stream=dynamic \\ --with-stream_ssl_module \\ --with-stream_realip_module \\ --with-stream_geoip_module \\ --add-module=./nginx-module-vts make -j 4 \u0026\u0026 make install ./configure \\ --prefix=/opt/nginx \\ --with-http_ssl_module \\ --with-http_realip_module \\ --with-http_geoip_module \\ --with-http_stub_status_module \\ --with-stream \\ --with-stream \\ --with-stream_ssl_module \\ --with-stream_realip_module \\ --with-stream_geoip_module \\ --add-module=./nginx-module-vts make -j 4 \u0026\u0026 make install ./configure \\ --user=nginx \\ --prefix=/opt/nginx \\ --sbin-path=/usr/bin \\ --conf-path=/etc/nginx/nginx.conf \\ --with-http_ssl_module \\ --with-http_realip_module \\ --with-http_geoip_module \\ --with-http_stub_status_module \\ --with-stream \\ --with-stream_ssl_module \\ --with-stream_realip_module \\ --with-stream_geoip_module \\ --add-module=./nginx-module-vts 基本的指标样例\nhttp { vhost_traffic_status_zone; ... server { ... location /status { vhost_traffic_status_display; vhost_traffic_status_display_format html; access_log off; } } } 统计客户端ip国家信息\nhttp { geoip_country /usr/share/GeoIP/GeoIP.dat; vhost_traffic_status_zone; vhost_traffic_status_filter_by_set_key $geoip_country_code country::*; ... server { ... vhost_traffic_status_filter_by_set_key $geoip_country_code country::$server_name; location /status { vhost_traffic_status_display; vhost_traffic_status_display_format html; } } } - job_name: nginx-module-vts metrics_path: /status/format/prometheus static_configs: - targets: - 127.0.0.1 ","categories":["prometheus","监控","exporter"],"description":"nginx-exporter|exporter|prometheus\n","excerpt":"nginx-exporter|exporter|prometheus\n","ref":"/prometheus/exporter/nginx-exporter.html","tags":["prometheus","exporter","nginx-exporter"],"title":"nginx-exporter"},{"body":" 部署安装\nminio自身暴露了prometheus 兼容指标。我们只需要在启动前添加环境变量: MINIO_PROMETHEUS_AUTH_TYPE=\"public\"\nwget https://dl.min.io/server/minio/release/linux-amd64/minio chmod +x minio sudo mv minio /usr/local/bin/ tee /etc/systemd/system/minio.service \u003c\u003cEOF [Unit] Description=minio serveice test After=network.target [Service] EnvironmentFile=/data/minio/conf/minio.env ExecStart=/data/minio/bin/minio server \\ /data/minio/data \\ --address :9000 \\ --console-address :9001 User=root [Install] WantedBy=multi-user.target EOF tee /data/minio/conf/minio.env\u003c\u003cEOF MINIO_ROOT_USER=admin MINIO_ROOT_PASSWORD=admin12345 MINIO_PROMETHEUS_AUTH_TYPE=\"public\" EOF 添加prometheus配置\n- job_name: minio honor_timestamps: true metrics_path: /minio/v2/metrics/cluster follow_redirects: true static_configs: - targets: - 10.4.7.251:9000 常用指标\nminio 挂载磁盘的总大小，就是例子中 /data/minio/data\nminio_node_disk_total_bytes minio_node_disk_free_bytes minio_node_disk_used_bytes 查看特定bucket池子的使用量 (gauge)\nminio_bucket_usage_total_bytes{bucket=\"loki\"} minio_bucket_usage_object_total{bucket=\"loki\"} 对象存储大小分布 (gauge)\nminio_bucket_objects_size_distribution ","categories":["prometheus","监控","exporter"],"description":"\nminio|exporter|prometheus\r\n\r\n","excerpt":"\nminio|exporter|prometheus\r\n\r\n","ref":"/prometheus/exporter/minio.html","tags":["prometheus","exporter","minio"],"title":"minio"},{"body":" redis-exporter(1.74.0版本) 支持redis 2.x-7.x 部署安装\n容器镜像 quay.io/oliver006/redis_exporter\n安装exporter\nwget https://github.com/oliver006/redis_exporter/releases/download/v1.74.0/redis_exporter-v1.74.0.linux-amd64.tar.gz # tar xf redis_exporter-v1.74.0.linux-amd64.tar.gz # mv redis_exporter-v1.74.0.linux-amd64/redis_exporter /usr/bin/ rm -fr redis_exporter* 启动服务\n-is-cluster 监控集群启动参数添加该选项\ncat \u003e/usr/lib/systemd/system/redis_exporter_redis01_6379.service\u003c\u003c'EOF' [Unit] Description = redis-exporter Documentation = https://github.com/oliver006/redis_exporter After = network-online.target [Service] Type=Simple ExecStart=/usr/bin/redis_exporter \\ -redis.password YSxeEr3e4ZdF \\ -redis.addr 10.128.111.66:6379 \\ -export-client-list \\ -include-system-metrics \\ -connection-timeout 60s \\ -web.listen-address :9121 \\ -config-command config \\ -set-client-name redis_exporter RestartSec=2 Restart=on-failure User=root Group=root CPUQuota=20% MemoryLimit=128m [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable redis_exporter_redis01_6379 --now systemctl status redis_exporter_redis01_6379 添加prometheus配置\n指标接口： http://127.0.0.1:9121/metrics\n类似于blackbox,可以采集其他redis: http://127.0.0.1:9121/scrape?127.0.0.1:6379\ndashboar和告警\n进程相关\n指标名称 判断依据 redis_up 进程状态1表示正常 redis_uptime_in_seconds 启动时长 redis_instance_info 内存相关\n指标名称 redis_memory_used_bytes 内存使用量 redis_memory_used_peak_bytes 内存使用峰值 redis_memory_used_rss_bytes redis_memory_max_bytes maxmemory redis_config_maxmemory redis_memory_used_dataset_bytes redis_memory_used_lua_bytes redis_memory_used_scripts_bytes redis_memory_used_startup_bytes redis_memory_used_overhead_bytes redis_allocator_rss_bytes 分配出常驻内存 redis_active_defrag_running 内存碎片整理 表示没有活动的defrag任务正在运行，1表示有活动的defrag任务正在运行 redis_allocator_frag_ratio 内存碎片比 used_memory_rss和used_memory之间的比率，小于1表示使用了swap，大于1表示碎片比较多 网络流量相关\n指标名称 redis_net_input_bytes_total redis_net_output_bytes_total 持久化相关\n指标名称 redis_aof_enabled 是否开启aof持久化 redis_aof_last_cow_size_bytes redis_aof_last_rewite_duration_sec redis_aof_last_write_status redis_aof_last_bgrewrite_status redis_aof_rewite_in_progress redis_aof_rewite_shceduled redis_aof_current_rewrite_duration_sec aof rewrite耗时 指标名称 redis_rdb_bgsave_in_progress redis_rdb_changes_since_last_save redis_rdb_current_bgsave_duration_sec redis_rdb_last_bgsave_duration_sec redis_rdb_last_bgsave_status redis_rdb_last_cow_size_bytes redis_rdb_last_save_timestamp_seconds key相关\n指标名称 redis_db_keys 实例keys数量 redis_db_avg_ttl_seconds 平均过期时长 redis_db_keys_expiring redis_expired_keys_total redis_expired_stale_percentage redis_evicted_keys_total redis_keyspace_hits_total 缓存命中量 redis_keyspace_misses_total 未命中缓存量 sum(redis_keyspace_hits_total{cluster_id=\"123\"})/sum(redis_keyspace_hits_total{cluster_id=\"123\"})+sum(redis_keyspace_misses_total{cluster_id=\"123\"}) 客户端连接相关\n指标名称 redis_connected_clients redis_connected_slaves redis_config_maxclients redis_rejected_connections_total redis_connections_received_total redis_blocked_clients redis_client_recent_max_input_buffer_bytes redis_client_recent_max_output_buffer_bytes redis_config_client_output_buffer_limit_bytes redis_config_client_output_buffer_limit_overcome_seconds 慢日志\n指标名称 redis_slowlog_last_id redis_slowlog_length 性能\n指标名称 redis_last_slow_execution_duration_seconds 最慢的执行耗时 redis_commands_duration_seconds_total 命令出来耗时 redis_commands_processed_total 已处理命令的数量 redis_commands_total 每一个命令调用的次数 tps\nQPS\n主从复制相关\n指标名称 redis_connected_slaves redis_mem_clients_slaves redis_repl_backlog_first_byte_offset redis_repl_backlog_history_bytes redis_repl_backlog_is_active redis_replication_backlog_bytes redis_replica_partial_resync_accepted redis_replica_partial_resync_denied redis_replica_resyncs_full redis_master_repl_offset redis_second_repl_offset #!/bin/bash startMasterSlave() { docker run -d --net=host --name=redis6379 redis:6.0.20 redis-server --bind 10.4.7.10 --port 6379 --daemonize no docker run -d --net=host --name=redis6380 redis:6.0.20 redis-server --bind 10.4.7.10 --port 6380 --daemonize no --SLAVEOF 10.4.7.10 6379 docker run -d --net=host --name=redis6381 redis:6.0.20 redis-server --bind 10.4.7.10 --port 6381 --daemonize no --SLAVEOF 10.4.7.10 6379 } startSentinel() { for port in 26379 26380 26381;do docker run -d --net=host --name=sentinel${port} redis:6.0.20 sh -c \"echo 'sentinel monitor mymaster 10.4.7.10 6379 2\\n' \u003e/etc/redis-sentinel.conf;echo 'sentinel down-after-milliseconds mymaster 30000\\n' \u003e\u003e/etc/redis-sentinel.conf;echo 'sentinel parallel-syncs mymaster 1\\n' \u003e\u003e/etc/redis-sentinel.conf;echo 'sentinel failover-timeout mymaster 180000\\n' \u003e\u003e/etc/redis-sentinel.conf; redis-server /etc/redis-sentinel.conf --sentinel --bind 10.4.7.10 --port ${port} --daemonize no\" done } stop(){ docker rm -f sentinel26379 docker rm -f sentinel26380 docker rm -f sentinel26381 docker rm -f redis6379 docker rm -f redis6380 docker rm -f redis6381 } main(){ stop startMasterSlave startSentinel } main root@master01:~# docker exec -it sentinel26379 redis-cli -p 26379 -h 10.4.7.10 info sentinel # Sentinel sentinel_masters:1 sentinel_tilt:0 sentinel_running_scripts:0 sentinel_scripts_queue_length:0 sentinel_simulate_failure_flags:0 master0:name=mymaster,status=ok,address=10.4.7.10:6379,slaves=2,sentinels=3 redis_sentinel_master_ckquorum_status ","categories":["prometheus","监控","exporter"],"description":"\nredis-exporter用于导出兼容redis协议的数据库指标给prometheus，例如Valkey和redis\r\n\r\n","excerpt":"\nredis-exporter用于导出兼容redis协议的数据库指标给prometheus，例如Valkey和redis\r\n\r\n","ref":"/prometheus/exporter/redis-exporter.html","tags":["prometheus","exporter","redis-exporter"],"title":"redis-exporter"},{"body":" mysql-exporter(0.17.2版本) 支持mysql\u003e=5.6 、MariaDB \u003e= 10.3 部署安装\n容器镜像 prom/mysqld-exporter\n安装exporter\nwget https://github.com/prometheus/mysqld_exporter/releases/download/v0.17.2/mysqld_exporter-0.17.2.linux-amd64.tar.gz # tar xf mysqld_exporter-0.17.2.linux-amd64.tar.gz # mv mysqld_exporter-0.17.2.linux-amd64/mysqld_exporter /usr/bin/ rm -fr mysqld_exporter* 添加监控用户\nCREATE USER 'exporter'@'%' IDENTIFIED BY 'GlKUhymaO76zY' WITH MAX_USER_CONNECTIONS 3; GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO 'exporter'@'%'; 启动服务\ncat \u003e/usr/lib/systemd/system/mysql_exporter_mysql01_3306.service\u003c\u003c'EOF' [Unit] Description = mysql-exporter Documentation = https://github.com/prometheus/mysqld_exporter After = network-online.target [Service] Type=Simple Environment=\"MYSQLD_EXPORTER_PASSWORD=GlKUhymaO76zY\" ExecStart=/usr/bin/mysqld_exporter \\ --mysqld.address 10.128.99.158:6301 \\ --mysqld.username exporter RestartSec=2 Restart=on-failure User=root Group=root CPUQuota=20% MemoryLimit=128m [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable mysql_exporter_mysql01_3306 --now systemctl status mysql_exporter_mysql01_3306 添加prometheus配置\n指标接口： http://127.0.0.1:9104/metrics\n类似于blackbox,可以采集其他redis: http://127.0.0.1:9104/probe?127.0.0.1:3306\ndashboar和告警\n进程相关\n指标名称 判断依据 mysql_up 进程状态1表示正常 mysql_uptime_in_seconds 启动时长 mysql_instance_info 内存相关\n网络流量相关\n持久化相关\n客户端连接相关\n慢日志\n性能\ntps\nQPS\n主从复制相关\n","categories":["prometheus","监控","exporter"],"description":"\nmysql-exporter|exporter|prometheus\r\n\r\n","excerpt":"\nmysql-exporter|exporter|prometheus\r\n\r\n","ref":"/prometheus/exporter/mysql-exporter.html","tags":["prometheus","exporter","mysql-exporter"],"title":"mysql-exporter"},{"body":" mysql-exporter(0.40版本) 支持mongodb\u003e=4.4 部署安装\n容器镜像 percona/mongodb_exporter:0.40\n安装exporter\n添加监控用户\n启动服务\ncat \u003e/usr/lib/systemd/system/mysql_exporter_mysql01_3306.service\u003c\u003c'EOF' [Unit] Description = mysql-exporter Documentation = https://github.com/prometheus/mysqld_exporter After = network-online.target [Service] Type=Simple Environment=\"MYSQLD_EXPORTER_PASSWORD=GlKUhymaO76zY\" ExecStart=/usr/bin/mysqld_exporter \\ --mysqld.address 10.128.99.158:6301 \\ --mysqld.username exporter RestartSec=2 Restart=on-failure User=root Group=root CPUQuota=20% MemoryLimit=128m [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable mysql_exporter_mysql01_3306 --now systemctl status mysql_exporter_mysql01_3306 添加prometheus配置\n指标接口： http://127.0.0.1:9104/metrics\n类似于blackbox,可以采集其他redis: http://127.0.0.1:9104/probe?127.0.0.1:3306\ndashboar和告警\n进程相关\n指标名称 判断依据 mysql_up 进程状态1表示正常 mysql_uptime_in_seconds 启动时长 mysql_instance_info 内存相关\n网络流量相关\n持久化相关\n客户端连接相关\n慢日志\n性能\ntps\nQPS\n主从复制相关\n","categories":["prometheus","监控","exporter"],"description":"\nmongo-exporter|exporter|prometheus\r\n\r\n","excerpt":"\nmongo-exporter|exporter|prometheus\r\n\r\n","ref":"/prometheus/exporter/mongo-exporter.html","tags":["prometheus","exporter","mongo-exporter"],"title":"mongo-exporter"},{"body":"告警规划 要求：准确，及时，告警信息明确, 告警频率合理\n漏报误报 ==》\npromethues 告警规则和评估周期 针对通知信息不足，接收告警人员无法通过告警信息判断出当前出现的问题\n调整告警模版\n通过开发脚本程序拦截告警添加更多标签，告警中新增富文本例如走势图\n告警滋扰 ==》\n通过分组，让同类告警分批次发出 路由，按照需要接收告警的人收告警 按照业务 按照故障等级 抑制手段 alertmanger 负责管理告警的静默、抑制、聚合和发送告警，常见告警等级和处理时效。\n等级 阿里 腾讯 先知 海鸥 定义方式 认领（接手）时间 解决（关闭）时间 通知渠道 P1 严重 致命 事故 致命 需要立即联系管理团队的关键性问题 1min 10min 电话+短信+IM通知（钉钉等）配合升级策略确保问题在规定时间内处理 P2 警告 严重 故障 严重 严重影响许多客户使用产品的能力的关键性系统问题 10min 1h 短信+IM通知（钉钉等）配合升级策略确保问题在规定时间内处理 P3 优化 警告 告警 警告 需要运维人员立即关注的稳定性问题或影响客户的小问题 1h 24h 短信+IM通知（钉钉等） P4 通知 提示 信息 通知 需要采取行动的小问题，但不影响客户使用产品 24h 7Day IM通知（钉钉等） 架构图 curl\n【博客484】alertmanager—–告警处理源码剖析_alertmanager 重复告警_lulu的云原生笔记的博客-CSDN博客\nAlertmanager—配置详解(记得看完)_StarsL的技术博客_51CTO博客\n","categories":"","description":"","excerpt":"告警规划 要求：准确，及时，告警信息明确, 告警频率合理\n漏报误报 ==》\npromethues 告警规则和评估周期 针对通知信息不足，接收告警人员无法通过告警信息判断出当前出现的问题\n调整告警模版\n通过开发脚本程序拦截告警添加更多标签，告警中新增富文本例如走势图\n告警滋扰 ==》\n通过分组，让同类告警分批次发出 路由，按照需要接收告警的人收告警 按照业务 按照故障等级 抑制手段 …","ref":"/prometheus/alertmanager/","tags":"","title":"alertmanager"},{"body":"简介\nNginx (engine x http://nginx.org)是一个高性能的HTTP和反向代理服务器，也是一个IMAP/POP3/SMTP服务器。Nginx是由伊戈尔·赛索耶夫开发的，第一个公开版本0.1.0发布于2004年10月4日。其特点是占用内存少，并发能力强。\n应用场景\n应用场景 竞争产品 静态服务器（图片、视频服务器 lighttpd 动态服务 nignx +fastcgi 反向代理 负载均衡 haproxy cache(web缓存) vanish 安装nginx 配置详解 全局配置 http服务 location upsteam rewrite log 模块 ngx_http_autoindex_module 生成目录列表所用 https://nginx.org/en/docs/http/ngx_http_autoindex_module.html\nhttp、 server、 location\nlocation / { autoindex on; autoindex_exact_size on; # 精确输出文件大小 autoindex_format html; # 展示格式 html/xml/json/jsonp autoindex_localtime on; # 时间展示 } ngx_http_stub_status_module stub_status\nstub_status syntax: stub_status on default: None context: location Enables the status handler in this location. location /nginx_status { stub_status on; access_log off; allow SOME.IP.ADD.RESS; deny all; } active connections -- number of all open connections including connections to backends server accepts handled requests -- nginx accepted 16630948 connections, handled 16630948 connections (no one was closed just it was accepted), and handles 31070465 requests (1.8 requests per connection) reading -- nginx reads request header writing -- nginx reads request body, processes request, or writes response to a client waiting -- keep-alive connections, actually it is active - (reading + writing) gx_http_rewrite_module ","categories":"","description":"","excerpt":"简介\nNginx (engine x http://nginx.org)是一个高性能的HTTP和反向代理服务器，也是一个IMAP/POP3/SMTP服务器。Nginx是由伊戈尔·赛索耶夫开发的，第一个公开版本0.1.0发布于2004年10月4日。其特点是占用内存少，并发能力强。\n应用场景\n应用场景 竞争产品 静态服务器（图片、视频服务器 lighttpd 动态服务 nignx +fastcgi 反 …","ref":"/linux/nginx/","tags":"","title":"nginx"},{"body":"\n开源软件对比 功能 kuboard（v3.3.0） KubeSphere(v3.1.1) rancher(v2.5.10) dashboard(v2.0.1) k8s集群部署 / 支持 支持 不支持 多集群管理 支持 支持 支持 不支持 资源管理 可以监视、管理和部署应用程序 可以监视、管理和部署应用程序 可以监视、管理和部署应用程序 仅支持deploy,dasemonset,statefustset等内置资源管理 集群证书有效期展示 支持 / / / 认证和权限 支持RBAC和AD/LDAP集成 支持RBAC和AD/LDAP集成 支持RBAC和AD/LDAP集成 支持RBAC 开源协议 / Apache2.0 Apache2.0 Apache2.0 主要维护者 / 青云 SUSE 属于kubernetes项目 编程语言 javascript go go go 活跃度 18.2k 12.7k 21.1k 12.6k 部署难度 一般 较复杂 较复杂 一般 应用市场 (关联helm仓库快速部署服务) 支持 / 支持 / 介绍 Rancher Server 由认证代理（Authentication Proxy）、Rancher API Server、集群控制器（Cluster Controller）、etcd 节点和集群 Agent（Cluster Agent） 组成。除了集群 Agent 以外，其他组件都运行在 Rancher Server 中。\n主要功能：\n创建k8s集群（rancher成为RKE） 支持管理已有集群 可以集成elk，prometheus、alertmanager功能 安装 准备TLS版本离线镜像 docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:stable\nrancher/rancher-agent:v2.10.3 rancher/rancher:v2.10.3 rancher/rancher-webhook:v0.6.4 版本 容器镜像 v2.10 rancher/rancher:v2.10.3 rancher/rancher:stable v2.9 rancher/rancher:v2.9.3 v2.8 rancher/rancher:v2.8.5 离线镜像下载并推送到内网仓库\nRelease 文件 描述 rancher-images.txt 此文件包含安装 Rancher、创建集群和运行 Rancher 工具所需的镜像列表。 rancher-save-images.sh 这个脚本会从 DockerHub 中拉取在文件rancher-images.txt中描述的所有镜像，并将它们保存为文件rancher-images.tar.gz。 rancher-load-images.sh 这个脚本会载入文件rancher-images.tar.gz中的镜像，并将它们推送到你自己的私有镜像库。 # https://github.com/rancher/rancher/releases/download/v2.10.3/rancher-save-images.sh chmod +x rancher-save-images.sh rancher-load-images.sh ./rancher-save-images.sh --image-list ./rancher-images.txt ./rancher-load-images.sh --image-list ./rancher-images.txt --registry \u003cREGISTRY.YOURDOMAIN.COM:PORT\u003e 准备离线YAML 添加helm仓库\nhelm repo add rancher-stable https://releases.rancher.com/server-charts/stable helm repo update 获取对应版本\nrancher.zero-dew.com rancher的访问域名\nregistry.zero-dew.com 私有仓库地址\nprivateCA=true 指明使用自签CA\nhelm template rancher rancher-stable/rancher \\ --version 2.10.3 --output-dir ./ \\ --no-hooks \\ --namespace rancher \\ --set hostname=rancher.zero-dew.com \\ --set ingress.tls.source=secret \\ --set privateCA=true \\ --set rancherImage=registry.zero-dew.com/rancher \\ --set systemDefaultRegistry=registry.zero-dew.com \\ --set useBundledSystemChart=true \\ --kube-version 1.31.2 自签https证书\n生成 CA 证书\nopenssl genrsa -out ca.key 2048 openssl req -x509 -new -nodes -key ca.key -sha256 -days 3650 -out ca.crt -subj \"/C=CN/ST=GD/L=SZ/O=zero-dew.com/CN=zero-dew.com\" 创建 SAN 配置文件\ncat \u003esan.cnf \u003c\u003c'EOF' [req] default_bits = 2048 prompt = no default_md = sha256 req_extensions = req_ext distinguished_name = dn [dn] C = CN ST = Gd L = SZ O = zero-dew.com CN = rancher.zero-dew.com [req_ext] subjectAltName = @alt_names [alt_names] DNS.1 = rancher.zero-dew.com EOF 生成服务端私钥和 CSR\nopenssl genrsa -out rancher.key 2048 openssl req -new -key rancher.key -out rancher.csr -config san.cnf 用 CA 签发服务端证书（带 SAN）\nopenssl x509 -req -in rancher.csr -CA ca.crt -CAkey ca.key -CAcreateserial \\ -out rancher.crt -days 3650 -sha256 -extensions req_ext -extfile san.cnf 部署\nkubectl create ns rancher # ingress tls证书 kubectl -n rancher create secret tls tls-rancher-ingress --cert=./rancher.crt --key=./rancher.key # 该ca会被传递到agent端用于访问https://rancher.zero-dew.com kubectl -n rancher create secret generic tls-ca --from-file=cacerts.pem=./ca.crt kubectl -n rancher apply -f rancher/templates 使用 登录和修改密码\n选择导入已有集群\n卸载 kubectl -n rancher delete -f rancher/templates ","categories":["kubernetes"],"description":"dashboard|k8s\n","excerpt":"dashboard|k8s\n","ref":"/kubernetes/dashboard.html","tags":["dashboard"],"title":"dashboard"},{"body":"promethues 为用户提供了自定义开发exporter的sdk, 支持大多数常见语言。\nsdk: python golang #!/usr/bin/env python3 import http.server from prometheus_client import start_http_server from prometheus_client import Counter #从prometheus_client 库导入 Counter REQUESTS = Counter('hello_worlds_total','Hello Worlds requested.') # 定义`hello_worlds_total` 指标，帮助信息为'Hello Worlds requested.' class MyHandler(http.server.BaseHTTPRequestHandler): def do_GET(self): REQUESTS.inc() # 请求后自动+1 self.send_response(200) self.end_headers() self.wfile.write(b\"Hello World\") if __name__ == \"__main__\": start_http_server(8000) server = http.server.HTTPServer(('localhost', 8001), MyHandler) server.serve_forever() package main import ( \"fmt\" \"github.com/prometheus/client_golang/prometheus\" \"github.com/prometheus/client_golang/prometheus/promhttp\" \"net/http\" ) func main() { var l = map[string]string{ \"app\": \"ping\", \"env\": \"pro\", } var pingCounter = prometheus.NewCounter( prometheus.CounterOpts{ Namespace: \"pay\", Subsystem: \"wechart\", Name: \"ping_request_count\", Help: \"No of request handled by Ping handler\", ConstLabels: l, }, ) var pingGuage = prometheus.NewGauge( prometheus.GaugeOpts{ Namespace: \"pay\", Subsystem: \"wechart\", Name: \"ping_request_gauge\", Help: \"No of request handled by Ping handler\", ConstLabels: l, }, ) //http.HandleFunc http.HandleFunc(\"/ping\", func(w http.ResponseWriter, req *http.Request) { pingCounter.Inc() pingGuage.Set(2.2) fmt.Fprint(w, \"pong\") }) // 注册metrics prometheus.MustRegister(pingCounter) prometheus.MustRegister(pingGuage) http.Handle(\"/metrics\", promhttp.Handler()) //启动服务 http.ListenAndServe(\":9998\", nil) } ","categories":["prometheus","监控","exporter"],"description":"prometheus-sdk|exporter|prometheus\n","excerpt":"prometheus-sdk|exporter|prometheus\n","ref":"/prometheus/exporter/prometheus-sdk.html","tags":["prometheus","exporter","prometheus-sdk"],"title":"prometheus-sdk"},{"body":"","categories":"","description":"\npackage 文档中心\r\n","excerpt":"\npackage 文档中心\r\n","ref":"/docs/code/python/package/","tags":"","title":""},{"body":"","categories":"","description":"","excerpt":"","ref":"/linux/ansible/","tags":"","title":"ansible"},{"body":"","categories":["prometheus","监控","exporter"],"description":"\nexporter|prometheus\r\n\r\n","excerpt":"\nexporter|prometheus\r\n\r\n","ref":"/prometheus/monitor/","tags":["prometheus","exporter"],"title":"monitor"},{"body":"","categories":["prometheus","监控","victorametrics"],"description":"\nvictorametrics|prometheus\r\n","excerpt":"\nvictorametrics|prometheus\r\n","ref":"/prometheus/victorametrics/","tags":["prometheus","victorametrics"],"title":"victorametrics"},{"body":"","categories":"","description":"\npackage 文档中心\r\n","excerpt":"\npackage 文档中心\r\n","ref":"/docs/code/golang/package/","tags":"","title":""},{"body":" 部署安装\n参见部署prometheus\n添加prometheus配置\nglobal: external_labels: prometheus: prom-xxx scrape_configs: - job_name: prometheus scrape_interval: 5s static_configs: - targets: - \"localhost:9090\" 常用指标\n指标 释义 指标类型 prometheus_config_last_reload_successful 最后重启是否成功 1 表示成功 0表示失败 ceil(time()-prometheus_config_last_reload_success_timestamp_seconds) 最后成功重启的距离现在过去了多少秒 prometheus_notifications_alertmanagers_discovered 发现alertmanager并处于活跃状态 delta(prometheus_notifications_dropped_total[1h]) 由于发生错误而导致发送到alert 失败的告警数量 prometheus_notifications_sent_total 自最后一次启动发送了多少条告警通知 prometheus_notifications_queue_capacity prometheus 处理告警队列的配额 prometheus_notifications_queue_length 有多少条告警位于当前队列中 irate(process_cpu_seconds_total{ job=\"prometheus\"}[15m]) cpu 使用时长 process_open_fds{ job=\"prometheus\"} 已打开文件描述符数量 prometheus_engine_query_duration_seconds prometheus 引擎查询响应时长 summary sum(rate(prometheus_tsdb_head_samples_appended_total[15m])) 指标采集率 evaluation_intervalrule_group_iterations_missed_total 空间预估\n# 预估磁盘大小 # needed_disk_space = retention_time_seconds * ingested_samples_per_second * bytes_per_sample 86400（1天）\t* 10000/s * 2byte 告警\n服务不可用 [严重] 查询响应高[警告] 存在告警发出失败[警告] ","categories":["prometheus","exporter"],"description":"","excerpt":" 部署安装\n参见部署prometheus\n添加prometheus配置\nglobal: external_labels: prometheus: prom-xxx scrape_configs: - job_name: prometheus scrape_interval: 5s static_configs: - targets: - \"localhost:9090\" 常用指标\n指标 释义 指标 …","ref":"/prometheus/monitor/prometheus.html","tags":["prometheus","exporter"],"title":"prometheus"},{"body":" - job_name: alertmanager scrape_path: \"/alert/metrics\" static_configs: - targets: - 127.0.0.1:9093 ","categories":["prometheus","exporter"],"description":"","excerpt":" - job_name: alertmanager scrape_path: \"/alert/metrics\" static_configs: - targets: - 127.0.0.1:9093 ","ref":"/prometheus/monitor/alertmanager.html","tags":["prometheus","exporter","alertmanager"],"title":"alertmanager"},{"body":" pushgateway 用于将瞬时指标推送到prometheus，更倾向于解决服务级别的指标暴露，对于主机级别的瞬时指标可以使用textfile\n安装部署\nDetails docker run -d -p 9091:9091 prom/pushgateway\napiVersion: v1 kind: Service metadata: name: pushgateway spec: externalIPs: - 10.4.7.10 ports: - port: 9091 protocol: TCP targetPort: 9091 selector: app: pushgateway --- apiVersion: apps/v1 kind: Deployment metadata: name: pushgateway spec: selector: matchLabels: app: pushgateway template: metadata: labels: app: pushgateway spec: containers: - name: pushgateway image: prom/pushgateway:v1.5.0 ports: - name: http containerPort: 9091 args: - --web.enable-admin-api resources: limits: cpu: 500m memory: 1024Mi requests: cpu: 200m memory: 512Mi readinessProbe: httpGet: port: 9091 path: /-/ready initialDelaySeconds: 30 failureThreshold: 3 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 2 livenessProbe: httpGet: port: 9091 path: /-/healthy initialDelaySeconds: 30 failureThreshold: 3 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 2 指标推送\n推送指标 metrics_01{job=\"my_job\"} 1.0 # metrics_01 和metrics_02 为自定义指标名称 # job=\"my_job\" 为job名称 echo \"metrics_01 1.0\" | curl --data-binary @- http://127.0.0.1:9091/metrics/job/my_job echo \"metrics_02 2.0\" | curl --data-binary @- http://127.0.0.1:9091/metrics/job/my_job 推送指标 metrics_01{job=\"my_job\",instance=\"host01\",env=\"test\"} 1.0 # /metrics/job/\u003cJOB_NAME\u003e{/\u003cLABEL_NAME\u003e/\u003cLABEL_VALUE\u003e} echo \"metrics_01 1.0\" | curl --data-binary @- http://127.0.0.1:9091/metrics/job/my_job/instance/host01/env/test 删除指标 # 删除分组为job=\"my_job\"的指标 curl -X DELETE http://127.0.0.1:9091/metrics/job/my_job # 删除分组为job=\"my_job\" ,instance=\"host01\" 的指标 curl -X DELETE http://127.0.0.1:9091/metrics/job/my_job/instance/host01 # 删除所有分组指标，启动时需要开启--web.enable-admin-api curl -X PUT http://127.0.0.1:9091/api/v1/admin/wipe 对接到promethues\nhonor_labels: true,否则后面设置的job，instance等标签将会被舍弃 global: external_labels: prometheus: prom-xxx scrape_configs: - job_name: pushgateway scrape_interval: 5m honor_labels: true static_configs: - targets: - \"localhost:9091\" pushgatway开启认证\n使用httpd:2-aplines镜像生成bcrypt类型密码，使用格式如下：\ndocker run --rm -it httpd:2-alpine htpasswd -nBC 12 admin 生成认证配置文件\n# 加密密码123 cat \u003eauth.yaml\u003c\u003c'EOF' basic_auth_users: admin: \"$2y$12$jmxtk0MBfUe6AsW5cWKhU.vsYLkaVkTfWgu3JKWxFcdwGzNpfGYKW\" EOF 启动带认证的pushgatway\npushgateway --web.config.file=auth.yaml 功能验证\necho \"metrics_01 1.0\" | curl -u admin:123 --data-binary @- http://127.0.0.1:9091/metrics/job/my_job from prometheus_client import CollectorRegistry, Gauge, push_to_gateway from prometheus_client.exposition import basic_auth_handler def my_auth_handler(url, method, timeout, headers, data): username = 'admin' password = '123' return basic_auth_handler(url, method, timeout, headers, data, username, password) registry = CollectorRegistry() g = Gauge('job_last_success_unixtime', 'Last time a batch job successfully finished', registry=registry) g.set_to_current_time() push_to_gateway('localhost:9091', job='batchA', registry=registry, handler=my_auth_handler) 参考:\nclient_python|doc\nclient_golang\nPeter Bourgon · Go: Best Practices for Production Environments\nWhen to use the Pushgateway | Prometheus\n","categories":["prometheus","exporter"],"description":"","excerpt":" pushgateway 用于将瞬时指标推送到prometheus，更倾向于解决服务级别的指标暴露，对于主机级别的瞬时指标可以使用textfile\n安装部署\nDetails docker run -d -p 9091:9091 prom/pushgateway\napiVersion: v1 kind: Service metadata: name: pushgateway spec: …","ref":"/prometheus/monitor/pushgateway.html","tags":["prometheus","exporter","pushgateway"],"title":"pushgateway"},{"body":"","categories":["prometheus","监控","exporter"],"description":"\nexporter|prometheus\r\n\r\n","excerpt":"\nexporter|prometheus\r\n\r\n","ref":"/prometheus/kubernetes/","tags":["prometheus","exporter"],"title":"kubernetes"},{"body":" 部署安装\n添加prometheus配置\n- job_name: kube-apiserver honor_timestamps: true scrape_interval: 1m scrape_timeout: 1m metrics_path: /metrics scheme: https tls_config: ca_file: /etc/kubernetes/pki/ca.crt cert_file: /etc/kubernetes/pki/apiserver-kubelet-client.crt key_file: /etc/kubernetes/pki/apiserver-kubelet-client.key insecure_skip_verify: false follow_redirects: true enable_http2: true static_configs: - targets: - 192.168.0.244:6443 常用指标\ncurl \\ --cacert /etc/kubernetes/ssl/ca.crt \\ --cert /etc/kubernetes/ssl/apiserver-kubelet-client.crt \\ --key /etc/kubernetes/ssl/apiserver-kubelet-client.key \\ https://10.4.7.12:6443/metrics apiserver 进程是否正常\nup{job=\"kube-apiserver\"} 每秒QPS\nAPI调用延时\nETCD调用延时\n指标名 含义 类型 apiserver_request_total 请求总数 counter apiserver_audit_event_total 包含所有暴露的审计事件数量的指标。 counter apiserver_audit_error_total 在暴露时由于发生错误而被丢弃的事件的数量 counter apiserver_request_duration_seconds_sum 请求耗时 gauge apiserver_request_duration_seconds_count authentication_attempts apiserver_tls_handshake_errors_total apiserver_client_certificate_expiration_seconds_sum kube-apiserver组件监控介绍和常见异常指标_容器服务Kubernetes版(ACK)-阿里云帮助中心 (aliyun.com)\n","categories":["prometheus","监控","exporter"],"description":"\nexporter|prometheus\r\n\r\n","excerpt":"\nexporter|prometheus\r\n\r\n","ref":"/prometheus/kubernetes/kube-apiserver.html","tags":["prometheus","exporter","kubernetes"],"title":"kube-apiserver"},{"body":"","categories":["prometheus","监控","exporter"],"description":"\nexporter|prometheus\r\n\r\n","excerpt":"\nexporter|prometheus\r\n\r\n","ref":"/prometheus/kubernetes/kube-controller.html","tags":["prometheus","exporter","kubernetes"],"title":"kube-controller"},{"body":"","categories":["prometheus","监控","exporter"],"description":"\nexporter|prometheus\r\n\r\n","excerpt":"\nexporter|prometheus\r\n\r\n","ref":"/prometheus/kubernetes/kube-controller.html","tags":["prometheus","exporter","kubernetes"],"title":"kube-controller"},{"body":"kube-proxy 监听在 http://127.0.0.1:10249/metrics\nscheduler 监听在 https://127.0.0.1:10259/metrics\ncontroller-manager监听在 https://127.0.0.1:10257/metrics\ncoredns http://172.168.0.10:9153/metric\n收集k8s事件到mongodb GitHub - AliyunContainerService/kube-eventer: kube-eventer emit kubernetes events to sinks\n我来解释如何在 Kubernetes 中持久化事件（Events）。 Kubernetes 事件默认只保留1小时，如果要持久化存储事件，有以下几种主要方案： 使用 Event Exporter 这是一个比较常用的方案，可以将 k8s 事件导出到外部存储系统。例如使用官方的 event-exporter： 使用 Event Logger apiVersion: apps/v1 kind: Deployment metadata: name: event-exporter namespace: monitoring spec: replicas: 1 template: spec: containers: - name: event-exporter image: gcr.io/google-containers/event-exporter:v0.3.1 command: - /event-exporter - --sink-opts=elasticsearch?hosts=http://elasticsearch:9200 可以部署一个自定义的 event logger，监听集群事件并写入到持久化存储中： 修改 kube-apiserver 配置 apiVersion: apps/v1 kind: Deployment metadata: name: event-logger spec: template: spec: containers: - name: event-logger image: k8s-event-logger env: - name: STORAGE_TYPE value: \"elasticsearch\" - name: ES_HOST value: \"elasticsearch-service:9200\" 可以通过修改 kube-apiserver 的配置来延长事件保留时间： apiVersion: v1 kind: Pod metadata: name: kube-apiserver spec: containers: - command: - kube-apiserver - --event-ttl=168h # 保留7天 常见的事件持久化存储后端包括： Elasticsearch InfluxDB MongoDB MySQL/PostgreSQL 建议： 根据实际需求选择合适的存储后端 注意设置合理的数据保留策略，避免存储空间耗尽 考虑事件过滤，只存储重要事件 建立监控告警，及时发现异常事件 如果你需要具体某个存储方案的详细配置，我可以为你展开说明。 ","categories":["prometheus","监控","exporter"],"description":"exporter|prometheus\n","excerpt":"exporter|prometheus\n","ref":"/prometheus/kubernetes/kube-proxy.html","tags":["prometheus","exporter","kubernetes"],"title":"kube-proxy"},{"body":"kubelet 还会在 /metrics/cadvisor， /metrics/resource 和 /metrics/probes 端点中公开度量值\ncurl 127.0.0.1:10248/healthz\ncurl 127.0.0.1:10250/healthz\n部署安装\n参见部署k8s\n添加prometheus配置\nprometheus 部署在k8s 内，需要提前rbac授权 tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true bearer_tonken_file: /var/run/secrets/kubernetes.io/serviceaccount/token prometheus 部署在k8s 外\nkubernetes_sd_configs 支持通过 api_server 或 kubeconfig_file 方式，二选一\ncurl \\ --cacert /etc/kubernetes/pki/ca.crt \\ --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt \\ --key /etc/kubernetes/pki/apiserver-kubelet-client.key \\ https://192.168.0.243:10250/metrics - job_name: kubelet tls_config: ca_file: /etc/kubernetes/pki/ca.crt cert_file: /etc/kubernetes/pki/apiserver-kubelet-client.crt key_file: /etc/kubernetes/pki/apiserver-kubelet-client.key insecure_skip_verify: true scheme: https kubernetes_sd_configs: - kubeconfig_file: config role: node relabel_configs: - source_labels: [__address__] regex: (.*):(.*) replacement: \"$1:10250\" target_label: __address__ curl -k -H \"Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6Ik5jb3R0bGdBVmwtalBuVW9JR0h3VG5nblRkMmVJZUFMdDRVaDd6U1ZNMDQifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImFkbWluLXRva2VuLTduejlwIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYzEyNjYyNDktMWFkNS00ZjA1LTljZDUtYjhjNTdjYWU2NDkzIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmRlZmF1bHQ6YWRtaW4ifQ.TXZNfk12OsTh5Z_dkOJOHmIGxV_YNS6QEB-ItgddleoQrlz85S43OMED7mPfnxt9CTuDuuTjRRThXwiw1CF4cdtmwj789IU0z67LyDbkdHC4uUwMnqnQrw3aeSv_dgEHlCJr1btqIImRlxHzxvRsP3Yeqb95hjOqDIMVN_HgfYdk835foAHawrOksHdneMwtoOlpiqjVm9bzjIjEF5ckdVX86hwBu3xB1Ml4R4xZwGIecs06nQoBEKO6MlSARgq8e5VQqdRVip7WJuoAlyq80AYW1gAR7I6RvXJVWECu2NpXzQoTROZ132VyCLAhVQVqf_yLrpnQTTxdOKgvS1tskA\" https://192.168.0.243:10250/metrics - job_name: kubelet tls_config: ca_file: /etc/kubernetes/pki/ca.crt insecure_skip_verify: true authorization: type: Bearer #提前创建RBAC ,提取sa的secret 中token并保存到文件中 #kubectl create clusterrolebinding admin --clusterrole=cluster-admin --serviceaccount=default:admin credentials_file: token scheme: https kubernetes_sd_configs: - kubeconfig_file: config role: node relabel_configs: - source_labels: [__address__] regex: (.*):(.*) replacement: \"$1:10250\" target_label: __address__ 通过api_server 执行动态发现\n- job_name: kubelet tls_config: ca_file: /etc/kubernetes/pki/ca.crt insecure_skip_verify: true authorization: type: Bearer #提前创建RBAC ,提取sa的secret 中token并保存到文件中 #kubectl create clusterrolebinding admin --clusterrole=cluster-admin --serviceaccount=default:admin credentials_file: token scheme: https kubernetes_sd_configs: - api_server: https://127.0.0.1:6443 tls_config: ca_file: /etc/kubernetes/pki/ca.pem cert_file: /etc/kubernetes/pki/cert.pem key_file: /etc/kubernetes/pki/cert.key insecure_skip_verify: true role: node relabel_configs: - source_labels: [__address__] regex: (.*):(.*) replacement: \"$1:10250\" target_label: __address__ 常用指标\n各节点running容器数量\nkubelet_running_containers{container_state=\"running\"} 各个节点运行pod数量\nkubelet_running_pods 各个节点运行ds数量\n各个节点运行sts数量\n各个节点运行job数量\n各个节点运行cronjob数量\n各个节点运行pvc数量\n各个节点运行pv数量\n各个节点容器日志占用量\nkubelet 内存使用量\nkubelet cpu使用量\n节点是否ready\n告警\nkubelet 进程异常告警\n有pod 被驱逐时告警\npod 、cpu 、memory数量即将超过限额告警\nKubelet是Kubernetes集群中的一个核心组件，用于管理和监控运行在节点上的容器。Kubelet会通过暴露一些监控指标来提供关于其自身和节点的健康状况、资源使用情况和性能数据。以下是一些kubelet暴露的常见监控指标：\nkubelet_runtime_operations_errors_total：Kubelet在容器运行时处理期间遇到的错误数。\nkubelet_runtime_operations_latency_seconds：Kubelet处理容器运行时操作的延迟时间。\nkubelet_volume_stats_available_bytes：节点上可用的存储卷容量。\nkubelet_volume_stats_capacity_bytes：存储卷的总容量。\nkubelet_volume_stats_used_bytes：已使用的存储卷容量。\nkubelet_node_status_capacity_cpu_cores：节点上可用的CPU核心数量。\nkubelet_node_status_capacity_memory_bytes：节点上可用的内存容量。\nkubelet_node_status_allocatable_cpu_cores：节点上分配给Pod的可用CPU核心数量。\nkubelet_node_status_allocatable_memory_bytes：节点上分配给Pod的可用内存容量。\nkubelet_node_status_kubelet_version：Kubelet的版本信息。\nkubelet_node_status_machine_id：节点的机器ID。\nkubelet_node_status_system_uptime：节点的系统运行时间。\nkubelet_volume_stats_inodes：存储卷的Inode使用情况。\n这些指标可以通过Prometheus等监控系统进行收集和分析，以监控和调优Kubernetes集群中的节点和容器的运行情况。\n","categories":["prometheus","监控","exporter"],"description":"\nexporter|prometheus\r\n\r\n","excerpt":"\nexporter|prometheus\r\n\r\n","ref":"/prometheus/kubernetes/kubelet.html","tags":["prometheus","exporter","kubernetes"],"title":"kubelet"},{"body":" 部署安装\n参见部署k8s\n【参数说明】\nsnapshot-count 每50000次修改执行一次snapshot\nauto-compaction-retention 自动压缩，默认为 0 不开启\nmax-request-bytes 单个请求最大字节，默认1.5MB\nquota-backend-bytes 指定数据库在磁盘上的配额大小，默认2GB\nheartbeat-interval leader向leaner发送心跳周期，默认 100ms\nelection-timeout 选举超时,默认1s\nmax-snapshots 最大保留快照数，默认 5 个\nExecStart=/opt/kube/bin/etcd \\ --name=master3 \\ --cert-file=/etc/etcd/ssl/etcd.pem \\ --key-file=/etc/etcd/ssl/etcd-key.pem \\ --peer-cert-file=/etc/etcd/ssl/etcd.pem \\ --peer-key-file=/etc/etcd/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://10.4.7.12:2380 \\ --listen-peer-urls=https://10.4.7.12:2380 \\ --listen-client-urls=https://10.4.7.12:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://10.4.7.12:2379 \\ --initial-cluster-token=etcd-cluster-0 \\ --initial-cluster=master1=https://10.4.7.22:2380,master2=https://10.4.7.21:2380,master3=https://10.4.7.12:2380 \\ --initial-cluster-state=new \\ --data-dir=/var/lib/etcd/ \\ --snapshot-count=50000 \\ --auto-compaction-retention=1 \\ --max-request-bytes=10485760 \\ --enable-v2=true \\ --quota-backend-bytes=8589934592 Restart=always RestartSec=15 LimitNOFILE=65535 OOMScoreAdjust=-999 添加prometheus配置\n- job_name: 'etcd' tls_config: ca_file: /etc/kubernetes/pki/etcd/ca.crt cert_file: /etc/kubernetes/pki/etcd/peer.crt key_file: /etc/kubernetes/pki/etcd/peer.key scheme: https static_configs: - targets: - '192.168.0.244:2379' 监控指标\ncurl \\ --cacert /etc/kubernetes/pki/etcd/ca.crt \\ --cert /etc/kubernetes/pki/etcd/peer.crt \\ --key /etc/kubernetes/pki/etcd/peer.key \\ https://10.4.7.12:2379/metrics 指标主要包含 ：\nServer相关，所有指标以etcd_server 开头。 Disk相关，所有指标以`etcd_disk 开头 Network相关，所有指标以etcd_network 开头 mvcc相关 ，所有指标以etcd_mvcc 开头 snap相关，所有指标以etcd_snap 开头 degugging相关，所有指标以etcd_degugging 开头 Server相关，所有指标以etcd_server 开头\n名称 描述 类型 etcd_server_has_leader 是否存在leader。1表示存在，0表示不存在 Gauge etcd_server_leader_changes_seen_total 主从切换的总次数 Counter etcd_server_proposals_committed_total 协商一致提交的请求 Counter etcd_server_proposals_applied_total 协商一致处理的请求 Counter etcd_server_proposals_pending 排队等待提交的提案数量 Gauge etcd_server_proposals_failed_total 失败的提案 Counter etcd_server_quota_backend_bytes 数据库配额（磁盘） Gauge Disk相关，所有指标以`etcd_disk 开头\n名称 描述 类型 etcd_disk_wal_fsync_duration_seconds wal 写入磁盘延迟时长 Histogram etcd_disk_backend_commit_duration_seconds 增量快照写入磁盘延迟时长 Histogram 名称 描述 类型 etcd_debugging_mvcc_keys_total key数量 Gauge etcd_debugging_mvcc_db_total_size_in_bytes 数据库大小 Gauge Network相关，所有指标以etcd_network 开头\n名称 描述 类型 peer_sent_bytes_total The total number of bytes sent to the peer with ID To. Counter(To) peer_received_bytes_total The total number of bytes received from the peer with ID From. Counter(From) peer_sent_failures_total The total number of send failures from the peer with ID To. Counter(To) peer_received_failures_total The total number of receive failures from the peer with ID From. Counter(From) peer_round_trip_time_seconds Round-Trip-Time histogram between peers. Histogram(To) client_grpc_sent_bytes_total The total number of bytes sent to grpc clients. Counter client_grpc_received_bytes_total The total number of bytes received to grpc clients. Counter process_open_fds Number of open file descriptors. Gauge process_max_fds Maximum number of open file descriptors. Gauge etcd_server_proposals_applied_total 接收到的客户端请求 Counter etcd_server_etcd_network_peer_sent_bytes_total etcd 成员之间发送的请求 Counter etcd_server_etcd_network_peer_latency_seconds etcd 成员之间网络延迟 Gauge 告警规则\n- name: \"Etcd\" rules: # 主从切换 - alert: \"EtcdLeaderSwitch\" expr: etcd_server_is_leader != etcd_server_is_leader offset 10m for: \"5m\" labels: app: etcd severity: \"warnning\" annotations: summary: \"Etcd Role changes\" description: \"{{ $labels.instace }}'s Role changes before 10 minute,now is {{ $value }}.\" # 宕机 - alert: \"EtcdDown\" expr: up{job=~\".*etcd.*\"} == 0 for: \"5m\" labels: app: etcd severity: \"critical\" annotations: summary: \"Etcd is down \" description: \"{{ $labels.instace }} is down for more than 5 minute.\" # 缺少leader - alert: \"EtcdNoLeader\" expr: etcd_server_has_leader == 0 for: \"5m\" labels: app: etcd severity: \"critical\" annotations: summary: \"Etcd no leader\" description: \"{{ $labels.instace }} no leader for more than 5 minute.\" etcd 问题、调优、监控 - 知乎 (zhihu.com)\netcd 原理解析：读《etcd 技术内幕》 | Vermouth | 博客 | docker | k8s | python | go | 开发 (xuyasong.com)\n","categories":["prometheus","监控","exporter"],"description":"\nexporter|prometheus\r\n\r\n","excerpt":"\nexporter|prometheus\r\n\r\n","ref":"/prometheus/kubernetes/etcd.html","tags":["prometheus","exporter","kubernetes"],"title":"etcd"},{"body":"","categories":["prometheus","监控","exporter"],"description":"\nexporter|prometheus\r\n\r\n","excerpt":"\nexporter|prometheus\r\n\r\n","ref":"/prometheus/kubernetes/coredns.html","tags":["prometheus","exporter","kubernetes"],"title":"coredns"},{"body":" 部署安装 cadvisor提供容器资源cpu，内存、网络、文件系统等方面的信息，在k8s环境中通过daemonset 资源来部署相关资源\nVERSION=v0.36.0 # use the latest release version from https://github.com/google/cadvisor/releases sudo docker run \\ --volume=/:/rootfs:ro \\ --volume=/var/run:/var/run:ro \\ --volume=/sys:/sys:ro \\ --volume=/var/lib/docker/:/var/lib/docker:ro \\ --volume=/dev/disk/:/dev/disk:ro \\ --publish=8080:8080 \\ --detach=true \\ --name=cadvisor \\ --privileged \\ --device=/dev/kmsg \\ gcr.io/cadvisor/cadvisor:$VERSION 添加prometheus配置\n- job_name: cadvisor static_configs: - targets: - 10.4.7.251:8080 常用指标\n指标 注释 rate(container_cpu_usage_seconds_total{name=\"cadvisor\"}[1m]) cup负载 container_memory_usage_bytes{name=\"cadvisor\"} 内存使用率 rate(container_network_transmit_bytes_total{name=\"cadvisor\"}[1m]) 网络发送 rate(container_network_receive_bytes_total{name=\"cadvisor\"}[1m]) 网络接收 应用自身指标暴露\nannotations:\rprometheus.io/port: \"9153\"\rprometheus.io/scrape: \"true\" github 和招商银行内部使用了内嵌入的cadvisor 配置方式\n通过apiserver\n- job_name: kubernetes-cadvisor scrape_interval: 30s scrape_timeout: 10s metrics_path: /metrics scheme: https kubernetes_sd_configs: - api_server: null role: node namespaces: names: [] tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: false bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - separator: ; regex: __meta_kubernetes_node_label_(.+) replacement: $1 action: labelmap - separator: ; regex: (.*) target_label: __address__ replacement: kubernetes.default.svc:443 action: replace - source_labels: [__meta_kubernetes_node_name] separator: ; regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor action: replace cadvisor 内嵌在kubelet 中\nprometheus运行在k8s外部\nkubernetes_sd_configs:\n​\tkubeconfig_file 和 api_server 二选其一\nhttps 可以通过证书方式认证或通过token 方法是认证\n- job_name: cadvisor metrics_path: /metrics/cadvisor tls_config: ca_file: /etc/kubernetes/pki/ca.crt cert_file: /etc/kubernetes/pki/apiserver-kubelet-client.crt key_file: /etc/kubernetes/pki/apiserver-kubelet-client.key insecure_skip_verify: true scheme: https #authorization: # type: Bearer # credentials_file: token kubernetes_sd_configs: - kubeconfig_file: config role: node relabel_configs: - source_labels: [__address__] regex: (.*):(.*) replacement: \"$1:10250\" target_label: __address__ - job_name: cadvisor metrics_path: /metrics/cadvisor tls_config: ca_file: /etc/kubernetes/pki/ca.crt # cert_file: /etc/kubernetes/pki/apiserver-kubelet-client.crt # key_file: /etc/kubernetes/pki/apiserver-kubelet-client.key insecure_skip_verify: true scheme: https authorization: type: Bearer credentials_file: token kubernetes_sd_configs: - api_server: https://192.168.0.244:6443 tls_config: ca_file: /etc/kubernetes/pki/ca.crt cert_file: /etc/kubernetes/pki/apiserver-kubelet-client.crt key_file: /etc/kubernetes/pki/apiserver-kubelet-client.key role: node relabel_configs: - source_labels: [__address__] regex: (.*):(.*) replacement: \"$1:10250\" target_label: __address__ ","categories":["prometheus","监控","exporter"],"description":"\nexporter|prometheus\r\n\r\n","excerpt":"\nexporter|prometheus\r\n\r\n","ref":"/prometheus/kubernetes/cadvisor.html","tags":["prometheus","exporter","kubernetes"],"title":"cadvisor"},{"body":"kube-state-metrics 通过监听kube-apiserver生成资源状态指标。包括node、pod、container、deplpyment、statefulset、job、pv、pvc 等。\nkube-state-metrics 使用client-go与kubernetes集成，因此在安装时需要选择对应的kubernetes版本。\n✓完全支持的版本范围。 -Kubernetes 集群具有 client-go 库无法使用的功能（额外的 API 对象、已弃用的 API 等）。 kube-state-metrics 指标 Kubernetes 1.20 版本 Kubernetes 1.21 版本 Kubernetes 1.22 版本 Kubernetes 1.23 版本 Kubernetes 1.24 版本 2.3.0 版 ✓ ✓ ✓ ✓ - 2.4.2 版 -/✓ -/✓ ✓ ✓ - 2.5.0 版 -/✓ -/✓ ✓ ✓ ✓ 2.6.0 版 -/✓ -/✓ ✓ ✓ ✓ 本示例kubernetes 版本 v1.23，kube-state-metrics版本2.6.0\n资源清单位置：kube-state-metrics/examples/standard\n--- apiVersion: v1 automountServiceAccountToken: false kind: ServiceAccount metadata: labels: app.kubernetes.io/component: exporter app.kubernetes.io/name: kube-state-metrics app.kubernetes.io/version: 2.6.0 name: kube-state-metrics namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: app.kubernetes.io/component: exporter app.kubernetes.io/name: kube-state-metrics app.kubernetes.io/version: 2.6.0 name: kube-state-metrics rules: - apiGroups: - \"\" resources: - configmaps - secrets - nodes - pods - services - serviceaccounts - resourcequotas - replicationcontrollers - limitranges - persistentvolumeclaims - persistentvolumes - namespaces - endpoints verbs: - list - watch - apiGroups: - apps resources: - statefulsets - daemonsets - deployments - replicasets verbs: - list - watch - apiGroups: - batch resources: - cronjobs - jobs verbs: - list - watch - apiGroups: - autoscaling resources: - horizontalpodautoscalers verbs: - list - watch - apiGroups: - authentication.k8s.io resources: - tokenreviews verbs: - create - apiGroups: - authorization.k8s.io resources: - subjectaccessreviews verbs: - create - apiGroups: - policy resources: - poddisruptionbudgets verbs: - list - watch - apiGroups: - certificates.k8s.io resources: - certificatesigningrequests verbs: - list - watch - apiGroups: - storage.k8s.io resources: - storageclasses - volumeattachments verbs: - list - watch - apiGroups: - admissionregistration.k8s.io resources: - mutatingwebhookconfigurations - validatingwebhookconfigurations verbs: - list - watch - apiGroups: - networking.k8s.io resources: - networkpolicies - ingresses verbs: - list - watch - apiGroups: - coordination.k8s.io resources: - leases verbs: - list - watch - apiGroups: - rbac.authorization.k8s.io resources: - clusterrolebindings - clusterroles - rolebindings - roles verbs: - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: app.kubernetes.io/component: exporter app.kubernetes.io/name: kube-state-metrics app.kubernetes.io/version: 2.6.0 name: kube-state-metrics roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kube-state-metrics subjects: - kind: ServiceAccount name: kube-state-metrics namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/component: exporter app.kubernetes.io/name: kube-state-metrics app.kubernetes.io/version: 2.6.0 name: kube-state-metrics namespace: kube-system spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: kube-state-metrics template: metadata: labels: app.kubernetes.io/component: exporter app.kubernetes.io/name: kube-state-metrics app.kubernetes.io/version: 2.6.0 spec: automountServiceAccountToken: true containers: - image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.6.0 livenessProbe: httpGet: path: /healthz port: 8080 initialDelaySeconds: 5 timeoutSeconds: 5 name: kube-state-metrics ports: - containerPort: 8080 name: http-metrics - containerPort: 8081 name: telemetry readinessProbe: httpGet: path: / port: 8081 initialDelaySeconds: 5 timeoutSeconds: 5 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true runAsUser: 65534 nodeSelector: kubernetes.io/os: linux serviceAccountName: kube-state-metrics --- apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/component: exporter app.kubernetes.io/name: kube-state-metrics app.kubernetes.io/version: 2.6.0 prometheus.io/external: 192.168.0.243 # 为项目改造 prometheus.io/ports: \"32080\" # 为项目改造 name: kube-state-metrics namespace: kube-system spec: type: NodePort ports: - name: http-metrics port: 8080 nodePort: 32080 targetPort: http-metrics - name: telemetry port: 8081 targetPort: telemetry selector: app.kubernetes.io/name: kube-state-metrics 部署安装\n8080 端口提供k8s 指标\n8081 端口提供kube_state_metrics 自身指标\nkubectl apply -f kube-state-metrics.yml 添加prometheus配置\n静态配置\nscrape_configs: - job_name: \"kube-state-metrics\" static_configs: - targets: - \"192.168.0.244:8080\" # 添加所属环境 relabel_configs: - replacement: dev target_label: environment 基于k8s动态发现\ncurl --cacert /etc/kubernetes/pki/ca.crt \\ --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt \\ --key /etc/kubernetes/pki/apiserver-kubelet-client.key \\ https://192.168.0.244:6443/api/v1/services?limit=100\u0026resourceVersion=0 kubeconfig_file和 api_server 二者选一\n- job_name: kube-state-metrics kubernetes_sd_configs: - api_server: https://192.168.0.244:6443 tls_config: ca_file: /etc/kubernetes/pki/ca.crt cert_file: /etc/kubernetes/pki/apiserver-kubelet-client.crt key_file: /etc/kubernetes/pki/apiserver-kubelet-client.key # 测试改成false也没问题 insecure_skip_verify: true role: service relabel_configs: - source_labels: [\"__meta_kubernetes_service_name\"] action: keep regex: \"kube-state-metrics\" - source_labels: [\"__meta_kubernetes_service_label_prometheus_io_external\",\"__meta_kubernetes_service_label_prometheus_io_ports\"] regex: ([0-9\\.]+);(\\d+) replacement: $1:$2 action: replace target_label: __address__ - job_name: kube-state-metrics\rkubernetes_sd_configs:\r- kubeconfig_file: config\rrole: service\rrelabel_configs:\r- source_labels: [\"__meta_kubernetes_service_name\"]\raction: keep\rregex: \"kube-state-metrics\"\r- source_labels: [\"__meta_kubernetes_service_label_prometheus_io_external\",\"__meta_kubernetes_service_label_prometheus_io_ports\"]\rregex: ([0-9\\.]+);(\\d+)\rreplacement: $1:$2\raction: replace\rtarget_label: __address__ 常用指标 集群状态指标是哪个 svc 状态（缺少endpoint）\nnode\n#节点 数量 sum(kube_node_info) #不可调度的节点数量 sum(kube_node_spec_unschedulable) # 集群cpu 数量 sum(kube_node_status_capacity{resource=\"cpu\"}) # 集群内存 数量 sum(kube_node_status_capacity{resource=\"memory\"}) # 磁盘存在压力的节点 kube_node_status_condition{condition=\"DiskPressure\",status=\"true\"} # 内存存在压力的节点 kube_node_status_condition{condition=\"MemoryPressure\",status=\"true\"} # pid存在压力的节点 kube_node_status_condition{condition=\"PIDPressure\",status=\"true\"} # 存在网络不可达节点 kube_node_status_condition{condition=\"NetworkUnavailable\",status=\"true\"} namespace\npod\nkube_pod_status_phase{phase=\"Failed\"} kube_pod_status_phase{phase=\"Pending\"} kube_pod_status_phase{phase=\"Running\"} kube_pod_status_phase{phase=\"Succeeded\"} kube_pod_status_phase{phase=\"Unknown\"} container\n# 容器状态 kube_pod_container_status_running kube_pod_container_status_waiting kube_pod_container_status_ready kube_pod_container_status_terminated kube_pod_container_status_terminated_reason # 30分钟内重启过的pod changes(kube_pod_container_status_restarts_total[30m]) # 容器资源配额 kube_pod_container_resource_requests{resource=\"cpu\"} kube_pod_container_resource_limits{resource=\"cpu\"} kube_pod_container_resource_requests{resource=\"memory\"} kube_pod_container_resource_limits{resource=\"memory\"} replicaset\ndeploy\n#各个deployment的副本数量 kube_deployment_status_replicas #各个deployment不可用的副本数量 kube_deployment_status_replicas_unavailable daemonset\nsts\njob\ncronjob\nservice\nendpoint\ningress\nstorageclass\npersistentvolume\nkube_persistentvolume_status_phase{phase=\"Available\"} kube_persistentvolume_status_phase{phase=\"Bound\"} kube_persistentvolume_status_phase{phase=\"Failed\"} kube_persistentvolume_status_phase{phase=\"Pending\"} kube_persistentvolume_status_phase{phase=\"Released\"} persistentvolumeclaim\nkube_persistentvolumeclaim_status_phase{phase=\"Bound\"} kube_persistentvolumeclaim_status_phase{phase=\"Lost\"} kube_persistentvolumeclaim_status_phase{phase=\"Pending\"} configmap\nsecret\npoddisruptionbudget\nhorizontalpodautoscaler\nnetworkpolicy\nlease\nlimitrange\nmutatingwebhookconfiguration\nvalidatingwebhookconfiguration\nvolumeattachment\ngroups: - name: kube-state-metrics rules: - alert: KubeStateMetricsListErrors annotations: description: kube-state-metrics is experiencing errors at an elevated rate in list operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all. summary: kube-state-metrics is experiencing errors in list operations. expr: | (sum(rate(kube_state_metrics_list_total{job=\"kube-state-metrics\",result=\"error\"}[5m])) / sum(rate(kube_state_metrics_list_total{job=\"kube-state-metrics\"}[5m]))) \u003e 0.01 for: 15m labels: severity: critical - alert: KubeStateMetricsWatchErrors annotations: description: kube-state-metrics is experiencing errors at an elevated rate in watch operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all. summary: kube-state-metrics is experiencing errors in watch operations. expr: | (sum(rate(kube_state_metrics_watch_total{job=\"kube-state-metrics\",result=\"error\"}[5m])) / sum(rate(kube_state_metrics_watch_total{job=\"kube-state-metrics\"}[5m]))) \u003e 0.01 for: 15m labels: severity: critical - alert: KubeStateMetricsShardingMismatch annotations: description: kube-state-metrics pods are running with different --total-shards configuration, some Kubernetes objects may be exposed multiple times or not exposed at all. summary: kube-state-metrics sharding is misconfigured. expr: | stdvar (kube_state_metrics_total_shards{job=\"kube-state-metrics\"}) != 0 for: 15m labels: severity: critical - alert: KubeStateMetricsShardsMissing annotations: description: kube-state-metrics shards are missing, some Kubernetes objects are not being exposed. summary: kube-state-metrics shards are missing. expr: | 2^max(kube_state_metrics_total_shards{job=\"kube-state-metrics\"}) - 1 - sum( 2 ^ max by (shard_ordinal) (kube_state_metrics_shard_ordinal{job=\"kube-state-metrics\"}) ) != 0 for: 15m labels: severity: critical ","categories":["prometheus","监控","exporter"],"description":"\nexporter|prometheus\r\n\r\n","excerpt":"\nexporter|prometheus\r\n\r\n","ref":"/prometheus/kubernetes/kube-state-metrics.html","tags":["prometheus","exporter","kubernetes"],"title":"kube-state-metrics"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/golang/","tags":"","title":"Golang"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/golang/","tags":"","title":"Golang"},{"body":" 了解文档 查看博客 技术分享，涵盖 kubernetes、prometheus、golang\rbuild once run anywhere kubernetes 云原生时代最有力的容器编排工具\nRead more\nmonitor anyone anywhere! prometheus 云原生时代最灵活的监控工具\nRead more\ncloud native language! golang 云原生时代最流行的开发语言\nRead more\n","categories":"","description":"","excerpt":" 了解文档 查看博客 技术分享，涵盖 kubernetes、prometheus、golang\rbuild once run anywhere kubernetes 云原生时代最有力的容器编排工具\nRead more\nmonitor anyone anywhere! prometheus 云原生时代最灵活的监控工具\nRead more\ncloud native language! golang 云 …","ref":"/","tags":"","title":"HOME"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/","tags":"","title":"Tags"},{"body":" 山不却垒土之功，故能成其高；\n海不避涓涓细流，故能成其大。\n","categories":"","description":"","excerpt":" 山不却垒土之功，故能成其高；\n海不避涓涓细流，故能成其大。\n","ref":"/docs/","tags":"","title":"文档中心"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E8%BF%90%E7%AE%97%E7%AC%A6/","tags":"","title":"运算符"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/2026/","tags":"","title":"2026"},{"body":" 羽翼未丰而遭众忌，乃招祸之端；\n谋未行而先令人知，乃凶兆之端。\n网站汇集了主流开源软件cve漏洞 磁盘压测工具 网络压测工具 redis压测工具\nfreessl\n","categories":"","description":"","excerpt":" 羽翼未丰而遭众忌，乃招祸之端；\n谋未行而先令人知，乃凶兆之端。\n网站汇集了主流开源软件cve漏洞 磁盘压测工具 网络压测工具 redis压测工具\nfreessl\n","ref":"/blog/","tags":"","title":"博客"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E8%AE%A1%E5%88%92/","tags":"","title":"计划"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/easyimg/","tags":"","title":"Easyimg"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E5%9B%BE%E5%BA%8A%E5%B7%A5%E5%85%B7/","tags":"","title":"图床工具"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/uuid/","tags":"","title":"Uuid"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/8.x/","tags":"","title":"8.x"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/redis/","tags":"","title":"Redis"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kubernetes/","tags":"","title":"Kubernetes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/operator/","tags":"","title":"Operator"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/python/","tags":"","title":"Python"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/redis/","tags":"","title":"Redis"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/autodiscover/","tags":"","title":"Autodiscover"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/elk/","tags":"","title":"ELK"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/filebeat/","tags":"","title":"Filebeat"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/kafka/","tags":"","title":"Kafka"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kafka/","tags":"","title":"Kafka"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/zookeeper/","tags":"","title":"Zookeeper"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E8%AE%A4%E8%AF%81%E6%8E%88%E6%9D%83/","tags":"","title":"认证授权"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/kubernetes/","tags":"","title":"Kubernetes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/network/","tags":"","title":"Network"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/qemu/","tags":"","title":"Qemu"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/sriov/","tags":"","title":"Sriov"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/python/","tags":"","title":"Python"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/linux/","tags":"","title":"Linux"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/logstash/","tags":"","title":"Logstash"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/fastapi/","tags":"","title":"Fastapi"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/fastapi/","tags":"","title":"Fastapi"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/vue/","tags":"","title":"Vue"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/vue/","tags":"","title":"Vue"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/%E5%81%87%E6%9C%9F/","tags":"","title":"假期"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E6%97%85%E6%B8%B8/","tags":"","title":"旅游"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E7%BE%8E%E9%A3%9F/","tags":"","title":"美食"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/elasticsearch/","tags":"","title":"Elasticsearch"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/linux/","tags":"","title":"Linux"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/log/","tags":"","title":"Log"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/exporter/","tags":"","title":"Exporter"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kafka-manager/","tags":"","title":"Kafka-Manager\""},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/monitor/","tags":"","title":"Monitor"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/nginx/","tags":"","title":"Nginx"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/prometheus/","tags":"","title":"Prometheus"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kibana/","tags":"","title":"Kibana"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/%E8%8B%B1%E8%AF%AD/","tags":"","title":"英语"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E8%8B%B1%E8%AF%AD%E5%8D%95%E8%AF%8D/","tags":"","title":"英语单词"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/jupyter-notebook/","tags":"","title":"Jupyter-Notebook"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/pcta/","tags":"","title":"Pcta"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/tidb/","tags":"","title":"Tidb"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/appolo/","tags":"","title":"Appolo"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/dubbo/","tags":"","title":"Dubbo"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/nacos/","tags":"","title":"Nacos"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/serviceless/","tags":"","title":"Serviceless"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/mysql/","tags":"","title":"Mysql"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/","tags":"","title":"安装部署"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E8%BF%81%E7%A7%BB%E6%A1%88%E4%BE%8B/","tags":"","title":"迁移案例"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/ha/","tags":"","title":"Ha"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/mha/","tags":"","title":"Mha"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/mysqldump/","tags":"","title":"Mysqldump"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/mysql%E5%AE%89%E8%A3%85/","tags":"","title":"Mysql安装"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/mysql%E9%85%8D%E7%BD%AE/","tags":"","title":"Mysql配置"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/rabbitmq/","tags":"","title":"Rabbitmq"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E5%A4%87%E4%BB%BD/","tags":"","title":"备份"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E5%AE%89%E8%A3%85mysql/","tags":"","title":"安装mysql"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E5%AE%89%E8%A3%85rabbitmq/","tags":"","title":"安装rabbitmq"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E5%81%9A%E9%A5%AD/","tags":"","title":"做饭"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/%E5%A8%B1%E4%B9%90/","tags":"","title":"娱乐"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E7%94%B5%E5%BD%B1/","tags":"","title":"电影"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/xtrabackup/","tags":"","title":"Xtrabackup"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kubeadm/","tags":"","title":"Kubeadm"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/exis/","tags":"","title":"Exis"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/vim-cmd/","tags":"","title":"Vim-Cmd"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E4%B8%BB%E4%BB%8E%E9%85%8D%E7%BD%AE/","tags":"","title":"主从配置"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/akshare/","tags":"","title":"Akshare"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/%E8%82%A1%E7%A5%A8/","tags":"","title":"股票"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/filebrowser-/","tags":"","title":"Filebrowser "},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/music_tag_web/","tags":"","title":"Music_tag_web"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/navidrome/","tags":"","title":"Navidrome"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/alertmanager/","tags":"","title":"Alertmanager"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/%E5%91%8A%E8%AD%A6/","tags":"","title":"告警"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/tiup%E5%B7%A5%E5%85%B7/","tags":"","title":"Tiup工具"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/mail/","tags":"","title":"Mail"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/input/","tags":"","title":"Input"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/output/","tags":"","title":"Output"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/processors/","tags":"","title":"Processors"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E6%89%A9%E5%B1%95%E9%85%8D%E7%BD%AE/","tags":"","title":"扩展配置"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E9%80%9A%E7%94%A8%E9%85%8D%E7%BD%AE/","tags":"","title":"通用配置"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/coredns/","tags":"","title":"Coredns"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/coredns/","tags":"","title":"Coredns"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/demo/","tags":"","title":"Demo"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/plugin/","tags":"","title":"Plugin"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/echart/","tags":"","title":"Echart"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/echart/","tags":"","title":"Echart"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/location%E6%8C%87%E4%BB%A4/","tags":"","title":"Location指令"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/nginx/","tags":"","title":"Nginx"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/nginx.conf/","tags":"","title":"Nginx.conf"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/http%E6%9C%8D%E5%8A%A1%E5%99%A8/","tags":"","title":"Http服务器"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E6%96%B9%E5%90%91%E4%BB%A3%E7%90%86/","tags":"","title":"方向代理"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/command/","tags":"","title":"Command"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/","tags":"","title":"问题排查"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/ansible/","tags":"","title":"Ansible"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/ansible/","tags":"","title":"Ansible"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/dashboard/","tags":"","title":"Dashboard"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/ingress/","tags":"","title":"Ingress"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kubeaz/","tags":"","title":"Kubeaz"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/hpa/","tags":"","title":"Hpa"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/cicd/","tags":"","title":"Cicd"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/devops/","tags":"","title":"Devops"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/etcd/","tags":"","title":"Etcd"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/etcd/","tags":"","title":"Etcd"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/gitlab/","tags":"","title":"Gitlab"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/redmine/","tags":"","title":"Redmine"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/sonarqube/","tags":"","title":"Sonarqube"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/jenkins/","tags":"","title":"Jenkins"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/groovy/","tags":"","title":"Groovy"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/install/","tags":"","title":"Install"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/lvm/","tags":"","title":"Lvm"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/lvm/","tags":"","title":"Lvm"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/prerun/","tags":"","title":"Prerun"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/tidb/","tags":"","title":"Tidb"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/rbac/","tags":"","title":"RBAC"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/alertmanager/","tags":"","title":"Alertmanager"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/amtool/","tags":"","title":"Amtool"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/exporter/","tags":"","title":"Exporter"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/grafana/","tags":"","title":"Grafana"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/grafana/","tags":"","title":"Grafana\""},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/prometheus/","tags":"","title":"Prometheus"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/%E7%9B%91%E6%8E%A7/","tags":"","title":"监控"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/%E5%A4%9C%E8%8E%BA%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/","tags":"","title":"夜莺·监控系统"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E5%A4%9C%E8%8E%BA%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/","tags":"","title":"夜莺·监控系统"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/blackbox/","tags":"","title":"Blackbox"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/consul_sd_configs/","tags":"","title":"Consul_sd_configs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/federation/","tags":"","title":"Federation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/jmx_exporter/","tags":"","title":"Jmx_exporter"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/kubernetes_sd_configs/","tags":"","title":"Kubernetes_sd_configs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/minio/","tags":"","title":"Minio"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/mongo-exporter/","tags":"","title":"Mongo-Exporter"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/mysql-exporter/","tags":"","title":"Mysql-Exporter"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/nginx-exporter/","tags":"","title":"Nginx-Exporter"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/node_exporter/","tags":"","title":"Node_exporter"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/promeql/","tags":"","title":"PromeQL"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/prometheus-sdk/","tags":"","title":"Prometheus-Sdk"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/pushgateway/","tags":"","title":"Pushgateway"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/redis-exporter/","tags":"","title":"Redis-Exporter"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/victorametrics/","tags":"","title":"Victorametrics"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/victorametrics/","tags":"","title":"Victorametrics"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/victoriametrics/","tags":"","title":"VictoriaMetrics"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/victoriametrics/","tags":"","title":"VictoriaMetrics"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E5%AE%89%E8%A3%85/","tags":"","title":"安装"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E9%85%8D%E7%BD%AE/","tags":"","title":"配置"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E9%99%84%E5%BD%95/","tags":"","title":"附录"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/cobra/","tags":"","title":"Cobra"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/flag/","tags":"","title":"Flag"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/fmt/","tags":"","title":"Fmt"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/hugo/","tags":"","title":"Hugo"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/hugo/","tags":"","title":"Hugo"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/io/","tags":"","title":"Io"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/net/http/","tags":"","title":"Net/Http"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/os/","tags":"","title":"Os"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/package/","tags":"","title":"Package"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/path/","tags":"","title":"Path"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/pflag/","tags":"","title":"Pflag"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/raft/","tags":"","title":"Raft"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/shortcode/","tags":"","title":"Shortcode"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/time/","tags":"","title":"Time"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/viper/","tags":"","title":"Viper"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E5%87%BD%E6%95%B0/","tags":"","title":"函数"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E5%8F%98%E9%87%8F/","tags":"","title":"变量"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E5%B8%B8%E9%87%8F/","tags":"","title":"常量"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E5%BB%BA%E7%AB%99/","tags":"","title":"建站"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E6%8E%A5%E5%8F%A3/","tags":"","title":"接口"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","tags":"","title":"数据类型"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E6%97%B6%E9%92%9F%E5%92%8C%E5%A4%A9%E6%B0%94/","tags":"","title":"时钟和天气"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6%E8%AF%AD%E5%8F%A5/","tags":"","title":"流程控制语句"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE/","tags":"","title":"环境设置"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/%E7%BB%93%E6%9E%84%E4%BD%93/","tags":"","title":"结构体"},{"body":" 梅西说，他想像孩子一样开心地踢球。毕加索也说，他穷尽一生追求像孩子一样画画。其实，我们也应该去追求孩子般的生活，看到燕子归来便兴奋不已，看到蚂蚁搬家便期待一场酣雨。 技术栈 本站使用以下技术构建：\n静态站点生成器: Hugo - 一个快速、现代的静态网站生成器 主题: 自定义主题，基于 Hugo 的默认主题进行开发 部署: 使用 GitHub Pages 进行静态托管 版本控制: Git 用于代码版本管理 图床工具: easyimg 或 RustFS 内容架构 本站主要展示以下内容：\n技术文档 Hugo: 静态站点生成器的使用指南和最佳实践 Kubernetes: 容器编排平台的学习笔记和实战经验 Prometheus: 监控系统的配置和使用方法 Jenkins: CI/CD 流水线的搭建和维护 Golang: Go 语言编程技巧和项目实践 特色功能 胶片式轮播展示 采用 CSS3 动画实现的无缝滚动效果 响应式设计，适配各种屏幕尺寸 鼠标悬停暂停，提供更好的交互体验 文档导航 清晰的文档结构 便捷的搜索功能 友好的阅读体验 开发理念 本站秉承以下开发理念：\n简洁至上: 保持界面简洁，突出内容 性能优先: 采用静态生成，确保访问速度 持续更新: 定期更新内容，保持技术文档的时效性 开放共享: 所有内容开源，欢迎贡献 贡献指南 欢迎通过以下方式参与贡献：\n在 GitHub 上提交 Issue 或 Pull Request 完善现有文档内容 分享使用经验和技术心得 联系方式 如有问题或建议，欢迎通过以下方式联系：\nGitHub Issues 邮件联系: 1209233066@qq.com 微信号: mingtian12-22 ","categories":"","description":"","excerpt":" 梅西说，他想像孩子一样开心地踢球。毕加索也说，他穷尽一生追求像孩子一样画画。其实，我们也应该去追求孩子般的生活，看到燕子归来便兴奋不已，看到蚂蚁搬家便期待一场酣雨。 技术栈 本站使用以下技术构建：\n静态站点生成器: Hugo - 一个快速、现代的静态网站生成器 主题: 自定义主题，基于 Hugo 的默认主题进行开发 部署: 使用 GitHub Pages 进行静态托管 版本控制: Git 用于代 …","ref":"/about/","tags":"","title":""},{"body":"共享库目录结构\njenkinslib\r├─resources\r│ └─config\r├─src\r│ └─org\r│ └─jenkins\r└─vars 在Jenkinsfile 中加载共享库 /* @Library 为固定语法声明加载共享库 jenkinslib 为jenkins 全局配置中配置的共享库的名称 _ 加载共享库 */ @Library(‘jenkinslib’) _\n","categories":"","description":"","excerpt":"共享库目录结构\njenkinslib\r├─resources\r│ └─config\r├─src\r│ └─org\r│ └─jenkins\r└─vars 在Jenkinsfile 中加载共享库 /* @Library 为固定语法声明加载共享库 jenkinslib 为jenkins 全局配置中配置的共享库的名称 _ 加载共享库 */ @Library(‘jenkinslib’) _\n","ref":"/docs/devops/jenkins/jenkinslib/readme/","tags":"","title":""},{"body":"doc|registry\nterraform 是HashiCorp 的基础设施即代码（IaC）的管理工具。使用golang编写 安装\nwget https://releases.hashicorp.com/terraform/1.9.2/terraform_1.9.2_linux_amd64.zip unzip terraform_1.9.2_linux_amd64.zip mv terraform /usr/bin/ 验证\n[root@jenkins01 ~]# terraform --version Terraform v1.9.2 on linux_amd64 安装命令补全\nterraform -install-autocomplete \u0026\u0026 logout 快速开始 创建一个空目录\nmkdir terraform-demo 将以下内容写入 terraform-demo/main.tf\nterraform { required_providers { docker = { source = \"kreuzwerker/docker\" version = \"~\u003e 3.0.1\" } } } provider \"docker\" {} resource \"docker_image\" \"nginx\" { name = \"nginx\" keep_locally = false } resource \"docker_container\" \"nginx\" { image = docker_image.nginx.name name = \"nginx\" ports { internal = 80 external = 8000 } } 初始化\nwget https://github.com/kreuzwerker/terraform-provider-docker/releases/download/v3.0.2/terraform-provider-docker_3.0.2_linux_amd64.zip mkdir -p ~/.terraform.d/plugins/registry.terraform.io/kreuzwerker/docker/3.0.2/linux_amd64 unzip terraform-provider-docker_3.0.2_linux_amd64.zip -d ~/.terraform.d/plugins/registry.terraform.io/kreuzwerker/docker/3.0.2/linux_amd64 terraform init [root@jenkins01 ~]# cd terraform-demo/ [root@jenkins01 terraform-demo]# terraform init Initializing the backend... Initializing provider plugins... - Finding kreuzwerker/docker versions matching \"~\u003e 3.0.1\"... - Installing kreuzwerker/docker v3.0.2... - Installed kreuzwerker/docker v3.0.2 (self-signed, key ID BD080C4571C6104C) Partner and community providers are signed by their developers. If you'd like to know more about provider signing, you can read about it here: https://www.terraform.io/docs/cli/plugins/signing.html Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \"terraform init\" in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. 预览应用计划\n[root@192 test]# terraform plan Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # docker_container.nginx will be created + resource \"docker_container\" \"nginx\" { + attach = false + bridge = (known after apply) + command = (known after apply) + container_logs = (known after apply) + container_read_refresh_timeout_milliseconds = 15000 + entrypoint = (known after apply) + env = (known after apply) + exit_code = (known after apply) + hostname = (known after apply) + id = (known after apply) + image = (known after apply) + init = (known after apply) + ipc_mode = (known after apply) + log_driver = (known after apply) + logs = false + must_run = true + name = \"tutorial\" + network_data = (known after apply) + read_only = false + remove_volumes = true + restart = \"no\" + rm = false + runtime = (known after apply) + security_opts = (known after apply) + shm_size = (known after apply) + start = true + stdin_open = false + stop_signal = (known after apply) + stop_timeout = (known after apply) + tty = false + wait = false + wait_timeout = 60 + healthcheck (known after apply) + labels (known after apply) + ports { + external = 8000 + internal = 80 + ip = \"0.0.0.0\" + protocol = \"tcp\" } } # docker_image.nginx will be created + resource \"docker_image\" \"nginx\" { + id = (known after apply) + image_id = (known after apply) + keep_locally = false + name = \"nginx\" + repo_digest = (known after apply) } Plan: 2 to add, 0 to change, 0 to destroy. ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── Note: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run \"terraform apply\" now. 应用该配置\n[root@192 test]# terraform apply Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # docker_container.nginx will be created + resource \"docker_container\" \"nginx\" { + attach = false + bridge = (known after apply) + command = (known after apply) + container_logs = (known after apply) + container_read_refresh_timeout_milliseconds = 15000 + entrypoint = (known after apply) + env = (known after apply) + exit_code = (known after apply) + hostname = (known after apply) + id = (known after apply) + image = (known after apply) + init = (known after apply) + ipc_mode = (known after apply) + log_driver = (known after apply) + logs = false + must_run = true + name = \"tutorial\" + network_data = (known after apply) + read_only = false + remove_volumes = true + restart = \"no\" + rm = false + runtime = (known after apply) + security_opts = (known after apply) + shm_size = (known after apply) + start = true + stdin_open = false + stop_signal = (known after apply) + stop_timeout = (known after apply) + tty = false + wait = false + wait_timeout = 60 + healthcheck (known after apply) + labels (known after apply) + ports { + external = 8000 + internal = 80 + ip = \"0.0.0.0\" + protocol = \"tcp\" } } # docker_image.nginx will be created + resource \"docker_image\" \"nginx\" { + id = (known after apply) + image_id = (known after apply) + keep_locally = false + name = \"nginx\" + repo_digest = (known after apply) } Plan: 2 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes docker_image.nginx: Creating... docker_image.nginx: Still creating... [10s elapsed] docker_image.nginx: Creation complete after 20s [id=sha256:fffffc90d343cbcb01a5032edac86db5998c536cd0a366514121a45c6723765cnginx] docker_container.nginx: Creating... docker_container.nginx: Creation complete after 1s [id=360704dcbb82c35c9e4c65f604ad6af22e4277fed5bb34b0b75eed58ead3ced6] Apply complete! Resources: 2 added, 0 changed, 0 destroyed. 创建了一个pod\n[root@192 test]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 360704dcbb82 fffffc90d343 \"/docker-entrypoint.…\" 5 seconds ago Up 4 seconds 0.0.0.0:8000-\u003e80/tcp tutorial 销毁pod\n[root@192 test]# terraform destroy docker_image.nginx: Refreshing state... [id=sha256:fffffc90d343cbcb01a5032edac86db5998c536cd0a366514121a45c6723765cnginx] docker_container.nginx: Refreshing state... [id=360704dcbb82c35c9e4c65f604ad6af22e4277fed5bb34b0b75eed58ead3ced6] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: - destroy Terraform will perform the following actions: # docker_container.nginx will be destroyed - resource \"docker_container\" \"nginx\" { - attach = false -\u003e null - command = [ - \"nginx\", - \"-g\", - \"daemon off;\", ] -\u003e null - container_read_refresh_timeout_milliseconds = 15000 -\u003e null - cpu_shares = 0 -\u003e null - dns = [] -\u003e null - dns_opts = [] -\u003e null - dns_search = [] -\u003e null - entrypoint = [ - \"/docker-entrypoint.sh\", ] -\u003e null - env = [] -\u003e null - group_add = [] -\u003e null - hostname = \"360704dcbb82\" -\u003e null - id = \"360704dcbb82c35c9e4c65f604ad6af22e4277fed5bb34b0b75eed58ead3ced6\" -\u003e null - image = \"sha256:fffffc90d343cbcb01a5032edac86db5998c536cd0a366514121a45c6723765c\" -\u003e null - init = false -\u003e null - ipc_mode = \"private\" -\u003e null - log_driver = \"json-file\" -\u003e null - log_opts = {} -\u003e null - logs = false -\u003e null - max_retry_count = 0 -\u003e null - memory = 0 -\u003e null - memory_swap = 0 -\u003e null - must_run = true -\u003e null - name = \"tutorial\" -\u003e null - network_data = [ - { - gateway = \"172.17.0.1\" - global_ipv6_prefix_length = 0 - ip_address = \"172.17.0.3\" - ip_prefix_length = 16 - mac_address = \"02:42:ac:11:00:03\" - network_name = \"bridge\" # (2 unchanged attributes hidden) }, ] -\u003e null - network_mode = \"default\" -\u003e null - privileged = false -\u003e null - publish_all_ports = false -\u003e null - read_only = false -\u003e null - remove_volumes = true -\u003e null - restart = \"no\" -\u003e null - rm = false -\u003e null - runtime = \"runc\" -\u003e null - security_opts = [] -\u003e null - shm_size = 64 -\u003e null - start = true -\u003e null - stdin_open = false -\u003e null - stop_signal = \"SIGQUIT\" -\u003e null - stop_timeout = 0 -\u003e null - storage_opts = {} -\u003e null - sysctls = {} -\u003e null - tmpfs = {} -\u003e null - tty = false -\u003e null - wait = false -\u003e null - wait_timeout = 60 -\u003e null # (7 unchanged attributes hidden) - ports { - external = 8000 -\u003e null - internal = 80 -\u003e null - ip = \"0.0.0.0\" -\u003e null - protocol = \"tcp\" -\u003e null } } # docker_image.nginx will be destroyed - resource \"docker_image\" \"nginx\" { - id = \"sha256:fffffc90d343cbcb01a5032edac86db5998c536cd0a366514121a45c6723765cnginx\" -\u003e null - image_id = \"sha256:fffffc90d343cbcb01a5032edac86db5998c536cd0a366514121a45c6723765c\" -\u003e null - keep_locally = false -\u003e null - name = \"nginx\" -\u003e null - repo_digest = \"nginx@sha256:67682bda769fae1ccf5183192b8daf37b64cae99c6c3302650f6f8bf5f0f95df\" -\u003e null } Plan: 0 to add, 0 to change, 2 to destroy. Do you really want to destroy all resources? Terraform will destroy all your managed infrastructure, as shown above. There is no undo. Only 'yes' will be accepted to confirm. Enter a value: yes docker_container.nginx: Destroying... [id=360704dcbb82c35c9e4c65f604ad6af22e4277fed5bb34b0b75eed58ead3ced6] docker_container.nginx: Destruction complete after 1s docker_image.nginx: Destroying... [id=sha256:fffffc90d343cbcb01a5032edac86db5998c536cd0a366514121a45c6723765cnginx] docker_image.nginx: Destruction complete after 0s Destroy complete! Resources: 2 destroyed. [root@jenkins01 terraform-exsi]# cat main.tf terraform { required_version = \"\u003e= 0.13\" required_providers { vsphere = { source = \"hashicorp/vsphere\" version = \"\u003e= 2.0.0\" } } } provider \"vsphere\" { user = \"administrator@vsphere.local\" password = \"Cc1020304050!\" vsphere_server = \"192.168.0.214\" allow_unverified_ssl = true } data \"vsphere_datacenter\" \"datacenter\" { name = \"Beijing\" } data \"vsphere_resource_pool\" \"pool\" { name = \"Resources\" datacenter_id = data.vsphere_datacenter.datacenter.id } data \"vsphere_datastore\" \"store\" { name = \"data\" datacenter_id = data.vsphere_datacenter.datacenter.id } data \"vsphere_network\" \"network\" { name = \"VM Network\" datacenter_id = data.vsphere_datacenter.datacenter.id } resource \"vsphere_virtual_machine\" \"example_vm\" { name = \"ExampleVM\" resource_pool_id = data.vsphere_resource_pool.pool.id datastore_id = data.vsphere_datastore.store.id num_cpus = 2 memory = 4096 guest_id = \"otherGuest\" network_interface { network_id = data.vsphere_network.network.id } disk { label = \"disk0\" size = 40 } cdrom { path = \"CentOS-7.9-x86_64-DVD-2009.iso\" datastore_id = data.vsphere_datastore.store.id } } datastore1/template-centos7/centos7-template-1.vmdk datastore1/template-centos7/centos7-template.ovf ","categories":"","description":"","excerpt":"doc|registry\nterraform 是HashiCorp 的基础设施即代码（IaC）的管理工具。使用golang编写 安装\nwget https://releases.hashicorp.com/terraform/1.9.2/terraform_1.9.2_linux_amd64.zip unzip terraform_1.9.2_linux_amd64.zip mv …","ref":"/docs/devops/terraform/terraform/","tags":"","title":""},{"body":" ","categories":"","description":"","excerpt":" ","ref":"/docs/linux/ansible/1/","tags":"","title":""}]